{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0342bf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 03:39:42.104520: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-13 03:39:42.104541: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,f1_score\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,f1_score,log_loss,precision_score,recall_score,classification_report\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4a23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long=pd.read_csv(\"../data/train_val_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff84a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprompt=\"Does this paper tackle a social problem?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcda97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.abstract=df_long.abstract.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd699da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.abstract=df_long.abstract.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True)\n",
    "df_long.title=df_long.title.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ec1a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long=df_long.assign(statement=preprompt+\"Title: \"+df_long.title+\"\\nAbstract: \"+df_long.abstract\n",
    "                       +\"\\nDoes this paper tackle a social problem? Answer \\\"Yes\\\" or \\\"No\\\": \\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54c3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = pipeline('fill-mask', model='bert-large-uncased',top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b191518d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does this paper tackle a social problem?\n",
      "Title: Syntactic Data Augmentation Increases Robustness to Inference Heuristics\n",
      "Abstract: Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model's limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT's accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.44755601324512384 No:  0.5524439867548762\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: IITP-AINLPML at SemEval-2020 Task 12: Offensive Tweet Identification and Target Categorization in a Multitask Environment\n",
      "Abstract: In this paper, we describe the participation of IITP-AINLPML team in the SemEval-2020 SharedTask 12 on Offensive Language Identification and Target Categorization in English Twitter data. Our proposed model learns to extract textual features using a BiGRU-based deep neural network supported by a Hierarchical Attention architecture to focus on the most relevant areas in the text. We leverage the effectiveness of multitask learning while building our models for sub-task A and B. We do necessary undersampling of the over-represented classes in the sub-tasks A and C.During training, we consider a threshold of 0.5 as the separation margin between the instances belonging to classes OFF and NOT in sub-task A and UNT and TIN in sub-task B. For sub-task C, the class corresponding to the maximum score among the given confidence scores of the classes(IND, GRP and OTH) is considered as the final label for an instance. Our proposed model obtains the macro F1-scores of 90.95\\%, 55.69\\% and 63.88\\% in sub-task A, B and C, respectively.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.47422573439958177 No:  0.5257742656004183\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Comparing learnability of two dependency schemes: `semantic' (UD) and `syntactic' (SUD)\n",
      "Abstract: This paper contributes to the thread of research on the learnability of different dependency annotation schemes: one (`semantic') favouring content words as heads of dependency relations and the other (`syntactic') favouring syntactic heads. Several studies have lent support to the idea that choosing syntactic criteria for assigning heads in dependency trees improves the performance of dependency parsers. This may be explained by postulating that syntactic approaches are generally more learnable. In this study, we test this hypothesis by comparing the performance of five parsing systems (both transition- and graph-based) on a selection of 21 treebanks, each in a `semantic' variant, represented by standard UD (Universal Dependencies), and a `syntactic' variant, represented by SUD (Surface-syntactic Universal Dependencies): unlike previously reported experiments, which considered learnability of `semantic' and `syntactic' annotations of particular constructions in vitro, the experiments reported here consider whole annotation schemes in vivo. Additionally, we compare these annotation schemes using a range of quantitative syntactic properties, which may also reflect their learnability. The results of the experiments show that SUD tends to be more learnable than UD, but the advantage of one or the other scheme depends on the parser and the corpus in question.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.37129133739423625 No:  0.6287086626057637\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Dense Information Flow for Neural Machine Translation\n",
      "Abstract: Recently, neural machine translation has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework. From the optimization perspective, residual connections are adopted to improve learning performance for both encoder and decoder in most of these deep architectures, and advanced attention connections are applied as well. Inspired by the success of the DenseNet model in computer vision problems, in this paper, we propose a densely connected NMT architecture (DenseNMT) that is able to train more efficiently for NMT. The proposed DenseNMT not only allows dense connection in creating new features for both encoder and decoder, but also uses the dense attention structure to improve attention quality. Our experiments on multiple datasets show that DenseNMT structure is more competitive and efficient.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.7659153832918062 No:  0.23408461670819383\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Query-based summarization using MDL principle\n",
      "Abstract: Query-based text summarization is aimed at extracting essential information that answers the query from original text. The answer is presented in a minimal, often predefined, number of words. In this paper we introduce a new unsupervised approach for query-based extractive summarization, based on the minimum description length (MDL) principle that employs Krimp compression algorithm (Vreeken et al., 2011). The key idea of our approach is to select frequent word sets related to a given query that compress document sentences better and therefore describe the document better. A summary is extracted by selecting sentences that best cover query-related frequent word sets. The approach is evaluated based on the DUC 2005 and DUC 2006 datasets which are specifically designed for query-based summarization (DUC, 2005 2006). It competes with the best results.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6840144616989429 No:  0.3159855383010572\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Morpholog: Constrained and Supervised Learning of Morphology\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Annotation of Temporal Relations with Tango\n",
      "Abstract: Temporal annotation is a complex task characterized by low markup speed and low inter-annotator agreements scores. Tango is a graphical annotation tool for temporal relations. It is developed for the TimeML annotation language and allows annotators to build a graph that resembles a timeline. Temporal relations are added by selecting events and drawing labeled arrows between them. Tango is integrated with a temporal closure component and includes features like SmartLink, user prompting and automatic linking of time expressions. Tango has been used to create two corpora with temporal annotation, TimeBank and the AQUAINT Opinion corpus.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Automated scoring across different modalities\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.7012033949995174 No:  0.29879660500048255\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: A Neural, Interactive-predictive System for Multimodal Sequence to Sequence Tasks\n",
      "Abstract: We present a demonstration of a neural interactive-predictive system for tackling multimodal sequence to sequence tasks. The system generates text predictions to different sequence to sequence tasks: machine translation, image and video captioning. These predictions are revised by a human agent, who introduces corrections in the form of characters. The system reacts to each correction, providing alternative hypotheses, compelling with the feedback provided by the user. The final objective is to reduce the human effort required during this correction process. This system is implemented following a client-server architecture. For accessing the system, we developed a website, which communicates with the neural model, hosted in a local server. From this website, the different tasks can be tackled following the interactive--predictive framework. We open-source all the code developed for building this system. The demonstration in hosted in http://casmacat.prhlt.upv.es/interactive-seq2seq.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:  0.6886825502054106 No:  0.3113174497945895\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: K2Q: Generating Natural Language Questions from Keywords with User Refinements\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Semantic Vectors: a Scalable Open Source Package and Online Technology Management Application\n",
      "Abstract: This paper describes the open source SemanticVectors package that efficiently creates semantic vectors for words and documents from a corpus of free text articles. We believe that this package can play an important role in furthering research in distributional semantics, and (perhaps more importantly) can help to significantly reduce the current gap that exists between good research results and valuable applications in production software. Two clear principles that have guided the creation of the package so far include ease-of-use and scalability. The basic package installs and runs easily on any Java-enabled platform, and depends only on Apache Lucene. Dimension reduction is performed using Random Projection, which enables the system to scale much more effectively than other algorithms used for the same purpose. This paper also describes a trial application in the Technology Management domain, which highlights some user-centred design challenges which we believe are also key to successful deployment of this technology.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.4664458853146866 No:  0.5335541146853134\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection\n",
      "Abstract: Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between MATE and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.3721793578843126 No:  0.6278206421156873\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: OntoLex as a possible Bridge between WordNets and full lexical Descriptions\n",
      "Abstract: In this paper we describe our current work on representing a recently created German lexical semantics resource in OntoLex-Lemon and in conformance with WordNet specifications. Besides presenting the representation effort, we show the utilization of OntoLex-Lemon to bridge from WordNet-like resources to full lexical descriptions and extend the coverage of WordNets to other types of lexical data, such as decomposition results, exemplified for German data, and inflectional phenomena, here outlined for English data.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6831185728892534 No:  0.3168814271107466\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Putting Meaning into Grammar Learning\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Divergence Patterns in Machine Translation between Hindi and English\n",
      "Abstract: The issue of translation divergence is an important research topic in the area of machine translation. An exhaustive study of the divergence issues in MT is necessary for their proper classification and resolution. In the literature on MT, scholars have examined the issue and have proposed ways for their classification and resolution (Dorr 1993, 1994). However, the topic still needs further exploration to identify different sources of translation divergence in different pairs of translation languages. In this paper, we discuss translation patterns between Hindi and English of different types of constructions with a view to identifying the potential topics of the translation divergences. We take Dorr's (1993, 1994) classification of translation divergence as the base to examine the different topics of translation divergence in Hindi and English. The primary goal of the paper is to point out different types of translation divergences in Hindi and English MT that have not been discussed in the existing literature.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.5243271346787753 No:  0.47567286532122466\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Crowdsourcing the evaluation of a domain-adapted named entity recognition system\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs\n",
      "Abstract: One of the biggest challenges of end-to-end language generation from meaning representations in dialogue systems is making the outputs more natural and varied. Here we take a large corpus of 50K crowd-sourced utterances in the restaurant domain and develop text analysis methods that systematically characterize types of sentences in the training data. We then automatically label the training data to allow us to conduct two kinds of experiments with a neural generator. First, we test the effect of training the system with different stylistic partitions and quantify the effect of smaller, but more stylistically controlled training data. Second, we propose a method of labeling the style variants during training, and show that we can modify the style of the generated utterances using our stylistic labels. We contrast and compare these methods that can be used with any existing large corpus, showing how they vary in terms of semantic quality and stylistic control.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.5196465306813784 No:  0.4803534693186216\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Extending WordNet with Hypernyms and Siblings Acquired from Wikipedia\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Extracting Key Paragraph based on Topic and Event Detection Towards Multi-Document Summarization\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Is your Statement Purposeless? Predicting Computer Science Graduation Admission Acceptance based on Statement Of Purpose\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Finding the Origin of a Translated Historical Document\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Exploiting a Probabilistic Hierarchical Model for Generation\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: The ICSI Meeting Recorder Dialog Act (MRDA) Corpus\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: One Language to rule them all: modelling Morphological Patterns in a Large Scale Italian Lexicon with SWRL\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or Lexical Resources?\n",
      "Abstract: This paper presents an approach combining lexico-semantic resources and distributed representations of words applied to the evaluation in machine translation (MT). This study is made through the enrichment of a well-known MT evaluation metric: METEOR. METEOR enables an approximate match (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semanticresources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.7021684982095038 No:  0.2978315017904962\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Proceedings, 13th Annual Meeting, Association for Computational Linguistics (1: Language Understanding Systems)\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Efficient Beam Thresholding for Statistical Machine Translation\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss\n",
      "Abstract: Classical and some deep learning techniques for Arabic text classification often depend on complex morphological analysis, word segmentation, and hand-crafted feature engineering. These could be eliminated by using character-level features. We propose a novel end-to-end Arabic document classification framework, Arabic document image-based classifier (AraDIC), inspired by the work on image-based character embeddings. AraDIC consists of an image-based character encoder and a classifier. They are trained in an end-to-end fashion using the class balanced loss to deal with the long-tailed data distribution problem. To evaluate the effectiveness of AraDIC, we created and published two datasets, the Arabic Wikipedia title (AWT) dataset and the Arabic poetry (AraP) dataset. To the best of our knowledge, this is the first image-based character embedding framework addressing the problem of Arabic text classification. We also present the first deep learning-based text classifier widely evaluated on modern standard Arabic, colloquial Arabic, and Classical Arabic. AraDIC shows performance improvement over classical and deep learning baselines by 12.29\\% and 23.05\\% for the micro and macro F-score, respectively.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.38303418998878935 No:  0.6169658100112106\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Induction of Fine-Grained Part-of-Speech Taggers via Classifier Combination and Crosslingual Projection\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6136228164357127 No:  0.3863771835642873\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: A Temporal Simulator for Developing Turn-Taking Methods for Spoken Dialogue Systems\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6372737480756886 No:  0.36272625192431135\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Sequence Models for Computational Etymology of Borrowings\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Improved Language Modeling for English-Persian Statistical Machine Translation\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.5996202354454843 No:  0.4003797645545157\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Centering Theory for Evaluation of Coherence in Computer-Aided Summaries\n",
      "Abstract: This paper investigates a new evaluation method for assessing the coherence of computer-aided summaries, justified by the inappropriacy of existing evaluation methods for this task. It develops a metric for Centering Theory (CT), a theory of local coherence and salience, to measure coherence in pairs of extracts and abstracts produced in a computer-aided summarisation environment. 100 news text summaries (50 pairs of extracts and their corresponding abstracts) are analysed using CT and the metric is applied to obtain a score for each summary; the summary with the higher score out of a pair is considered more coherent. Human judgement is also obtained to allow a comparison with the CT evaluation to assess the validity of the development of CT as a useful evaluation metric in computer-aided summarisation.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6189516062079545 No:  0.3810483937920455\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: FENAS: Flexible and Expressive Neural Architecture Search\n",
      "Abstract: Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image classification) with reasonable training speed using a weight sharing-based approach called Efficient Neural Architecture Search (ENAS). In this work, we propose a novel architecture search algorithm called Flexible and Expressible Neural Architecture Search (FENAS), with more flexible and expressible search space than ENAS, in terms of more activation functions, input edges, and atomic operations. Also, our FENAS approach is able to reproduce the well-known LSTM and GRU architectures (unlike ENAS), and is also able to initialize with them for finding architectures more efficiently. We explore this extended search space via evolutionary search and show that FENAS performs significantly better on several popular text classification tasks and performs similar to ENAS on standard language model benchmark. Further, we present ablations and analyses on our FENAS approach.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.35499167488201794 No:  0.645008325117982\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: On The Semantic Interpretation of Nominals\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Left-to-Right Target Generation for Hierarchical Phrase-Based Translation\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Stochastic Definite Clause Grammars\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:  0.9999999999999999 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Natural Language Reasoning using Coq: Interaction and Automation\n",
      "Abstract: Dans cet article, nous pr\\'esentons une utilisation des assistants des preuves pour traiter l'inf\\'erence en Language Naturel (NLI). D' abord, nous proposons d'utiliser les theories des types modernes comme langue dans laquelle traduire la s\\'emantique du langage naturel. Ensuite, nous impl\\'ementons cette s\\'emantique dans l'assistant de preuve Coq pour raisonner sur ceux-ci. En particulier, nous \\'evaluons notre proposition sur un sous-ensemble de la suite de tests FraCas, et nous montrons que 95.2\\% des exemples peuvent \\^etre correctement pr\\'edits. Nous discutons ensuite la question de l'automatisation et il est d\\'emontr\\'e que le langage de tactiques de Coq permet de construire des tactiques qui peuvent automatiser enti\\`erement les preuves, au moins pour les cas qui nous int\\'eressent.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.24056082864635625 No:  0.7594391713536437\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Affective and Contextual Embedding for Sarcasm Detection\n",
      "Abstract: Automatic sarcasm detection from text is an important classification task that can help identify the actual sentiment in user-generated data, such as reviews or tweets. Despite its usefulness, sarcasm detection remains a challenging task, due to a lack of any vocal intonation or facial gestures in textual data. To date, most of the approaches to addressing the problem have relied on hand-crafted affect features, or pre-trained models of non-contextual word embeddings, such as Word2vec. However, these models inherit limitations that render them inadequate for the task of sarcasm detection. In this paper, we propose two novel deep neural network models for sarcasm detection, namely ACE 1 and ACE 2. Given as input a text passage, the models predict whether it is sarcastic (or not). Our models extend the architecture of BERT by incorporating both affective and contextual features. To the best of our knowledge, this is the first attempt to directly alter BERT's architecture and train it from scratch to build a sarcasm classifier. Extensive experiments on different datasets demonstrate that the proposed models outperform state-of-the-art models for sarcasm detection with significant margins.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.43778877258396437 No:  0.5622112274160357\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: A Unification-based Approach to Mandarin Questions\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Parsing into Variable-in-situ Logico-Semantic Graphs\n",
      "Abstract: We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics. At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model. Our parser achieves an accuracy of 92.39\\% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature. The output of our parser is highly coherent: at least 91\\% graphs are valid, in that they allow at least one sound scope-resolved logical form.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.42914922611919515 No:  0.5708507738808049\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Computer-Produced Representation of Dialectal Variation: Initial Fricatives in Southern British English\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: NLP Web Services for Resource-Scarce Languages\n",
      "Abstract: In this paper, we present a project where existing text-based core technologies were ported to Java-based web services from various architectures. These technologies were developed over a period of eight years through various government funded projects for 10 resource-scarce languages spoken in South Africa. We describe the API and a simple web front-end capable of completing various predefined tasks.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.4727788076053115 No:  0.5272211923946886\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Graph-to-Tree Learning for Solving Math Word Problems\n",
      "Abstract: While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs. We conduct extensive experiments on two available datasets. Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. We also discuss case studies and empirically examine Graph2Tree's effectiveness in translating the MWP text into solution expressions.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.5216720120999971 No:  0.4783279879000028\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Detecting Opinion Polarities using Kernel Methods\n",
      "Abstract: We investigate the application of kernel methods to representing both structural and lexical knowledge for predicting polarity of opinions in consumer product review. We introduce any-gram kernels which model lexical information in a significantly faster way than the traditional n-gram features, while capturing all possible orders of n-grams n in a sequence without the need to explicitly present a pre-specified set of such orders. We also present an effective format to represent constituency and dependency structure together with aspect terms and sentiment polarity scores. Furthermore, we modify the traditional tree kernel function to compute the similarity based on word embedding vectors instead of exact string match and present experiments using the new models.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6682091927591116 No:  0.33179080724088844\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Learning the Relative Usefulness of Questions in Community QA\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Embedding Lexical Features via Low-Rank Tensors\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Automaton-based Parsing for Lexicalised Grammars\n",
      "Abstract: In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that during parsing some of the computation associated with different structures is duplicated. This paper explores ways in which the grammar can be precompiled into finite state automata so that some of this shared structure results in shared computation at run-time.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.40580924311356137 No:  0.5941907568864386\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: SWATCS65: Sentiment Classification Using an Ensemble of Class Projects\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: CodeQA: A Question Answering Dataset for Source Code Comprehension\n",
      "Abstract: We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.7525244407028191 No:  0.24747555929718093\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Agile Corpus Annotation in Practice: An Overview of Manual and Automatic Annotation of CVs\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Book Review: Discourse Processing by Manfred Stede\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6196776412919771 No:  0.3803223587080229\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Ontology-based Linguistic Annotation\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Arabic-Segmentation Combination Strategies for Statistical Machine Translation\n",
      "Abstract: Arabic segmentation was already applied successfully for the task of statistical machine translation (SMT). Yet, there is no consistent comparison of the effect of different techniques and methods over the final translation quality. In this work, we use existing tools and further re-implement and develop new methods for segmentation. We compare the resulting SMT systems based on the different segmentation methods over the small IWSLT 2010 BTEC and the large NIST 2009 Arabic-to-English translation tasks. Our results show that for both small and large training data, segmentation yields strong improvements, but, the differences between the top ranked segmenters are statistically insignificant. Due to the different methodologies that we apply for segmentation, we expect a complimentary variation in the results achieved by each method. As done in previous work, we combine several segmentation schemes of the same model but achieve modest improvements. Next, we try a different strategy, where we combine the different segmentation methods rather than the different segmentation schemes. In this case, we achieve stronger improvements over the best single system. Finally, combining schemes and methods has another slight gain over the best combination strategy.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.4354475986507333 No:  0.5645524013492667\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Inferring User Political Preferences from Streaming Communications\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6053744382279698 No:  0.3946255617720303\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: An Assessment of Experimental Protocols for Tracing Changes in Word Semantics Relative to Accuracy and Reliability\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Augmentable Paraphrase Extraction Framework\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: \\textlessStuMaBa\\textgreater: From Deep Representation to Surface\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Does this paper tackle a social problem?\n",
      "Title: Lachmannian Archetype Reconstruction for Ancient Manuscript Corpora\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: The Dialog State Tracking Challenge\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6082243826175905 No:  0.39177561738240957\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Social Network Analysis of Alice in Wonderland\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Observation de l'exp\\'erience client dans les restaurants (Mapping Reviewers' Experience in Restaurants)\n",
      "Abstract: Ces derni\\`eres ann\\'ees, les recherches sur la fouille d'opinions ou l'analyse des sentiments sont men\\'ees activement dans le domaine du Traitement Automatique des Langues (TAL). De nombreuses \\'etudes scientifiques portent sur l'extraction automatique des opinions positives ou n\\'egatives et de leurs cibles. Ce travail propose d'identifier automatiquement une \\'evaluation, exprim\\'ee explicitement ou implicitement par des internautes dans le corpus d'avis tir\\'e du Web. Six cat\\'egories d'\\'evaluation sont propos\\'ees : opinion positive, opinion n\\'egative, opinion mixte, intention, suggestion et description. La m\\'ethode utilis\\'ee est fond\\'ee sur l'apprentissage supervis\\'e qui tient compte des caract\\'eristiques linguistiques de chaque cat\\'egorie retenue. L'une des difficult\\'es que nous avons rencontr\\'ee concerne le d\\'es\\'equilibre entre les classes d'\\'evaluation cr\\'e\\'ees, cependant, cet obstacle a pu \\^etre surmont\\'e dans l'apprentissage gr\\^ace aux strat\\'egies de sur-\\'echantillonnage et aux strat\\'egies algorithmiques.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.0 No:  1.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Narrative Generation from Extracted Associations\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation\n",
      "Abstract: In the recent advances of natural language processing, the scale of the state-of-the-art models and datasets is usually extensive, which challenges the application of sample-based explanation methods in many aspects, such as explanation interpretability, efficiency, and faithfulness. In this work, for the first time, we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit. On top of this, we implement a hessian-free method with a model faithfulness guarantee. Finally, to compare our method with the others, we propose a semantic-based evaluation metric that can better align with humans' judgment of explanations than the widely adopted diagnostic or re-training measures. The empirical results on multiple real data sets demonstrate the proposed method's superior performance to popular explanation techniques such as Influence Function or TracIn on semantic evaluation.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:  0.7357321422753841 No:  0.2642678577246158\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation\n",
      "Abstract: Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at https://github.com/SunbowLiu/PTvsBT.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6464898645861495 No:  0.3535101354138505\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling\n",
      "Abstract: Current state-of-the-art systems for sequence labeling are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (Glove), our GCDT achieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000 Chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on Chunking.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.43419887731799717 No:  0.5658011226820028\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Enhancing Referential Success by Tracking Hearer Gaze\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.627687805640904 No:  0.372312194359096\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Large Linguistic Corpus Reduction with SCP Algorithms\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6500313144813495 No:  0.3499686855186504\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Finite-state Description of Semitic Morphology: A Case Study of Ancient Accadian\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.9999999999999999 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: SemEval-2007 Task 14: Affective Text\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space\n",
      "Abstract: Finding the right reviewers to assess the quality of conference submissions is a time consuming process for conference organizers. Given the importance of this step, various automated reviewer-paper matching solutions have been proposed to alleviate the burden. Prior approaches including bag-of-words model and probabilistic topic model are less effective to deal with the vocabulary mismatch and partial topic overlap between the submission and reviewer. Our approach, the common topic model, jointly models the topics common to the submission and the reviewer's profile while relying on abstract topic vectors. Experiments and insightful evaluations on two datasets demonstrate that the proposed method achieves consistent improvements compared to the state-of-the-art.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6607284451838228 No:  0.3392715548161772\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: A Character-level Decoder without Explicit Segmentation for Neural Machine Translation\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  1.0 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Grammatical Error Detection Using Tagger Disagreement\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.6208409691616984 No:  0.3791590308383016\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: Predicting failure of a mediated conversation in the context of asymetric role dialogues\n",
      "Abstract: In a human-to-human conversation between a user and his interlocutor in an assistance center, we suppose a context where the conclusion of the dialog can characterize a notion of success or failure, explicitly annotated or deduced. The study involves different approaches expected to have an influence on predictive classification model of failures. On the one hand, we will aim at taking into account the asymmetry of the speakers' roles in the modelling of the lexical distribution. On the other hand, we will determine whether the part of the lexicon most closely relating to the domain of customer assistance studied here, modifies the quality of the prediction. We will eventually assess the perspectives of generalization to morphologically comparable corpora.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n",
      "Yes:  0.7172755530916336 No:  0.2827244469083664\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP, and FrameNet Annotation Standards\n",
      "Abstract: \n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes:  0.9999999999999999 No:  0.0\n",
      "Label:  0\n",
      "################################\n",
      "Does this paper tackle a social problem?\n",
      "Title: La perception des s\\'equences consonantiques non-natives par les locuteurs monolingues de mandarin (Perception of non-native consonant sequences by Mandarin monolingual speakers)\n",
      "Abstract: Cette \\'etude examine le r\\^ole de la structure phonotactique native et des facteurs phon\\'etiques dans la perception des s\\'equences consonantiques non-natives. Des locuteurs monolingues de mandarin ont \\'et\\'e test\\'es dans les deux exp\\'eriences suivantes: dans la premi\\`ere exp\\'erience, les locuteurs ont du d\\'ecider s'ils entendaient une voyelle entre deux consonnes en \\'ecoutant des s\\'equences intervocaliques-CC (akta) et leurs contr\\^oles CVC (akata). Les participants mandarins monolingues ont tendance \\`a percevoir une voyelle entre deux consonnes dans les deux s\\'equences CC et CVC. Mais le pourcentage de la voyelle per\\ccue varie selon les diff\\'erentes s\\'equences. Dans la deuxi\\`eme exp\\'erience, les m\\^emes participants ont \\'ecout\\'e des s\\'equences CC initiales et intervocaliques (ktapa, akta) ainsi que CVC (katapa, akata) et les ont transcrites en Pinyin. Les strat\\'egies observ\\'ees dans la transcription: l'\\'epenth\\`ese, la m\\'etath\\`ese, l'omission de C1 et celle de C2, montrent que les participants sont sensibles aux facteurs phon\\'etiques. Les r\\'esultats des deux exp\\'eriences sugg\\`erent que la phonotactique native ainsi que des facteurs phon\\'etiques affectent la perception des s\\'equences non-natives.\n",
      "Does this paper tackle a social problem? Answer \"Yes\" or \"No\": \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (526) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8833/1067194467.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statement'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statement'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" [MASK]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredicted_blanks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print the completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmasked\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         )\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m         )\n\u001b[1;32m    996\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scirex/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (526) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for i,d in df_long.iterrows():\n",
    "    print(d['statement'])\n",
    "    input_prompt=d['statement']+\" [MASK]\"\n",
    "    predicted_blanks = model(input_prompt)\n",
    "    # print the completion\n",
    "\n",
    "    dict_uniques={}\n",
    "    dict_norm={}\n",
    "    for p in predicted_blanks:\n",
    "        p_modified=p['token_str'].lower().lstrip(' ')\n",
    "        if p_modified in dict_uniques:\n",
    "            dict_uniques[p_modified]=dict_uniques[p_modified]+p['score']\n",
    "        else:\n",
    "            dict_uniques[p_modified]=p['score']\n",
    "\n",
    "    if ('no' in dict_uniques.keys()) and ('yes' in dict_uniques.keys()):\n",
    "        dict_norm={'no':dict_uniques['no'],'yes':dict_uniques['yes']}\n",
    "    elif ('no' in dict_uniques.keys()):\n",
    "        dict_norm={'no':dict_uniques['no'],'yes':0}\n",
    "    elif ('yes' in dict_uniques.keys()):\n",
    "        dict_norm={'no':0,'yes':dict_uniques['yes']}\n",
    "    try:\n",
    "        factor=1.0/sum(dict_norm.values())\n",
    "        for k in dict_norm:\n",
    "            dict_norm[k] = dict_norm[k]*factor    \n",
    "\n",
    "        df_long.loc[i,'proba_1']=dict_norm['yes']\n",
    "        df_long.loc[i,'proba_0']=dict_norm['no']\n",
    "\n",
    "        #df_long.loc[i,'BERT_response']=predicted_blanks\n",
    "        print(\"Yes: \",dict_norm['yes'],\"No: \",dict_norm['no'])\n",
    "        print(\"Label: \",d['label'])\n",
    "        print(\"################################\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfd9d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0.48027274012565613,\n",
       " '?': 0.4616122245788574,\n",
       " '!': 0.026248710229992867,\n",
       " ';': 0.017631106078624725,\n",
       " '|': 0.011219942010939121,\n",
       " '\"': 0.002135386224836111,\n",
       " '...': 0.00042458821553736925,\n",
       " \"'\": 0.00014783229562453926,\n",
       " '': 3.122911584796384e-05,\n",
       " '[unk]': 2.2687643649987876e-05,\n",
       " '-': 1.4771692804060876e-05,\n",
       " '': 1.2983095984964166e-05,\n",
       " ']': 1.2935525774082635e-05,\n",
       " ':': 1.0214113899564836e-05,\n",
       " '}': 8.818041351332795e-06,\n",
       " ')': 4.571681984089082e-06,\n",
       " ',': 4.232653736835346e-06,\n",
       " 'if': 3.419616405153647e-06,\n",
       " 'and': 3.273455831731553e-06,\n",
       " '(': 3.2142281725100474e-06}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84157699",
   "metadata": {},
   "source": [
    "df_long=pd.read_csv(\"predictions_bert_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a233ee3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_long=df_long.assign(human_response_binary=np.where(df_long['human.response']>0.5,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18082a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long=df_long.assign(study_aux=np.where(df_long.study.isin(['deli.lines','snack.lines']),'lines',df_long.study))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "787b138c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    148.000000\n",
       "mean       0.494062\n",
       "std        0.108784\n",
       "min        0.343827\n",
       "25%        0.422775\n",
       "50%        0.458609\n",
       "75%        0.531043\n",
       "max        0.775715\n",
       "Name: proba_1, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.proba_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04e2d4",
   "metadata": {},
   "source": [
    "## alternative response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "518e9974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_long=df_long.assign(GPT3_response_probas_binary=np.where(df_long.proba_1>0.5,1,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35352e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long=df_long.assign(correct_probas=np.where(df_long.human_response_binary==df_long.GPT3_response_probas_binary,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2faecea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5405405405405406"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.correct_probas.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7404305f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "study_aux\n",
       "blue.house    0.537037\n",
       "cannonball    0.571429\n",
       "lines         0.530303\n",
       "Name: correct_probas, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.groupby(['study_aux']).correct_probas.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a331db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.270138318109218\n",
      "accuracy:  0.5405405405405406\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE: \",mean_absolute_error(df_long['human.response'], df_long.proba_1))\n",
    "\n",
    "print(\"accuracy: \",f1_score(df_long.loc[:,'human_response_binary'], df_long.loc[:,'GPT3_response_probas_binary'],average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8165170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64        90\n",
      "           1       0.40      0.34      0.37        58\n",
      "\n",
      "    accuracy                           0.54       148\n",
      "   macro avg       0.51      0.51      0.50       148\n",
      "weighted avg       0.53      0.54      0.53       148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## gamma long\n",
    "print(classification_report(df_long.loc[:,'human_response_binary'], df_long.loc[:,'GPT3_response_probas_binary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1847a41d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue.house\n",
      "57.24\n",
      "cannonball\n",
      "50.88\n",
      "lines\n",
      "45.16\n"
     ]
    }
   ],
   "source": [
    "for c in df_long['study_aux'].unique():\n",
    "    print(c)\n",
    "    dd=classification_report(df_long.loc[df_long['study_aux']==c,'human_response_binary'],\n",
    "                            df_long.loc[df_long['study_aux']==c,'GPT3_response_probas_binary'],digits=4,output_dict=True)\n",
    "    print(\"{0:.2f}\".format(100*dd['weighted avg']['f1-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17efd687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1w  53.33\n",
      "accuracy  54.05\n",
      "w-precision  52.91\n",
      "w-recall  54.05\n"
     ]
    }
   ],
   "source": [
    "## domain feat\n",
    "gen=classification_report(df_long.loc[:,'human_response_binary'],\n",
    "                            df_long.loc[:,'GPT3_response_probas_binary'],digits=4,output_dict=True)\n",
    "\n",
    "print(\"f1w \",\"{0:.2f}\".format(100*gen['weighted avg']['f1-score']))\n",
    "print(\"accuracy \",\"{0:.2f}\".format(100*gen['accuracy']))\n",
    "print(\"w-precision \",\"{0:.2f}\".format(100*gen['weighted avg']['precision']))\n",
    "print(\"w-recall \",\"{0:.2f}\".format(100*gen['weighted avg']['recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f7dbd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    98\n",
       "1    50\n",
       "Name: GPT3_response_probas_binary, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.GPT3_response_probas_binary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f01f1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.to_csv(\"predictions_bert_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28121528",
   "metadata": {},
   "source": [
    "## Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec39fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "920abc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Entropy: 0.714 nats\n"
     ]
    }
   ],
   "source": [
    "mean_ce = backend.eval(binary_crossentropy(df_long['human.response'], df_long['proba_1']))\n",
    "print('Average Cross Entropy: %.3f nats' % mean_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,f1_score\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,f1_score,log_loss,precision_score,recall_score,classification_report\n",
    "from transformers import pipeline\n",
    "\n",
    "df_long=pd.read_csv(\"./short_long_prompts_v2.csv\")\n",
    "\n",
    "preprompt=\"Read the situation and answer the question.\\nSituation: \"\n",
    "\n",
    "\n",
    "df_long=df_long.assign(pre_statement=np.where(df_long.study=='blue.house',\"Analyze Hank's action.\",\n",
    "                                             \"Analyze that person's action.\"))\n",
    "\n",
    "df_long=df_long.assign(statement=preprompt+df_long.prompt_short+\"\\nQuestion: \"+df_long.pre_statement+\" Is it ok to break the rule in this situation? Answer by one capitalized word, either \\\"YES\\\" or \\\"NO\\\".\"+\"\\nAnswer: \\\"\")\n",
    "\n",
    "df_long=df_long.assign(pre_statement=np.where(df_long.study=='blue.house',\"Analyze Hank's action. Is it ok to break the rule in this situation? Answer just \\\"yes\\\" or \\\"no\\\".\",\n",
    "                                             \"Analyze that person's action. Is it ok to break the rule in this situation? Answer just \\\"yes\\\" or \\\"no\\\".\"))\n",
    "\n",
    "df_long=df_long.assign(statement=preprompt+df_long.prompt_short+\"\\nQuestion: \"+df_long.pre_statement+\"\\nAnswer: \\\"\")\n",
    "\n",
    "df_long=df_long.assign(pre_statement=np.where(df_long.study=='blue.house',\"Analyze Hank's action. Is it ok to break the rule in this situation? Answer by one capitalized word, either \\\"YES\\\" or \\\"NO\\\".\",\n",
    "                                             \"Analyze that person's action. Is it ok to break the rule in this situation? Answer by one capitalized word, either \\\"YES\\\" or \\\"NO\\\".\"))\n",
    "\n",
    "df_long=df_long.assign(statement=preprompt+df_long.prompt_short+\"\\nQuestion: \"+df_long.pre_statement+\"\\nAnswer: \\\"\")\n",
    "\n",
    "model = pipeline('fill-mask', model='bert-large-uncased',top_k=15)\n",
    "\n",
    "for i,d in df_long.iterrows():\n",
    "    print(d['statement'])\n",
    "    input_prompt=d['statement']+\" [MASK]\"\n",
    "    predicted_blanks = model(input_prompt)\n",
    "    # print the completion\n",
    "\n",
    "    dict_uniques={}\n",
    "    dict_norm={}\n",
    "    for p in predicted_blanks:\n",
    "        p_modified=p['token_str'].lower().lstrip(' ')\n",
    "        if p_modified in dict_uniques:\n",
    "            dict_uniques[p_modified]=dict_uniques[p_modified]+p['score']\n",
    "        else:\n",
    "            dict_uniques[p_modified]=p['score']\n",
    "\n",
    "    if ('no' in dict_uniques.keys()) and ('yes' in dict_uniques.keys()):\n",
    "        dict_norm={'no':dict_uniques['no'],'yes':dict_uniques['yes']}\n",
    "    elif ('no' in dict_uniques.keys()):\n",
    "        dict_norm={'no':dict_uniques['no'],'yes':0}\n",
    "    elif ('yes' in dict_uniques.keys()):\n",
    "        dict_norm={'no':0,'yes':dict_uniques['yes']}\n",
    "\n",
    "    factor=1.0/sum(dict_norm.values())\n",
    "    for k in dict_norm:\n",
    "        dict_norm[k] = dict_norm[k]*factor    \n",
    "\n",
    "    df_long.loc[i,'proba_1']=dict_norm['yes']\n",
    "    df_long.loc[i,'proba_0']=dict_norm['no']\n",
    "    \n",
    "    #df_long.loc[i,'BERT_response']=predicted_blanks\n",
    "    print(\"Yes: \",dict_norm['yes'],\"No: \",dict_norm['no'])\n",
    "    print(\"Human response: \",d['human.response'])\n",
    "    print(\"################################\")\n",
    "\n",
    "df_long=pd.read_csv(\"predictions_bert_1.csv\")\n",
    "\n",
    "df_long=df_long.assign(human_response_binary=np.where(df_long['human.response']>0.5,1,0))\n",
    "\n",
    "df_long=df_long.assign(study_aux=np.where(df_long.study.isin(['deli.lines','snack.lines']),'lines',df_long.study))\n",
    "\n",
    "df_long.proba_1.describe()\n",
    "\n",
    "## alternative response\n",
    "\n",
    "df_long=df_long.assign(GPT3_response_probas_binary=np.where(df_long.proba_1>0.5,1,0))\n",
    "\n",
    "\n",
    "df_long=df_long.assign(correct_probas=np.where(df_long.human_response_binary==df_long.GPT3_response_probas_binary,1,0))\n",
    "\n",
    "df_long.correct_probas.mean()\n",
    "\n",
    "df_long.groupby(['study_aux']).correct_probas.mean()\n",
    "\n",
    "print(\"MAE: \",mean_absolute_error(df_long['human.response'], df_long.proba_1))\n",
    "\n",
    "print(\"accuracy: \",f1_score(df_long.loc[:,'human_response_binary'], df_long.loc[:,'GPT3_response_probas_binary'],average='micro'))\n",
    "\n",
    "## gamma long\n",
    "print(classification_report(df_long.loc[:,'human_response_binary'], df_long.loc[:,'GPT3_response_probas_binary']))\n",
    "\n",
    "for c in df_long['study_aux'].unique():\n",
    "    print(c)\n",
    "    dd=classification_report(df_long.loc[df_long['study_aux']==c,'human_response_binary'],\n",
    "                            df_long.loc[df_long['study_aux']==c,'GPT3_response_probas_binary'],digits=4,output_dict=True)\n",
    "    print(\"{0:.2f}\".format(100*dd['weighted avg']['f1-score']))\n",
    "\n",
    "## domain feat\n",
    "gen=classification_report(df_long.loc[:,'human_response_binary'],\n",
    "                            df_long.loc[:,'GPT3_response_probas_binary'],digits=4,output_dict=True)\n",
    "\n",
    "print(\"f1w \",\"{0:.2f}\".format(100*gen['weighted avg']['f1-score']))\n",
    "print(\"accuracy \",\"{0:.2f}\".format(100*gen['accuracy']))\n",
    "print(\"w-precision \",\"{0:.2f}\".format(100*gen['weighted avg']['precision']))\n",
    "print(\"w-recall \",\"{0:.2f}\".format(100*gen['weighted avg']['recall']))\n",
    "\n",
    "df_long.GPT3_response_probas_binary.value_counts()\n",
    "\n",
    "df_long.to_csv(\"predictions_bert_1.csv\",index=False)\n",
    "\n",
    "## Cross entropy\n",
    "\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend\n",
    "\n",
    "mean_ce = backend.eval(binary_crossentropy(df_long['human.response'], df_long['proba_1']))\n",
    "print('Average Cross Entropy: %.3f nats' % mean_ce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
