{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6c6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "POS_PROPORTION=15\n",
    "TRAIN_SET_SIZE=30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee1c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labeled_data(df_test_final,train_set,test_set,acl_labeled,website_positive):\n",
    "\n",
    "    ## make sure it doesnt contain observations in the training set\n",
    "    acl_labeled=acl_labeled.loc[(~acl_labeled.paper_name.isin(test_set.title)) & (~acl_labeled.paper_name.isin(test_set.title_clean))]\n",
    "    ## label them\n",
    "    acl_labeled=acl_labeled.assign(label=np.where(acl_labeled['social good domain'].isna(),0,1))\n",
    "    acl_labeled=acl_labeled.loc[:,['paper_name','label']].rename(columns={'paper_name':'title'})\n",
    "    acl_labeled_in_train=acl_labeled.loc[acl_labeled.title.isin(train_set.title.values)].reset_index(drop=True)\n",
    "    acl_labeled_add=acl_labeled.loc[~acl_labeled.title.isin(train_set.title.values)].reset_index(drop=True)\n",
    "    acl_labeled_add=acl_labeled_add\n",
    "    acl_labeled_add=acl_labeled_add.assign(title_abstract=acl_labeled_add.title)\n",
    "    acl_labeled_add=acl_labeled_add.assign(abstract=\"\")\n",
    "    acl_labeled_add=acl_labeled_add.assign(year=2020)\n",
    "    acl_labeled_add=acl_labeled_add.assign(ID=acl_labeled_add.title)\n",
    "\n",
    "    train_set=train_set.merge(acl_labeled_in_train,on='title',how='left')\n",
    "    ## concat acl labeled\n",
    "    train_set=pd.concat([train_set,acl_labeled_add])\n",
    "\n",
    "    train_set=train_set.assign(abstract=train_set.abstract.fillna(''))\n",
    "    train_set=train_set.assign(title_abstract=train_set.title+\". \"+train_set.abstract)\n",
    "    train_set.title_abstract=train_set.title_abstract.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True)\n",
    "\n",
    "    ## website labeled positive examples\n",
    "\n",
    "    website_positive=website_positive.assign(title_abstract=website_positive.title+\". \"+website_positive.abstract)\n",
    "    website_positive=website_positive.rename(columns={'paperId':'ID'})\n",
    "    website_positive=website_positive.drop_duplicates(subset=['ID'])\n",
    "    website_positive=website_positive.assign(label=1)\n",
    "    website_positive=website_positive.loc[:,['ID','title','abstract','title_abstract','label','year','url']]\n",
    "    website_positive_in_train=website_positive.loc[((website_positive.title.isin(train_set.title.unique()))|(website_positive.ID.isin(train_set.ID.unique())))].reset_index(drop=True)\n",
    "    website_positive_add=website_positive.loc[~website_positive.ID.isin(website_positive_in_train.ID.unique())].reset_index(drop=True)\n",
    "\n",
    "    train_set=train_set.assign(label=np.where(train_set.title.isin(website_positive_in_train.title.unique()),1,train_set.label))\n",
    "\n",
    "    df_test_final=df_test_final.assign(title_abstract=df_test_final.title_abstract_clean)\n",
    "\n",
    "    ## concat website\n",
    "    train_set=pd.concat([train_set,website_positive_add,df_test_final])\n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9185eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_types(keywords):\n",
    "    ## keywords\n",
    "    keywords=keywords.loc[(~keywords.Keywords.isna())&(keywords['Concern w.r.t. precision'].isna()),['Keywords','python_checker (default = string_match, other_options=nltk.word_tokenize + match; lower_case+remove_non_alphabet+string_match']]\n",
    "    keywords.columns=['keywords','method']\n",
    "    keywords.keywords=keywords.keywords.str.lower()\n",
    "    keywords=keywords.assign(extraction_method=np.where(keywords.method.isna(),'contains',\n",
    "                                            np.where(keywords.method.str.lower().str.contains('exclude'),'start_special',\n",
    "                                            np.where(keywords.method.str.lower().str.contains('not'),'not_in',\n",
    "                                            np.where(keywords.method.str.lower().str.contains('starts'),'start','in')))))\n",
    "    keywords=keywords.assign(keywords=keywords.keywords.replace(\"-\",\" \",regex=True))\n",
    "    keywords=keywords.assign(keywords=keywords.keywords.apply(lambda x:re.sub('[^a-zA-Z0-9 ]+', '',x)))\n",
    "    key_start=keywords.loc[keywords.extraction_method=='start']\n",
    "    key_contains=keywords.loc[keywords.extraction_method=='contains']\n",
    "    key_in=keywords.loc[keywords.extraction_method=='in']\n",
    "    key_not_in=keywords.loc[keywords.extraction_method=='not_in']\n",
    "    key_special=keywords.loc[keywords.extraction_method=='start_special']\n",
    "    return (key_start,key_contains,key_in,key_not_in,key_special)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea68fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(train_set,keywords):\n",
    "\n",
    "    key_start,key_contains,key_in,key_not_in,key_special=get_keyword_types(keywords)\n",
    "    train_set=train_set.assign(title_abstract_search=train_set.title_clean.replace(\"-\",\" \",regex=True))\n",
    "    train_set=train_set.assign(title_abstract_search=train_set.title_abstract_search.apply(lambda x:re.sub('[^a-zA-Z0-9 ]+', '',x)))\n",
    "\n",
    "    ## keywords\n",
    "    train_set=train_set.assign(silver_pos=np.where(\n",
    "        (train_set.title_abstract_search.apply(lambda x:any(word.startswith(tuple(key_special.keywords)) for word in x.lower().split()))) \n",
    "        ,1,0))\n",
    "\n",
    "    train_set=train_set.assign(silver_pos=np.where(\n",
    "        (train_set.title_abstract_search.apply(lambda x: any(word.startswith(tuple(key_not_in.keywords)) for word in x.lower().split())))\n",
    "        ,0,train_set.silver_pos))\n",
    "\n",
    "    train_set=train_set.assign(silver_pos=np.where(\n",
    "        (train_set.title_abstract_search.str.lower().str.contains('|'.join(list(key_contains.keywords.values)))) |\n",
    "        (train_set.title_abstract_search.apply(lambda x:any(word.startswith(tuple(key_start.keywords)) for word in x.lower().split()))) |\n",
    "        (train_set.title_abstract_search.apply(lambda x:any(word in (tuple(key_in.keywords)) for word in x.lower().split()))) \n",
    "        ,1,train_set.silver_pos))\n",
    "\n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0906bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_set(train_set):\n",
    "    train_set_positive=train_set.loc[\n",
    "        (train_set.label==1)\n",
    "        ,:]\n",
    "\n",
    "    train_set_negative=train_set.loc[\n",
    "        (train_set.label==0)\n",
    "        ,:]\n",
    "\n",
    "    train_set_positive=train_set_positive.loc[:,['ID','title','abstract','title_abstract','label','year','url']]\n",
    "    train_set_negative=train_set_negative.loc[:,['ID','title','abstract','title_abstract','label','year','url']]\n",
    "\n",
    "\n",
    "    ## fix the proportion of positive and negative examples\n",
    "    obs_total=TRAIN_SET_SIZE\n",
    "    pct=POS_PROPORTION\n",
    "    pos_obs=int(obs_total*pct/100)\n",
    "    neg_obs=int(obs_total*(100-pct)/(100))\n",
    "\n",
    "    pos_obs=pos_obs-train_set.loc[(train_set.label==1)&(train_set.gold==1)].shape[0]\n",
    "    neg_obs=neg_obs-train_set.loc[(train_set.label==0)&(train_set.gold==1)].shape[0]\n",
    "\n",
    "    ## always select the gold data and sample from silver data\n",
    "    positive_gold=train_set.loc[(train_set.label==1)&(train_set.gold==1)]\n",
    "    positive_silver=train_set.loc[(train_set.label==1)&(train_set.gold==0)]\n",
    "    negative_gold=train_set.loc[(train_set.label==0)&(train_set.gold==1)]\n",
    "    negative_silver=train_set.loc[(train_set.label==0)&(train_set.gold==0)]\n",
    "\n",
    "    positive_silver_sample=positive_silver.sample(n=pos_obs,random_state=42)\n",
    "    negative_silver_sample=negative_silver.sample(n=neg_obs,random_state=42)\n",
    "    train_set_final=pd.concat([positive_gold,negative_gold,positive_silver_sample,negative_silver_sample]).sample(frac=1,random_state=42)\n",
    "\n",
    "    train_set_final=train_set_final.reset_index(drop=True)\n",
    "    train_set_final.label=train_set_final.label.apply(int)\n",
    "    return train_set_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ef6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/project/sachan/fgonzalez/ie/lib64/python3.7/site-packages/ipykernel_launcher.py:35: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_path=\"../../data/\"\n",
    "    outputs_path=\"../../outputs/\"\n",
    "    train_set=pd.read_csv(outputs_path+\"general/others_SG.csv\")\n",
    "    test_set=pd.read_csv(data_path+\"test_data/test_set_SG_annotate_5k2_gold_new_annot.csv\")\n",
    "    df_test_final=pd.read_csv(outputs_path+\"general/train_set_final.csv\")\n",
    "    ## help for filtering positive examples\n",
    "    workshops=pd.read_csv(data_path+\"others/sg_workshops_v3.csv\")\n",
    "    keywords=pd.read_csv(data_path+\"others/sg_keywords_v6.csv\")\n",
    "    ## labeled positive examples\n",
    "    acl_labeled=pd.read_csv(data_path+\"papers/acl20_long.csv\",error_bad_lines=False)\n",
    "    website_positive=pd.read_json(data_path+\"papers/papers.json\")\n",
    "\n",
    "    test_set=test_set.assign(title_clean=test_set.title.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True))\n",
    "\n",
    "    train_set=add_labeled_data(df_test_final,train_set,test_set,acl_labeled,website_positive)\n",
    "\n",
    "    train_set['title_clean']=train_set.title.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True)\n",
    "\n",
    "    train_set=train_set.assign(gold=np.where(~(train_set.label.isna()),1,0))\n",
    "\n",
    "    train_set=keyword_search(train_set,keywords)\n",
    "\n",
    "    #workshops comment to omit workshops\n",
    "    #train_set=train_set.assign(silver_pos=np.where((train_set.label.isna())&(train_set.url.str.lower().str.contains('|'.join(list(workshops.event.values)))),1,train_set.silver_pos))\n",
    "    #train_set.label.value_counts()\n",
    "\n",
    "    train_set=train_set.assign(label=np.where((train_set.gold==0),train_set.silver_pos,train_set.label))\n",
    "\n",
    "    train_set_final=create_augmented_set(train_set)\n",
    "\n",
    "    train_set_final.to_csv(outputs_path+\"sg_classifier/train_set_labeled_silver_keytitle_15pct_f.csv\",index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7650bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
