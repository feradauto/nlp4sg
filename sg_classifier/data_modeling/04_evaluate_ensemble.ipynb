{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204090f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 02:29:46.888840: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/apps/gcc-6.3.0/openblas-0.2.20-cot3cawsqf4pkxjwzjexaykbwn2ch3ii/lib:/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib\n",
      "2022-06-21 02:29:46.888881: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caff7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "    recall = recall_score(y_true=labels, y_pred=predictions)\n",
    "    precision = precision_score(y_true=labels, y_pred=predictions)\n",
    "    recall_w = recall_score(y_true=labels, y_pred=predictions,average='weighted')\n",
    "    precision_w = precision_score(y_true=labels, y_pred=predictions,average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=predictions)\n",
    "    f1_pos = f1_score(y_true=labels, y_pred=predictions,average='binary',pos_label=1)\n",
    "    f1_micro = f1_score(y_true=labels, y_pred=predictions,average='micro')\n",
    "    f1_weighted = f1_score(y_true=labels, y_pred=predictions,average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall,\n",
    "             \"precision_w\": precision_w, \"recall_w\": recall_w,\n",
    "             \"f1\": f1,\"f1_pos\": f1_pos,\n",
    "            \"f1_micro\": f1_micro,\"f1_weighted\": f1_weighted} \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8ab7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules_classifier(df_test_final,match_unique,keywords,workshops):\n",
    "    \"\"\"Rule classifier\n",
    "\n",
    "    Parameters:\n",
    "    train_set (df): Dataframe with train setpapers information\n",
    "    match_unique (df): Dataframe with all papers information\n",
    "    keywords (df):\n",
    "    workshops (df):\n",
    "    Returns:\n",
    "    dataframe with predictions\n",
    "    \"\"\"\n",
    "    keywords=keywords.assign(Keywords=np.where(keywords.Keywords=='asl',' asl ',keywords.Keywords))\n",
    "\n",
    "    df_test_final=df_test_final.merge(match_unique,on=['ID'])\n",
    "    ## percentile 99 of the cosine similarity with social needs\n",
    "    perc_99=0.222915\n",
    "\n",
    "    df_test_final=df_test_final.assign(abstract=df_test_final.abstract.fillna(''))\n",
    "\n",
    "    df_test_final=df_test_final.assign(title_abstract=df_test_final.title+\". \"+df_test_final.abstract)\n",
    "\n",
    "    df_test_final.title_abstract=df_test_final.title_abstract.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True)\n",
    "\n",
    "    df_test_final_positive=df_test_final.loc[(df_test_final.url.str.lower().str.contains('|'.join(list(workshops.Event.values))))|\n",
    "               (df_test_final.title_abstract.str.lower().str.contains('|'.join(list(keywords.Keywords.values)))) |\n",
    "               (df_test_final.cosine_similarity>=perc_99),:]\n",
    "\n",
    "    df_test_final_negative=df_test_final.loc[~((df_test_final.url.str.lower().str.contains('|'.join(list(workshops.Event.values))))|\n",
    "               (df_test_final.title_abstract.str.lower().str.contains('|'.join(list(keywords.Keywords.values)))) |\n",
    "               (df_test_final.cosine_similarity>=perc_99)),:]\n",
    "\n",
    "    df_test_final_positive=df_test_final_positive.assign(prediction=1)\n",
    "    df_test_final_negative=df_test_final_negative.assign(prediction=0)\n",
    "\n",
    "\n",
    "    df_rules=pd.concat([df_test_final_positive,df_test_final_negative])\n",
    "    return df_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861a710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_dataset(df_test_final,anthology):\n",
    "    \"\"\"Format test dataset\n",
    "\n",
    "    Parameters:\n",
    "    df_test_final (df): Test dataset \n",
    "    anthology (df): Dataframe with all papers information\n",
    "    Returns:\n",
    "    df_test_final\n",
    "    \"\"\"\n",
    "    df_test_final=df_test_final.loc[~df_test_final[\"SG_or_not (Jad's Annotation)\"].isna(),:]\n",
    "    df_test_final=df_test_final.assign(SG_or_not=np.where(df_test_final[\"Zhijing's annotation of SG_or_not\"]+\n",
    "                                       df_test_final[\"SG_or_not (Jad's Annotation)\"]>0,1,0\n",
    "                                      ))\n",
    "    anthology=anthology.assign(abstract=anthology.abstract.fillna(''))\n",
    "    anthology=anthology.assign(title_abstract=anthology.title+\". \"+anthology.abstract)\n",
    "    anthology=anthology.loc[:,['ID','title_abstract']]\n",
    "    anthology.title_abstract=anthology.title_abstract.replace(\"{\",\"\",regex=True).replace(\"}\",\"\",regex=True)\n",
    "    df_test_final=df_test_final.merge(anthology,on=['ID'])\n",
    "    df_test_final=df_test_final.loc[:,['ID','SG_or_not','title_abstract','url']].rename(columns={'SG_or_not':'label','title_abstract':'text'})\n",
    "    df_test_final=df_test_final.loc[:,['ID','text','label','url']]\n",
    "    df_test_final=df_test_final.loc[~df_test_final.label.isna()]\n",
    "    df_test_final.label=df_test_final.label.apply(int)\n",
    "    return df_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(df_results):\n",
    "    \"\"\"Get classification report\n",
    "\n",
    "    Parameters:\n",
    "    df_results: should have column label and prediction\n",
    "    \"\"\"\n",
    "    cr=classification_report(df_results.label,df_results.prediction,digits=4,output_dict=True)\n",
    "\n",
    "    cr=pd.DataFrame(cr).reset_index().rename(columns={'index':'metric'})\n",
    "\n",
    "    cr_df=pd.melt(cr,id_vars=['metric'],value_vars=['0','1','accuracy','macro avg','weighted avg'])\n",
    "\n",
    "    cr_df=cr_df.loc[cr_df.metric!=\"support\"]\n",
    "    cr_df=cr_df.loc[~((cr_df.variable==\"accuracy\") & (cr_df.metric.isin(['precision','recall'])))]\n",
    "\n",
    "    cr_df=cr_df.assign(variable=np.where(cr_df.variable=='0','negative',\n",
    "                                        np.where(cr_df.variable=='1','positive',cr_df.variable)))\n",
    "\n",
    "    cr_df=cr_df.assign(value=cr_df.value.apply(lambda x:round(x,4)*100))\n",
    "    return cr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4639d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_results(df_unused_positive,df_labeled,df_unlabeled):\n",
    "\n",
    "    total_positives=pd.concat([df_unused_positive,df_labeled.loc[df_labeled.label==1],df_unlabeled.loc[df_unlabeled.prediction==1]])\n",
    "\n",
    "    total_negatives=pd.concat([df_labeled.loc[df_labeled.label==0],df_unlabeled.loc[df_unlabeled.prediction==0]])\n",
    "\n",
    "    total_negatives=total_negatives.loc[:,['ID','title','abstract','url','year','title_abstract']]\n",
    "\n",
    "    total_negatives=total_negatives.assign(label=0)\n",
    "\n",
    "    total_positives=total_positives.loc[:,['ID','title','abstract','url','year','title_abstract']]\n",
    "\n",
    "    total_positives=total_positives.assign(label=1)\n",
    "    return total_positives,total_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4478cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df_test_final,trainer):\n",
    "    \"\"\"Get predictions\n",
    "\n",
    "    Parameters:\n",
    "    df_test_final (df): dataframe with text for predictions\n",
    "    trainer: Trainer with all the configurations\n",
    "    Returns:\n",
    "    dataset_test_final_pd\n",
    "    \"\"\"\n",
    "    dataset_test_final = Dataset.from_pandas(df_test_final)\n",
    "    tokenized_datasets_test_final = dataset_test_final.map(tokenize_function, batched=True)\n",
    "\n",
    "    test_results_final = trainer.predict(tokenized_datasets_test_final)\n",
    "    preds_final=[]\n",
    "    for e in test_results_final.predictions:\n",
    "        preds_final.append(np.array(torch.softmax(torch.Tensor(e), dim=0)))\n",
    "\n",
    "    preds_final=np.vstack(preds_final)\n",
    "    dataset_test_final_pd=tokenized_datasets_test_final.data.to_pandas()\n",
    "\n",
    "    dataset_test_final_pd=dataset_test_final_pd.assign(proba0=preds_final[:,0])\n",
    "    dataset_test_final_pd=dataset_test_final_pd.assign(proba1=preds_final[:,1])\n",
    "    dataset_test_final_pd=dataset_test_final_pd.assign(prediction=np.where(dataset_test_final_pd.proba1>0.5,1,0))\n",
    "    return dataset_test_final_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../../data/\"\n",
    "outputs_path=\"../../outputs/\"\n",
    "## READ DATA\n",
    "workshops=pd.read_csv(data_path+\"others/sg_workshops.csv\")\n",
    "keywords=pd.read_csv(data_path+\"others/sg_keywords.csv\")\n",
    "## text info of the dataset (it is more complete since it was extracted directly from the pdfs)\n",
    "anthology_test=pd.read_csv(data_path+\"test_data/papers_ack.csv\")\n",
    "## annotated test dataset\n",
    "df_test_final=pd.read_csv(data_path+\"test_data/test_set_SG_annotate_500.csv\")\n",
    "\n",
    "match_unique=pd.read_csv(outputs_path+\"general/papers_uniques.csv\")\n",
    "\n",
    "df_unused_positive=pd.read_csv(outputs_path+\"sg_classifier/unused_positive.csv\")\n",
    "\n",
    "df_unlabeled=pd.read_csv(outputs_path+\"sg_classifier/unlabeled_set.csv\")\n",
    "\n",
    "df_labeled=pd.read_csv(outputs_path+\"sg_classifier/train_set_labeled.csv\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./model/\", num_labels=2)\n",
    "\n",
    "df_unlabeled=df_unlabeled.assign(text=df_unlabeled.title_abstract)\n",
    "\n",
    "## Predict test dataset\n",
    "df_test_final=process_test_dataset(df_test_final,anthology_test)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"model_finetuned\", evaluation_strategy=\"epoch\",\n",
    "                                 per_device_train_batch_size=16,per_device_eval_batch_size=16,\n",
    "                                 seed=42,num_train_epochs=5,auto_find_batch_size=True,\n",
    "                                     do_train = False,do_predict = True)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19792b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.opyion=='0':\n",
    "    df_results=evaluate(df_test_final,trainer)\n",
    "    cr_final=get_report(df_results)\n",
    "    cr_final=cr_final.rename(columns={'value':'Fine tuned BERT'})\n",
    "    cr_final.to_csv(outputs_path+\"sg_classifier/scores_bert.csv\",index=False)\n",
    "elif args.option=='1':\n",
    "    df_results=evaluate(df_test_final,trainer)\n",
    "    df_rules=get_rules_classifier(df_test_final,match_unique,keywords,workshops)\n",
    "    df_unlabeled=evaluate(df_unlabeled,trainer)\n",
    "\n",
    "    total_positives,total_negatives=get_all_results(df_unused_positive,df_labeled,df_unlabeled)\n",
    "    total_positives.to_csv(outputs_path+\"sg_classifier/all_positive_examples.csv\",index=False)\n",
    "    total_negatives.to_csv(outputs_path+\"sg_classifier/all_negative_examples.csv\",index=False)\n",
    "\n",
    "    cr_model=get_report(df_results)\n",
    "    cr_df_rules=get_report(df_rules)\n",
    "    cr_df_rules=cr_df_rules.rename(columns={'value':'Rules classifier'})\n",
    "    cr_model=cr_model.rename(columns={'value':'Fine tuned BERT'})\n",
    "    cr_final=cr_model.merge(cr_df_rules,on=['metric','variable'])\n",
    "    cr_final.to_csv(outputs_path+\"sg_classifier/scores.csv\",index=False)\n",
    "\n",
    "print(cr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5bb0bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       425\n",
      "           1       0.67      0.50      0.57        74\n",
      "\n",
      "    accuracy                           0.89       499\n",
      "   macro avg       0.79      0.73      0.76       499\n",
      "weighted avg       0.88      0.89      0.88       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(df_rules.label,df_rules.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b13e645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       425\n",
      "           1       0.59      0.55      0.57        74\n",
      "\n",
      "    accuracy                           0.88       499\n",
      "   macro avg       0.75      0.74      0.75       499\n",
      "weighted avg       0.87      0.88      0.87       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(df_results.label,df_results.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b933b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument('-o','--option', nargs='?', help='1 for full results, 0 evaluate only test set',default='1')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b2c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49beb3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe61bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
