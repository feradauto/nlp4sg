ID,url,title,abstract,label,goal1,goal2,goal3,acknowledgments
nn-2012-organic,https://aclanthology.org/2012.eamt-1.53,Organic.Lingua - Demonstrating the Potential of a multilingual Web portal for Sustainable Agricultural \& Environmental Education,,1,Quality Education,Climate Action,Zero Hunger,
marsi-etal-2017-marine,https://aclanthology.org/E17-3023,Marine Variable Linker: Exploring Relations between Changing Variables in Marine Science Literature,"We report on a demonstration system for text mining of literature in marine science and related disciplines. It automatically extracts variables (``CO2'') involved in events of change/increase/decrease (``increasing CO2''), as well as co-occurrence and causal relations among these events (``increasing CO2 causes a decrease in pH in seawater''), resulting in a big knowledge graph. A web-based graphical user interface targeted at marine scientists facilitates searching, browsing and visualising events and their relations in an interactive way.",1,Life Below Water,,,
sarkar-etal-2020-social,https://aclanthology.org/2020.emnlp-main.109,Social Media Attributions in the Context of Water Crisis,"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of \textitattribution tie detection of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34\% on attribution detection and 81.37\% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.",1,Clean Water and Sanitation,"Peace, Justice and Strong Institutions",,
janin-portine-2017-conceptual,https://aclanthology.org/W17-7003,A conceptual ontology in the water domain of knowledge to bridge the lexical semantics of stratified discursive strata,"This paper illustrates the semantic articulation between pivot and satellite lexical units of different discursive strata in the water lexicon model supported by the French Water Academy (Lexeau project of a Bilingual lexicon of water related texts and data, to improve public awareness of water related issues and ensure a better inter-comprehension among stakeholders. The lexical treatment of the discursive unit ""water withdrawal"" into stratified entries shows the capacity of a domain ontology to set a bridge between discursive strata in different languages, making easier internal and external translations between stakeholders. With more than a hundred lexical entries tested, there is an opportunity for a consortium of realization and a project of a pilot internet application.",1,Clean Water and Sanitation,,,
bartie-etal-2016-real,https://aclanthology.org/L16-1341,The REAL Corpus: A Crowd-Sourced Corpus of Human Generated and Evaluated Spatial References to Real-World Urban Scenes,"We present a newly crowd-sourced data set of natural language references to objects anchored in complex urban scenes (In short: The REAL Corpus-Referring Expressions Anchored Language). The REAL corpus contains a collection of images of real-world urban scenes together with verbal descriptions of target objects generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions. In total, the corpus contains 32 images with on average 27 descriptions per image and 3 verifications for each description. In addition, the corpus is annotated with a variety of linguistically motivated features. The paper highlights issues posed by collecting data using crowd-sourcing with an unrestricted input format, as well as using real-world urban scenes. The corpus will be released via the ELRA repository as part of this submission.",1,Sustainable Cities and Communities,,,"This research received funding from the EPSRC projects GUI ""Generation for Uncertain Information"" (EP/L026775/1) and DILiGENt ""Domain-Independent Language Generation"""""
romberg-conrad-2021-citizen,https://aclanthology.org/2021.argmining-1.9,Citizen Involvement in Urban Planning - How Can Municipalities Be Supported in Evaluating Public Participation Processes for Mobility Transitions?,"Public participation processes allow citizens to engage in municipal decision-making processes by expressing their opinions on specific issues. Municipalities often only have limited resources to analyze a possibly large amount of textual contributions that need to be evaluated in a timely and detailed manner. Automated support for the evaluation is therefore essential, e.g. to analyze arguments. In this paper, we address (A) the identification of argumentative discourse units and (B) their classification as major position or premise in German public participation processes. The objective of our work is to make argument mining viable for use in municipalities. We compare different argument mining approaches and develop a generic model that can successfully detect argument structures in different datasets of mobility-related urban planning. We introduce a new data corpus comprising five public participation processes. In our evaluation, we achieve high macro F1 scores (0.76 - 0.80 for the identification of argumentative units; 0.86 - 0.93 for their classification) on all datasets. Additionally, we improve previous results for the classification of argumentative units on a similar German online participation dataset.",1,Sustainable Cities and Communities,"Peace, Justice and Strong Institutions",,
boschetti-etal-2020-voices,https://aclanthology.org/2020.lrec-1.114,``Voices of the Great War'': A Richly Annotated Corpus of Italian Texts on the First World War,"Voci della Grande Guerra (""Voices of the Great War"") is the first large corpus of Italian historical texts dating back to the period of First World War. This corpus differs from other existing resources in several respects. First, from the linguistic point of view it gives account of the wide range of varieties in which Italian was articulated in that period, namely from a diastratic (educated vs. uneducated writers), diaphasic (low/informal vs. high/formal registers) and diatopic (regional varieties, dialects) points of view. From the historical perspective, through a collection of texts belonging to different genres it represents different views on the war and the various styles of narrating war events and experiences. The final corpus is balanced along various dimensions, corresponding to the textual genre, the language variety used, the author type and the typology of conveyed contents. The corpus is annotated with lemmas, part-of-speech, terminology, and named entities. Significant corpus samples representative of the different ""voices"" have also been enriched with meta-linguistic and syntactic information. The layer of syntactic annotation forms the first nucleus of an Italian historical treebank complying with the Universal Dependencies standard. The paper illustrates the final resource, the methodology and tools used to build it, and the Web Interface for navigating it.",1,"Peace, Justice and Strong Institutions",Partnership for the goals,,
kacorri-huenerfauth-2015-evaluating,https://aclanthology.org/W15-5106,Evaluating a Dynamic Time Warping Based Scoring Algorithm for Facial Expressions in ASL Animations,"Advancing the automatic synthesis of linguistically accurate and natural-looking American Sign Language (ASL) animations from an easy-to-update script would increase information accessibility for many people who are deaf by facilitating more ASL content to websites and media. We are investigating the production of ASL grammatical facial expressions and head movements coordinated with the manual signs that are crucial for the interpretation of signed sentences. It would be useful for researchers to have an automatic scoring algorithm that could be used to rate the similarity of two animation sequences of ASL facial movements (or an animation sequence and a motioncapture recording of a human signer). We present a novel, sign-language specific similarity scoring algorithm, based on Dynamic Time Warping (DTW), for facial expression performances and the results of a user-study in which the predictions of this algorithm were compared to the judgments of ASL signers. We found that our algorithm had significant correlations with participants' comprehension scores for the animations and the degree to which they reported noticing specific facial expressions.",1,Reduced Inequalities,,,"This material is based upon work supported by the National Science Foundation under award number 1506786. We are grateful for assistance from Andy Cocksey, Alexis Heloir, and student researchers, including Christine Singh, Evans Seraphin, Kaushik Pillapakkam, Jennifer Marfino, Fang Yang, and Priscilla Diaz. We would like to thank Miriam Morrow and Jonathan Lamberton for their ASL linguistic expertise and assistance in recruiting participants and conducting studies."
luo-etal-2020-detecting,https://aclanthology.org/2020.findings-emnlp.296,Detecting Stance in Media On Global Warming,"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, ""Leading scientists agree that global warming is a serious concern,"" framing a clause which affirms their own stance (that global warming is serious) as an opinion endorsed ([scientists] agree) by a reputable source (leading). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: ""Mistaken scientists claim [...]."" Our work studies opinion-framing in the global warming (GW) debate, 1 an increasingly partisan issue that has received little attention in NLP. We introduce Global Warming Stance Dataset (GWSD), a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other's opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GWskeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author's own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.",1,Climate Action,"Peace, Justice and Strong Institutions",,"We thank the reviewers and the Stanford NLP Group for helpful feedback, and Adina Abeles for feedback on the demographics portion of the MTurk task."
girardi-etal-2018-patient,https://aclanthology.org/W18-5616,Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks,"We present an operational component of a real-world patient triage system. Given a specific patient presentation, the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best \textitpoint of care and \textittime to treat. We use an attention-based convolutional neural network architecture trained on 600,000 doctor notes in German. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79\% and 66\% precision, respectively, but on a confidence threshold of 0.6, precision increases to 85\% and 75\%, respectively. In addition, a method to detect \textitwarning symptoms is implemented to render the classification task transparent from a medical perspective. The method is based on the learning of attention scores and a method of automatic validation using the same data.",1,Good Health and Well-Being,,,
van-den-heuvel-oostdijk-2016-falling,https://aclanthology.org/L16-1158,"Falling silent, lost for words ... Tracing personal involvement in interviews with Dutch war veterans","In sources used in oral history research (such as interviews with eye witnesses), passages where the degree of personal emotional involvement is found to be high can be of particular interest, as these may give insight into how historical events were experienced, and what moral dilemmas and psychological or religious struggles were encountered. In a pilot study involving a large corpus of interview recordings with Dutch war veterans, we have investigated if it is possible to develop a method for automatically identifying those passages where the degree of personal emotional involvement is high. The method is based on the automatic detection of exceptionally large silences and filled pause segments (using Automatic Speech Recognition), and cues taken from specific n-grams. The first results appear to be encouraging enough for further elaboration of the method.",1,Reduced Inequalities,"Peace, Justice and Strong Institutions",,
1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5,https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5,DeSMOG: Detecting Stance in Media On Global Warming,"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, “Leading scientists agree that global warming is a serious concern,” framing a clause which affirms their own stance (“that global warming is serious”) as an opinion endorsed (""[scientists] agree”) by a reputable source (“leading”). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: “Mistaken scientists claim [...]."" Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other’s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author’s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.",1,Climate Action,"Peace, Justice and Strong Institutions",,
donaghy-1990-recognizing,https://aclanthology.org/C90-3065,"Recognizing Advice, Warnings, Promises and Threats",,1,Good Health and Well-Being,Decent Work and Economic Growth,"Peace, Justice and Strong Institutions",
jiang-etal-2017-comparing,https://aclanthology.org/W17-4205,Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation,"News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.",1,Climate Action,,,
willett-etal-2013-automatic,https://aclanthology.org/U13-1018,Automatic Climate Classification of Environmental Science Literature,"Climate type is one of the potentially most relevant pieces of metadata for identifying studies in evidence-based environmental management. In this paper, we propose a method for automatically predicting the climate type in environmental science literature using NLP techniques, relative to a pre-existing set of climate type categories. Our main approaches combine toponym detection and resolution using two different resources with support vector machines. The results show great promise, but also further challenges, for using NLP to extract information from the vast and rapidly growing collection of environmental sciences literature.",1,Climate Action,,,"NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program."
vaid-etal-2022-towards,https://aclanthology.org/2022.acl-srw.35,Towards Fine-grained Classification of Climate Change related Social Media Text,"With climate change becoming a cause of concern worldwide, it becomes essential to gauge people's reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets, ClimateStance and ClimateEng, consisting of 3777 tweets each, posted during the 2019 United Nations Framework Convention on Climate Change and comprehensively outline the dataset collection, annotation methodology, and dataset composition. Secondly, we propose the task of Climate Change prevention stance detection based on our proposed ClimateStance dataset. Thirdly, we propose a fine-grained classification based on the ClimateEng dataset, classifying social media text into five categories: Disaster, Ocean/Water, Agriculture/Forestry, Politics, and General. We benchmark both the datasets for climate change prevention stance detection and fine-grained classification using state-of-the-art methods in text classification. We also create a Reddit-based dataset for both the tasks, ClimateReddit, consisting of 6262 pseudo-labeled comments along with 329 manually annotated comments for the label. We then perform semi-supervised experiments for both the tasks and benchmark their results using the best-performing model for the supervised experiments. Lastly, we provide insights into the ClimateStance and ClimateReddit using part-of-speech tagging and named-entity recognition.",1,Climate Action,"Peace, Justice and Strong Institutions",,We would like to thank the annotators Kumar Abhishek and Shreya Chandorkar for their immensely useful contribution to this work. We would like
ruiz-etal-2016-word,https://aclanthology.org/L16-1300,More than Word Cooccurrence: Exploring Support and Opposition in International Climate Negotiations with Semantic Parsing,"Text analysis methods widely used in digital humanities often involve word co-occurrence, e.g. concept co-occurrence networks. These methods provide a useful corpus overview, but cannot determine the predicates that relate co-occurring concepts. Our goal was identifying propositions expressing the points supported or opposed by participants in international climate negotiations. Word co-occurrence methods were not sufficient, and an analysis based on open relation extraction had limited coverage for nominal predicates. We present a pipeline which identifies the points that different actors support and oppose, via a domain model with support/opposition predicates, and analysis rules that exploit the output of semantic role labelling, syntactic dependencies and anaphora resolution. Entity linking and keyphrase extraction are also performed on the propositions related to each actor. A user interface allows examining the main concepts in points supported or opposed by each participant, which participants agree or disagree with each other, and about which issues. The system is an example of tools that digital humanities scholars are asking for, to render rich textual information (beyond word co-occurrence) more amenable to quantitative treatment. An evaluation of the tool was satisfactory.",1,Climate Action,Partnership for the goals,,"We thank Tommaso Venturini, Audrey Baneyx, Kari de Pryck and Diégo Antolinos-Basso from the Sciences Po médialab in Paris for feedback on the system. Pablo Ruiz is supported through a PhD grant from Région Ile-de-France."
086e6733e5fda70bbce0c7545bd06d5634918a60,https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60,CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims,"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \textscfever framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.",1,Climate Action,"Peace, Justice and Strong Institutions",,
134c8486ae58b421681656c85bbc48dc862f6f98,https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98,ClimaText: A Dataset for Climate Change Topic Detection,"Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce \textscClimaText, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT \citedevlin2018bert can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.",1,Climate Action,,,
418b9f37b07a05b38798e97f57e17a6cc1048b92,https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92,You are right. I am ALARMED - But by Climate Change Counter Movement,"The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.",1,Climate Action,"Peace, Justice and Strong Institutions",,
998039a4876edc440e0cabb0bc42239b0eb29644,https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644,Tackling Climate Change with Machine Learning,"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.",1,Climate Action,,,
1d37460baded22f488085e82985419178679dce0,https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0,The Climate Change Debate and Natural Language Processing,"The debate around climate change (CC)-its extent, its causes, and the necessary responses-is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the ""text-as-data"" paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.",1,Climate Action,"Peace, Justice and Strong Institutions",,We thank the anonymous reviewers for detailed constructive suggestions for improving the previous version of the paper.
f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4,https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4,Learning Twitter User Sentiments on Climate Change with Limited Labeled Data,"While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users' opinions on climate change are fairly ingrained on this subset of natural disasters.",1,Climate Action,,,
a9d09338a3a0d9102c1e623e6ad434a446c0bbfe,https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe,Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism,"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We collect manual annotations of neutralised techniques used in these texts, and explore semi-supervised models to automatically classify them.",1,Climate Action,,,
bbc6486ad37365b77b9d0de8894f595d70af49ac,https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac,Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure,"We use BERT, an AI-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (CDS) market. Risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads, especially after the Paris Climate Agreement of 2015, while disclosing physical climate risks leads to a decrease in CDS spreads. These impacts are statistically and economically highly significant.",1,Climate Action,Decent Work and Economic Growth,,
52ef2bea83eba1575fcfd02c82b4228f7aab0bd6,https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6,Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures,"Disclosure of climate-related financial risks greatly helps investors assess companies' preparedness for climate change. Voluntary disclosures such as those based on the recommendations of the Task Force for Climate-related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management. We ask whether this expectation is justified. We do so with the help of a deep neural language model, which we christen ClimateBert. We train ClimateBert on thousands of sentences related to climate-risk disclosures aligned with the TCFD recommendations. In analyzing the disclosures of TCFD-supporting firms, ClimateBert comes to the sobering conclusion that the firms' TCFD support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. From our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.",1,Climate Action,Decent Work and Economic Growth,,
1778d1da6264ed4274fb94115ebbc3b4f34c9f7a,https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a,Misinfo Belief Frames: A Case Study on Covid & Climate News,"Prior beliefs of readers impact the way in which they project meaning onto news headlines. These beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. However, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. We propose Misinfo Belief Frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. We also introduce the Misinfo Belief Frames (MBF) corpus, a dataset of 66k inferences over 23.5k headlines. Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the Covid-19 pandemic and climate change. Our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers’ trust in news headlines (readers’ trust in news headlines was affected in 29.3% of cases). This demonstrates the potential effectiveness of using generated frames to counter misinformation.",1,Climate Action,Good Health and Well-Being,"Peace, Justice and Strong Institutions",
aamot-2014-literature,https://aclanthology.org/E14-3001,Literature-based discovery for Oceanographic climate science,,1,Climate Action,"Industry, Innovation and Infrastructure",,
molina-steinberger-elias-2011-criando,https://aclanthology.org/W11-4525,Criando um corpus sobre desastres clim\'aticos com apoio da ferramenta NLTK (Creating a Corpus about Climate Disasters with the Support of the NLTK Tool) [in Portuguese],,1,Climate Action,,,
schaler-2002-translation,https://aclanthology.org/2002.tc-1.4,Can Translation Companies Survive the Current Economic Climate?,,1,Decent Work and Economic Growth,,,
kumar-etal-2020-fair,https://aclanthology.org/2020.nlposs-1.5,Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings,"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.",1,Gender Equality,,,
zhao-etal-2018-gender,https://aclanthology.org/N18-2003,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,"We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.",1,Gender Equality,,,"This work was supported in part by National Science Foundation Grant IIS-1760523, two NVIDIA GPU Grants, and a Google Faculty Research Award. We would like to thank Luke Zettlemoyer, Eunsol Choi, and Mohit Iyyer for helpful discussion and feedback."
stanovsky-etal-2019-evaluating,https://aclanthology.org/P19-1164,Evaluating Gender Bias in Machine Translation,"We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., ""The doctor asked the nurse to help her in the operation""). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word ""doctor""). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender.",1,Gender Equality,,,"We would like to thank Mark Yatskar, Iz Beltagy, Tim Dettmers, Ronan Le Bras, Kyle Richardson, Ariel and Claudia Stanovsky, and Paola Virga for many insightful discussions about the role gender plays in the languages evaluated in this work, as well as the reviewers for their helpful comments."
zhao-etal-2020-gender,https://aclanthology.org/2020.acl-main.260,Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer,"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the crosslingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.",1,Gender Equality,,,"This work was supported in part by NSF Grant IIS-1927554. We would like to thank Maria De-Arteaga and Andi Peng for the helpful discussion, and thank all the reviewers for their feedback."
zhang-etal-2020-robustness,https://aclanthology.org/2020.aacl-main.76,Robustness and Reliability of Gender Bias Assessment in Word Embeddings: The Role of Base Pairs,"It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy ""man is to computer-programmer as woman is to homemaker"" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.",1,Gender Equality,,,This work was supported by the Institute of Coding which received funding from the Office for Students (OfS) in the United Kingdom.
zhou-etal-2019-examining,https://aclanthology.org/D19-1531,Examining Gender Bias in Languages with Grammatical Gender,"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches effectively reduce the gender bias while preserving the utility of the embeddings.",1,Gender Equality,,,"This work was supported in part by DARPA (HR0011-18-9-0019). Ryan Cotterell was supported by a Facebook Fellowship. We acknowledge fruitful discussions with Fred Morstatter, Nanyun Peng, Ninareh Mehrab, Shunjie Wang, Yizhou Yao, and Aram Galstyan. We also thank anonymous reviewers for their comments."
park-etal-2018-reducing,https://aclanthology.org/D18-1302,Reducing Gender Bias in Abusive Language Detection,"Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, ""You are a good woman"" was considered ""sexist"" when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios.",1,Gender Equality,,,"This work is partially funded by ITS/319/16FP of Innovation Technology Commission, HKUST, and 16248016 of Hong Kong Research Grants Council."
gonzalez-etal-2020-type,https://aclanthology.org/2020.emnlp-main.209,Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias,"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and Wino-Gender highlight model preferences that are ""hallucinatory"", e.g., disambiguating genderambiguous occurrences of 'doctor' as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of 'the doctor removed his mask' is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.",1,Gender Equality,,,"We want to thank Heather Lent, Dustin Wright and Daniel Hershcovich as well as the anonymous reviewers for their valuable feedback. This work was supported by Google Focused Research Award, as well as a performance contract allocated to the Alexandra Institute by the Danish Ministry of Higher Education and Science."
vargas-cotterell-2020-exploring,https://aclanthology.org/2020.emnlp-main.232,Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a nonlinear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).",1,Gender Equality,,,
matthews-etal-2021-gender,https://aclanthology.org/2021.trustnlp-1.6,Gender Bias in Natural Language Processing Across Human Languages,"Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. Gender bias in NLP has been well studied in English, but has been less studied in other languages. In this paper, a team including speakers of 9 languages-Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolofreports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. We develop extensions to professionlevel and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages that have grammatically gendered nouns including different feminine, masculine, and neuter profession words. We discuss future work that would benefit immensely from a computational linguistics perspective.",1,Gender Equality,,,"We'd like to thank the Clarkson Open Source Institute for their help and support with infrastructure and hosting of our experiments. We'd like to thank Golshan Madraki, Marzieh Babaeianjelodar, and Ewan Middelton for help with language translations as well as our wider team including William Smialek, Graham Northup, Cameron Weinfurt, Joshua Gordon, and Hunter Bashaw for their support."
gala-etal-2020-analyzing,https://aclanthology.org/2020.nlpcss-1.23,Analyzing Gender Bias within Narrative Tropes,"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias within a large collection of tropes. To enable our study, we crawl tvtropes.org, an online user-created repository that contains 30K tropes associated with 1.9M examples of their occurrences across film, television, and literature. We automatically score the ""genderedness"" of each trope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered topics within tropes, (2) the relationship between gender bias and popular reception, and (3) how the gender of a work's creator correlates with the types of tropes that they use.",1,Gender Equality,,,We would like to thank Jesse Thomason for his valuable advice.
terkik-etal-2016-analyzing,https://aclanthology.org/C16-1083,Analyzing Gender Bias in Student Evaluations,"University students in the United States are routinely asked to provide feedback on the quality of the instruction they have received. Such feedback is widely used by university administrators to evaluate teaching ability, despite growing evidence that students assign lower numerical scores to women and people of color, regardless of the actual quality of instruction. In this paper, we analyze students' written comments on faculty evaluation forms spanning eight years and five STEM disciplines in order to determine whether open-ended comments reflect these same biases. First, we apply sentiment analysis techniques to the corpus of comments to determine the overall affect of each comment. We then use this information, in combination with other features, to explore whether there is bias in how students describe their instructors. We show that while the gender of the evaluated instructor does not seem to affect students' expressed level of overall satisfaction with their instruction, it does strongly influence the language that they use to describe their instructors and their experience in class.",1,Gender Equality,Quality Education,,Many thanks to Ja'Nai Gray for providing manual annotations of the data used in this study and to the anonymous reviewers for their helpful feedback and suggestions.
fan-gardent-2022-generating,https://aclanthology.org/2022.acl-long.586,Generating Biographies on Wikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies,"Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoderdecoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.",1,Gender Equality,,,
dinan-etal-2020-multi,https://aclanthology.org/2020.emnlp-main.23,Multi-Dimensional Gender Bias Classification,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",1,Gender Equality,,,"Thanks to Isabelle Kloumann and our other colleagues from Responsible AI at Facebook, the audience of the NY NLP Meetup, Isaac Bleaman, Sarah Phillips, and Zeerak Waseem Butt for discussions on this topic."
jia-etal-2020-mitigating,https://aclanthology.org/2020.acl-main.264,Mitigating Gender Bias Amplification in Distribution by Posterior Regularization,"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., Zhao et al. (2017) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models' top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.",1,Gender Equality,,,Acknowledgement This work was supported in part by National Science Foundation Grant IIS-1927554. We thank anonymous reviewers and members of the UCLA-NLP lab for their feedback.
doughman-etal-2021-gender,https://aclanthology.org/2021.gebnlp-1.5,"Gender Bias in Text: Origin, Taxonomy, and Implications","Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of AI, Natural Language Processing (NLP) and Machine Learning (ML) have been shown to reflect and even amplify gender biases and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of gender bias in English text, we develop a comprehensive taxonomy that relies on the following gender bias types: Generic Pronouns, Sexism, Occupational Bias, Exclusionary Bias, and Semantics. We also provide a bottom-up overview of gender bias, from its societal origin to its spillover onto language. Finally, we link the societal implications of gender bias to their corresponding type(s) in the proposed taxonomy. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.",1,Gender Equality,,,
dinan-etal-2020-queens,https://aclanthology.org/2020.emnlp-main.656,Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chitchat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player textbased fantasy adventure dataset LIGHT (Urbanek et al., 2019), as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods-including the quantity of gendered words, a dialogue safety classifier, and human assessments-all of which show that our models generate less gendered, but equally engaging chitchat responses.",1,Gender Equality,,,"Thanks to Isabelle Kloumann, Ledell Wu, and Hila Gonen for comments and advice on this project."
ciora-etal-2021-examining,https://aclanthology.org/2021.inlg-1.7,Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models,"As Machine Translation (MT) has become increasingly more powerful, accessible, and widespread, the potential for the perpetuation of bias has grown alongside its advances. While overt indicators of bias have been studied in machine translation, we argue that covert biases expose a problem that is further entrenched. Through the use of the genderneutral language Turkish and the gendered language English, we examine cases of both overt and covert gender bias in MT models. Specifically, we introduce a method to investigate asymmetrical gender markings. We also assess bias in the attribution of personhood and examine occupational and personality stereotypes through overt bias indicators in MT models. Our work explores a deeper layer of bias in MT models and demonstrates the continued need for language-specific, interdisciplinary methodology in MT model development. * * Equal contribution. Figure 1: Using Google Translate, ""My sister is a soccer player"" accurately translates to ""My female sibling is a soccer player"" while ""My brother is a soccer player"" is translated to ""My sibling is a soccer player"". Gender is overtly marked only when the subject is female.",1,Gender Equality,,,We would like to thank Sami Iren for his assistance and insights in verifying our translation data sets for accuracy.
bartl-etal-2020-unmasking,https://aclanthology.org/2020.gebnlp-1.1,Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias,"Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically, especially in view of the current emphasis on large-scale, multilingual language models.",1,Gender Equality,,,"This work is based on the first author's master thesis, which was conducted under the ERASMUS Mundus Program Language and Communication Technologies (EMLCT). We would like to thank Rowan Hall Maudslay (Maudslay et al., 2019) and Ran Zmigrod (Zmigrod et al., 2019) for sharing their code. Moreover, we would like to thank the Center for Information Technology of the University of Groningen for providing access to the Peregrine high performance computing cluster."
gaido-etal-2021-split,https://aclanthology.org/2021.findings-acl.313,How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation,"This work is motivated by the intent to shed light on whether issues in the generation of feminine forms are also a by-product of current algorithms and techniques. In our view, architectural improvements of ST systems should also account for the trade-offs between overall translation quality and gender representation: our proposal of a model that combines two segmentation techniques is a step towards this goal. Note that technical mitigation approaches should be integrated with the long-term multidisciplinary commitment (Criado-Perez, 2019; Benjamin, 2019; D'Ignazio and Klein, 2020) necessary to radically address bias in our community. Also, we recognize the limits of working on binary gender, as we further discuss in the ethic section (§8).",1,Gender Equality,,,"This work is part of the ""End-to-end Spoken Language Translation in Rich Data Conditions"" project, 16 which is financially supported by an Amazon AWS ML Grant. The authors also wish to thank Duygu Ataman for the insightful discussions on this work."
gonen-goldberg-2019-lipstick-pig,https://aclanthology.org/W19-3621,Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between ``gender-neutralized'' words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",1,Gender Equality,,,
levy-etal-2021-collecting-large,https://aclanthology.org/2021.findings-emnlp.211,Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation,"Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com/ SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings. 1 We acknowledge that gender identity is non-binary. Throughout this work we refer to grammatical gender, which has categorical inflections in the discussed languages (e.g., masculine and feminine pronouns in English).",1,Gender Equality,,,"We thank Micah Shlain, Hillel Taub-Tabib, Shoval Sadde, and Yoav Goldberg for their help with SPIKE during our experiments, for fruitful discussions and their comments on earlier drafts of the paper, and the anonymous reviewers for their helpful comments and feedback. This work was supported in part by a research gift from the Allen Institute for AI."
chaloner-maldonado-2019-measuring,https://aclanthology.org/W19-3804,Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories,"Prior work has shown that word embeddings capture human stereotypes, including gender bias. However, there is a lack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking, biomedical and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings. We detect some gender bias in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT's hypothesis testing mechanism and find it useful for expanding the relatively small set of wellknown gender bias word categories commonly used in the literature.",1,Gender Equality,,,The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. We wish to thank our anonymous reviewers for their invaluable feedback.
escude-font-costa-jussa-2019-equalizing,https://aclanthology.org/W19-3821,Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.",1,Gender Equality,,,"This work is supported in part by the Spanish Ministerio de Economía y Competitividad, the European Regional Development Fund and the Agencia Estatal de Investigación, through the postdoctoral senior grant Ramón y Cajal, the contract TEC2015-69266-P (MINECO/FEDER,EU) and the contract PCIN-2017-079 (AEI/MINECO)."
cho-etal-2019-measuring,https://aclanthology.org/W19-3824,On Measuring Gender Bias in Translation of Gender-neutral Pronouns,"Ethics regarding social bias has recently thrown striking issues in natural language processing. Especially for gender-related topics, the need for a system that reduces the model bias has grown in areas such as image captioning, content recommendation, and automated employment. However, detection and evaluation of gender bias in the machine translation systems are not yet thoroughly investigated, for the task being cross-lingual and challenging to define. In this paper, we propose a scheme for making up a test set that evaluates the gender bias in a machine translation system, with Korean, a language with genderneutral pronouns. Three word/phrase sets are primarily constructed, each incorporating positive/negative expressions or occupations; all the terms are gender-independent or at least not biased to one side severely. Then, additional sentence lists are constructed concerning formality of the pronouns and politeness of the sentences. With the generated sentence set of size 4,236 in total, we evaluate gender bias in conventional machine translation systems utilizing the proposed measure, which is termed here as translation gender bias index (TGBI). The corpus and the code for evaluation is available on-line 1 .",1,Gender Equality,,,"This research was supported by Projects for Research and Development of Police science and Technology under Center for Research and Development of Police science and Technology and Korean National Police Agency funded by the Ministry of Science, ICT and Future Planning (PA-J000001-2017-101). Also, this work was supported by the Technology Innovation Program (10076583, Development of free-running speech recognition technologies for embedded robot system) funded by the Ministry of Trade, Industry & Energy (MOTIE, Korea). The authors appreciate helpful comments from Ye Seul Jung and Jeonghwa Cho. After all, the authors send great thanks to Seong-hun Kim for providing a rigorous proof for the boundedness of the proposed measure."
friedman-etal-2019-relating,https://aclanthology.org/W19-3803,Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis,"Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.",1,Gender Equality,,,"This research was supported by funding from the Defense Advanced Research Projects Agency (DARPA HR00111890015). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
rios-etal-2020-quantifying,https://aclanthology.org/2020.bionlp-1.1,Quantifying 60 Years of Gender Bias in Biomedical Research with Word Embeddings,"Gender bias in biomedical research can have an adverse impact on the health of real people. For example, there is evidence that heart disease-related funded research generally focuses on men. Health disparities can form between men and at-risk groups of women (i.e., elderly and low-income) if there is not an equal number of heart disease-related studies for both genders. In this paper, we study temporal bias in biomedical research articles by measuring gender differences in word embeddings. Specifically, we address multiple questions, including, How has gender bias changed over time in biomedical research, and what health-related concepts are the most biased? Overall, we find that traditional gender stereotypes have reduced over time. However, we also find that the embeddings of many medical conditions are as biased today as they were 60 years ago (e.g., concepts related to drug addiction and body dysmorphia).",1,Gender Equality,Good Health and Well-Being,,We would like to thank the anonymous reviewers for their invaluable help improving this manuscript. This material is based upon work supported by the National Science Foundation under Grant No. 1947697.
jiao-luo-2021-gender,https://aclanthology.org/2021.gebnlp-1.2,Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives,"Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people's attitudes.",1,Gender Equality,,,"Acknowledgments This project grew out of a master course project for the Fall 2020 Uppsala University 5LN714, Language Technology: Research and Development. We would like to thank Sara Stymne and Ali Basirat for some great suggestions and the anonymous reviewers for their excellent feedback."
dawkins-2021-second,https://aclanthology.org/2021.gebnlp-1.12,Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution,"We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWino-Bias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWino-Bias test set, especially in reference to the method's design and altered embedding space properties.",1,Gender Equality,,,"We thank Daniel Gillis, Judi McCuaig, Stefan Kremer, Graham Taylor, and anonymous reviewers for their time and thoughtful discussion. This work is financially supported by the Government of Canada through the NSERC CGS-D program (CGSD3-518897-2018)."
wang-etal-2021-gender,https://aclanthology.org/2021.emnlp-main.151,Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search,"Internet search affects people's cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for genderneutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pretrained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a postprocessing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO (Lin et al., 2014) and Flickr30K (Young et al., 2014) benchmarks show that our methods significantly reduce the gender bias in image search models.",1,Gender Equality,,,"The authors would like to thank anonymous reviewers for their constructive comments. This work is supported by the UC Santa Cruz Startup Funding, and the National Science Foundation (NSF) under grants IIS-2040800 and CCF-2023495."
sahlgren-olsson-2019-gender,https://aclanthology.org/W19-6104,Gender Bias in Pretrained Swedish Embeddings,"This paper investigates the presence of gender bias in pretrained Swedish embeddings. We focus on a scenario where names are matched with occupations, and we demonstrate how a number of standard pretrained embeddings handle this task. Our experiments show some significant differences between the pretrained embeddings, with word-based methods showing the most bias and contextualized language models showing the least. We also demonstrate that a previously proposed debiasing method does not affect the performance of the various embeddings in this scenario.",1,Gender Equality,,,
de-vassimon-manela-etal-2021-stereotype,https://aclanthology.org/2021.eacl-main.190,Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models,"This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice. Our code is available online.",1,Gender Equality,,,"We thank the anonymous reviewers for the insightful comments, and Hadas Orgad for helping us correcting a mistake in a previous version of this paper. This research was supported by the European Union's Horizon 2020 research and innovation programme under grant agreement no. 875160."
renduchintala-etal-2021-gender,https://aclanthology.org/2021.acl-short.15,Gender bias amplification during Speed-Quality optimization in Neural Machine Translation,"Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.",1,Gender Equality,,,
zhao-etal-2019-gender,https://aclanthology.org/N19-1064,Gender Bias in Contextualized Word Embeddings,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",1,Gender Equality,,,This work was supported in part by National Science Foundation Grant IIS-1760523. RC was supported by a Facebook Fellowship. We also acknowledge partial support from the Institute of the the Humanities and Global Cultures at the University of Virginia. We thank all reviewers for their comments.
3d505c5eff8752ac1805ef546d683bfa40aec4b1,https://www.semanticscholar.org/paper/3d505c5eff8752ac1805ef546d683bfa40aec4b1,Tie-breaker: Using language models to quantify gender bias in sports journalism,"Gender bias is an increasingly important issue in sports journalism. In this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. We also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.",1,Gender Equality,,,
5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811,https://www.semanticscholar.org/paper/5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811,"Relating Linguistic Gender Bias, Gender Values, and Gender Gaps: An International Analysis","Recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. While these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures’ survey results more accurately than either in isolation. We validate this approach using 2018 Twitter data spanning 99 countries, 18 Global Gender Gap statistics from the World Economic Forum, and 8 international survey results from the World Value Survey. Integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.",1,Gender Equality,,,
ef5fa2e95fc853defb902b58d8e4e4fe95a01c75,https://www.semanticscholar.org/paper/ef5fa2e95fc853defb902b58d8e4e4fe95a01c75,Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community,"Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors' use of gender stereotypes is associated with the community's ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.",1,Gender Equality,,,
e85a50b523915b5fba3e3f1fdb743650f7d21bed,https://www.semanticscholar.org/paper/e85a50b523915b5fba3e3f1fdb743650f7d21bed,Women’s Syntactic Resilience and Men’s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing,"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles’ authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.",1,Gender Equality,,,
c590d2c8c2fb6ce5d32ee9165ab24171165f2b70,https://www.semanticscholar.org/paper/c590d2c8c2fb6ce5d32ee9165ab24171165f2b70,Assessing gender bias in machine translation: a case study with Google Translate,"Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models—unbeknownst to their creators—grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple’s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos’ mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like “He/She is an Engineer” (where “Engineer” is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS’ data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques—which can already be found in the scientific literature.",1,Gender Equality,,,
00059087c954c1af6ece33115315e3e0ecc2f2c2,https://www.semanticscholar.org/paper/00059087c954c1af6ece33115315e3e0ecc2f2c2,Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem,"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al., 2019). Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a 'balanced' dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is 'catastrophic forgetting', which we address both in adaptation and in inference. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. During inference we propose a latticerescoring scheme which outperforms all systems evaluated in Stanovsky et al. (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of 'black box' online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",1,Gender Equality,,,This work was supported by EPSRC grants EP/M508007/1 and EP/N509620/1 and has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service 6 funded by EPSRC Tier-2 capital grant EP/P020259/1.
alshahrani-etal-2022-roadblocks,https://aclanthology.org/2022.lchange-1.15,Roadblocks in Gender Bias Measurement for Diachronic Corpora,"The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text. One important question that has been raised about word embeddings is the degree of gender bias learned from corpora. Bolukbasi et al. (2016) proposed an important technique for quantifying gender bias in word embeddings that, at its heart, is lexically based and relies on sets of highly gendered word pairs (e.g., mother/father and madam/sir) and a list of professions words (e.g., doctor and nurse). In this paper, we document problems that arise with this method to quantify gender bias in diachronic corpora. Focusing on Arabic and Chinese corpora, in particular, we document clear changes in profession words used over time and, somewhat surprisingly, even changes in the simpler gendered defining set word pairs. We further document complications in languages such as Arabic, where many words are highly polysemous/homonymous, especially female professions words.",1,Gender Equality,,,
savoldi-etal-2022-morphosyntactic,https://aclanthology.org/2022.acl-long.127,Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation,"Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS). To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews. Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques. By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.",1,Gender Equality,,,"We would like to thank the 2021 Summer Internship students at FBK for their contribution: Francesco Fernicola, Sara Giuliani, Lorena Rocio Martín, Silvia Alma Piazzolla, Mélanie Prati, Jana Waldmann. This work was made possible thanks to their extensive annotation work and active participation in fruitful discussions."
7b8318894cbeca32f1ae55780a0903445a3f4ac6,https://www.semanticscholar.org/paper/7b8318894cbeca32f1ae55780a0903445a3f4ac6,Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition,"In this paper, we study the bias in named entity recognition (NER) models---specifically, the difference in the ability to recognize male and female names as PERSON entity types. We evaluate NER models on a dataset containing 139 years of U.S. census baby names and find that relatively more female names, as opposed to male names, are not recognized as PERSON entities. The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. The data and code for the application of this benchmark is publicly available for researchers to use.",1,Gender Equality,,,
shin-etal-2020-neutralizing,https://aclanthology.org/2020.findings-emnlp.280,Neutralizing Gender Bias in Word Embeddings with Latent Disentanglement and Counterfactual Generation,"Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear subspace for debiasing, we introduce a Latent Disentanglement method with a siamese auto-encoder structure with an adapted gradient reversal layer. Our structure enables the separation of the semantic latent information and gender latent information of given word into the disjoint latent dimensions. Afterwards, we introduce a Counterfactual Generation to convert the gender information of words, so the original and the modified embeddings can produce a gender-neutralized word embedding after geometric alignment regularization, without loss of semantic information. From the various quantitative and qualitative debiasing experiments, our method shows to be better than existing debiasing methods in debiasing word embeddings. In addition, Our method shows the ability to preserve semantic information during debiasing by minimizing the semantic information losses for extrinsic NLP downstream tasks.",1,Gender Equality,,,
ramesh-etal-2021-evaluating,https://aclanthology.org/2021.gebnlp-1.3,Evaluating Gender Bias in Hindi-English Machine Translation,"With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.",1,Gender Equality,,,
liu-etal-2020-mitigating,https://aclanthology.org/2020.emnlp-main.64,Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning,"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.",1,Gender Equality,,,
wevers-2019-using,https://aclanthology.org/W19-4712,"Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990","Contemporary debates on filter bubbles and polarization in public and social media raise the question to what extent news media of the past exhibited biases. This paper specifically examines bias related to gender in six Dutch national newspapers between 1950 and 1990. We measure bias related to gender by comparing local changes in word embedding models trained on newspapers with divergent ideological backgrounds. We demonstrate clear differences in gender bias and changes within and between newspapers over time. In relation to themes such as sexuality and leisure, we see the bias moving toward women, whereas, generally, the bias shifts in the direction of men, despite growing female employment number and feminist movements. Even though Dutch society became less stratified ideologically (depillarization), we found an increasing divergence in gender bias between religious and social-democratic on the one hand and liberal newspapers on the other. Methodologically, this paper illustrates how word embeddings can be used to examine historical language change. Future work will investigate how fine-tuning deep contextualized embedding models, such as ELMO, might be used for similar tasks with greater contextual information.",1,Gender Equality,,,
field-tsvetkov-2020-unsupervised,https://aclanthology.org/2020.emnlp-main.44,Unsupervised Discovery of Implicit Gender Bias,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. 1 I love tennis! Tennis is great! Do I look ok? Bro <title>, golf is better UR hot! Me too <3 UR hot! Canada's got no game OW",1,Gender Equality,,,"We would like to thank reviewers and area chairs, as well as Vidhisha Balachandran, Amanda Coston, Xiaochuang Han, Sachin Kumar, Artidoro Pagnoni, Chan Young Park, and Shuly Wintner for their helpful feedback on this work. This material is based upon work supported by the NSF Graduate Research Fellowship Program under Grant No. DGE1745016, the Google PhD Fellowship program, NSF grants IIS1812327 and SES1926043, an Okawa Grant, and the Public Interest Technology University Network Grant No. NVF-PITU-Carnegie Mellon University-Subgrant-009246-2019-10-01. We would also like to thank Amazon for providing GPU credits. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF."
qian-etal-2019-reducing,https://aclanthology.org/P19-2031,Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function,"Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.",1,Gender Equality,,,
hall-maudslay-etal-2019-name,https://aclanthology.org/D19-1530,It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution,"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.",1,Gender Equality,,,We would like to thank Francisco Vargas Palomo for pointing out a few typos in the proofs App. A post publication.
hitti-etal-2019-proposed,https://aclanthology.org/W19-3802,Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype,"The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75%. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14%.",1,Gender Equality,,,"We would like to acknowledge the guidance of our mentors at Mila: Kris Sankaran, Dmitriy Serdyuk, and Francis Grégoire. Also, thank you to Professor Deborah Cameron and Professor Sally McConnell-Ginet for taking the time to speak with us via email and Skype. Lastly, we'd like to acknowledge the aiforsocialgood.ca summer lab, where this project was originally created."
du-etal-2021-assessing,https://aclanthology.org/2021.emnlp-main.785,Assessing the Reliability of Word Embedding Gender Bias Measures,"Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely testretest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures' reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures. 1",1,Gender Equality,,,We thank all anonymous reviewers for their constructive and helpful feedback. We also thank Anna Wegmann for the proofreading and productive discussions. This work was partially supported by the Dutch Research Council (NWO) (grant number: VI.Veni.192.130 to D. Nguyen; grant number: VI.Vidi.195.152 to D. L. Oberski).
huang-etal-2021-uncovering-implicit,https://aclanthology.org/2021.findings-emnlp.326,Uncovering Implicit Gender Bias in Narratives through Commonsense Inference,"Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly (""women can't park"") or implicitly (e.g. an unsolicited male character guides her into a parking space). We focus on implicit biases, and use a commonsense reasoning engine to uncover them. Specifically, we infer and analyze the protagonist's motivations, attributes, mental states, and implications on others. Our findings regarding implicit biases are in line with prior work that studied explicit biases, for example showing that female characters' portrayal is centered around appearance, while male figures' focus on intellect.",1,Gender Equality,,,We would like to thank the anonymous reviewers for their helpful comments.
du-etal-2022-understanding,https://aclanthology.org/2022.acl-long.98,Understanding Gender Bias in Knowledge Base Embeddings,"Knowledge base (KB) embeddings have been shown to contain gender biases (Fisher et al., 2020b). In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB? Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. Evidence of their validity is observed by comparison with real-world census data. Second, we use influence function to inspect the contribution of each triple in KB to the overall group bias. To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings.",1,Gender Equality,,,"We thank Dong Nguyen for her meticulous and valuable suggestions, as well as productive discussions. We also thank all anonymous reviewers for their constructive and helpful feedback. This research was (partially) supported by NSFC (62076097), STCSM(20511101205), and Shanghai Key Laboratory of Multidimensional Information Processing, ECNU (2020KEY001). The corresponding authors are Yuanbin Wu and Yan Yang."
schmahl-etal-2020-wikipedia,https://aclanthology.org/2020.nlpcss-1.11,Is Wikipedia succeeding in reducing gender bias? Assessing changes in gender bias in Wikipedia using word embeddings,"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.",1,Gender Equality,,,
rudinger-etal-2018-gender,https://aclanthology.org/N18-2002,Gender Bias in Coreference Resolution,"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",1,Gender Equality,,,"The authors thank Rebecca Knowles and Chandler May for their valuable feedback on this work. This research was supported by the JHU HLT-COE, DARPA AIDA, and NSF-GRFP (1232825). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government."
8417424bf9fe7a67f06f15c487403e953ab24a96,https://www.semanticscholar.org/paper/8417424bf9fe7a67f06f15c487403e953ab24a96,Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints,"Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.",1,Gender Equality,,,Acknowledgement This work was supported in part by National Science Foundation Grant IIS-1657193 and two NVIDIA Hardware Grants.
bordia-bowman-2019-identifying,https://aclanthology.org/N19-3002,Identifying and Reducing Gender Bias in Word-Level Language Models,"Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora-Penn Treebank, WikiText-2, and CNN/Daily Mail-resulting in similar conclusions.",1,Gender Equality,,,"We are grateful to Yu Wang and Jason Cramer for helping to initiate this project, to Nishant Subramani for helpful discussion, and to our reviewers for their thoughtful feedback. Bowman acknowledges support from Samsung Research."
0ab2fb6c850bd1c5882deb4984d37b4ccbee580c,https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c,Towards Understanding Gender Bias in Relation Extraction,"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards automated knowledge base construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birth date or birth place. We also analyze two existing bias mitigation techniques, word embedding debiasing and data augmentation. Unfortunately, due to NRE models relying heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in relation extraction.",1,Gender Equality,,,We thank anonymous reviewers for their helpful feedback. This material is based upon work supported in part by the National Science Foundation under IIS Grant 1927554 and Grant 1821415: Scaling the Early Research Scholars Program.
493fac37cea49afb98c52c2f5dd75c303a325b25,https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25,Mitigating Gender Bias in Natural Language Processing: Literature Review,"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP. * Equal Contribution.",1,Gender Equality,,,"We thank anonymous reviewers for their helpful feedback. We also acknowledge the thoughtful talks in related topics by Kate Crawford, Margaret Mitchell, Joanna J. Bryson, and several others. This material is based upon work supported in part by the National Science Foundation under Grants 1821415 and 1760523."
sun-peng-2021-men,https://aclanthology.org/2021.acl-short.45,"Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia","Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level. Name Wikipedia Description Loretta Young (F) Career: In 1930, when she was 17, she eloped with 26-year-old actor Grant Withers; they were married in Yuma, Arizona. The marriage was annulled the next year, just as their second movie together (ironically entitled Too Young to Marry) was released. Grant Withers (M) Personal Life: In 1930, at 26, he eloped to Yuma, Arizona with 17-year-old actress Loretta Young. The marriage ended in annulment in 1931 just as their second movie together, titled Too Young to Marry, was released .",1,Gender Equality,,,"This material is based on research supported by IARPA BETTER program via Contract No. 2019-19051600007. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA or the U.S. Government."
gupta-etal-2022-mitigating,https://aclanthology.org/2022.findings-acl.55,Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal,"Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal---modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT--2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",1,Gender Equality,,,
devinney-etal-2020-semi,https://aclanthology.org/2020.gebnlp-1.8,Semi-Supervised Topic Modeling for Gender Bias Discovery in English and Swedish,"Gender bias has been identified in many models for Natural Language Processing, stemming from implicit biases in the text corpora used to train the models. Such corpora are too large to closely analyze for biased or stereotypical content. Thus, we argue for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis. We investigate the usefulness of semi-supervised topic modeling for the detection and analysis of gender bias in three corpora (mainstream news articles in English and Swedish, and LGBTQ+ web content in English). We compare differences in topic models for three gender categories (masculine, feminine, and nonbinary or neutral) in each corpus. We find that in all corpora, genders are treated differently and that these differences tend to correspond to hegemonic ideas of gender.",1,Gender Equality,,,
basta-etal-2020-towards,https://aclanthology.org/2020.winlp-1.25,Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information,"Gender bias negatively impacts many natural language processing applications, including machine translation (MT). The motivation behind this work is to study whether recent proposed MT techniques are significantly contributing to attenuate biases in document-level and gender-balanced data. For the study, we consider approaches of adding the previous sentence and the speaker information, implemented in a decoder-based neural MT system. We show improvements both in translation quality (+1 BLEU point) as well as in gender bias mitigation on WinoMT (+5\% accuracy).",1,Gender Equality,,,
liang-etal-2020-monolingual,https://aclanthology.org/2020.coling-main.446,Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations,"Pretrained language models (PLMs) learn stereotypes held by humans and reflected in text from their training corpora, including gender bias. When PLMs are used for downstream tasks such as picking candidates for a job, people's lives can be negatively affected by these learned stereotypes. Prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. Following this line of work, we propose to use DensRay, an analytical method for obtaining interpretable dense subspaces. We show that DensRay performs on-par with prior approaches, but provide arguments that it is more robust and provide indications that it preserves language model performance better. By applying DensRay to attention heads and layers of BERT we show that gender information is spread across all attention heads and most of the layers. Also we show that DensRay can obtain gender bias scores on both token and sentence levels. Finally, we demonstrate that we can remove bias multilingually, e.g., from Chinese, using only English training data.",1,Gender Equality,,,We gratefully acknowledge funding through a Zentrum Digitalisierung.Bayern fellowship awarded to the second author. This work was supported by the European Research Council (# 740516). We thank the anonymous reviewers for valuable comments.
troles-schmid-2021-extending,https://aclanthology.org/2021.wmt-1.61,Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives,"Human gender bias is reflected in language and text production. Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT. For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System. Recent research showed that MT systems are biased towards stereotypical translation of occupations. In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published. In this set measurement of gender bias is solely based on the translation of occupations. With our paper we present an extension of this challenge set, called WiBeMT 1 , which adds gender-biased adjectives and sentences with gender-biased verbs. The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate. Results show a gender bias for all three MT systems. This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.",1,Gender Equality,,,
wang-etal-2020-double,https://aclanthology.org/2020.acl-main.484,Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation,"Word embeddings derived from humangenerated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm (Bolukbasi et al., 2016), apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double-Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.",1,Gender Equality,,,
