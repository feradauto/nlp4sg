{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21651aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a45b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../data/\"\n",
    "output_path=\"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67e1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpredicted = [json.loads(line) for line in open('../../../test_outputs/sg_papers_scirex_predictions_final_3.jsonl')]\n",
    "tdf_predicted = pd.DataFrame(tpredicted)\n",
    "salient_test = [json.loads(line) for line in open('../../../test_outputs/salient_mentions_predictions_sg_papers_scirex_final_3.jsonl')]\n",
    "df_salient_test = pd.DataFrame(salient_test)\n",
    "sclusters = [json.loads(line) for line in open('../../../test_outputs/sg_papers_scirex_cluster_predictions_final_3.jsonl')]\n",
    "tdf_sclusters = pd.DataFrame(sclusters)\n",
    "\n",
    "df_salient_test=tdf_predicted.merge(df_salient_test,on='doc_id',how='left')\n",
    "df_salient_test=df_salient_test.merge(tdf_sclusters,on='doc_id',how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5870e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8133"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tpredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3632d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salient=df_salient_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a2b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salient['Method']=[list() for _ in range(df_salient.shape[0])]\n",
    "df_salient['Task']=[list() for _ in range(df_salient.shape[0])]\n",
    "df_salient['Metric']=[list() for _ in range(df_salient.shape[0])]\n",
    "df_salient['Dataset']=[list() for _ in range(df_salient.shape[0])]\n",
    "df_salient['Material']=[list() for _ in range(df_salient.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6a1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salient=df_salient.assign(clusters=np.where(df_salient.clusters.isna(),None,df_salient.clusters))\n",
    "df_salient=df_salient.assign(saliency=np.where(df_salient.saliency.isna(),None,df_salient.saliency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00fc0f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>sections</th>\n",
       "      <th>sentences</th>\n",
       "      <th>ner</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>saliency</th>\n",
       "      <th>spans</th>\n",
       "      <th>clusters</th>\n",
       "      <th>Method</th>\n",
       "      <th>Task</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>[Breaking, the, quality]</td>\n",
       "      <td>[[0, 3]]</td>\n",
       "      <td>[[0, 3]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>farwell-1998-breaking</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[Calls, for, Participation, Calls, for, Partic...</td>\n",
       "      <td>[[0, 32]]</td>\n",
       "      <td>[[0, 32]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>nn-1984-calls-participation</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>[A, Large, Rated, Lexicon, with, French, Medic...</td>\n",
       "      <td>[[0, 234]]</td>\n",
       "      <td>[[0, 39], [39, 58], [58, 85], [85, 103], [103,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>grabar-hamon-2016-large</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>[HimL, (Health, in, my]</td>\n",
       "      <td>[[0, 4]]</td>\n",
       "      <td>[[0, 4]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>haddow-2015-himl-health</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>[``A, pessimist, sees, the, difficulty, in, ev...</td>\n",
       "      <td>[[0, 140]]</td>\n",
       "      <td>[[0, 46], [46, 64], [64, 88], [88, 114], [114,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>kumar-etal-2017-pessimist</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6940</th>\n",
       "      <td>[Bilingual, Sign, Language, Dictionary, to, Le...</td>\n",
       "      <td>[[0, 15]]</td>\n",
       "      <td>[[0, 15]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>suzuki-etal-2004-bilingual</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>[日本學生學習華語的聲調偏誤分析, :, 以二字調為例, (Tonal, errors, o...</td>\n",
       "      <td>[[0, 457]]</td>\n",
       "      <td>[[0, 389], [389, 457]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zhang-chen-2005-ri</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>[Catching, the, news, :, two, key, cases, from...</td>\n",
       "      <td>[[0, 111]]</td>\n",
       "      <td>[[0, 22], [22, 29], [29, 63], [63, 80], [80, 1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>margova-temnikova-2009-catching</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>[Building, Renewable, Language, Assets, in, Go...</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>flaherty-johanson-2016-building</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7847</th>\n",
       "      <td>[Menzerath, -, Altmann, Law, in, Syntactic, De...</td>\n",
       "      <td>[[0, 113]]</td>\n",
       "      <td>[[0, 33], [33, 49], [49, 51], [51, 66], [66, 8...</td>\n",
       "      <td>[]</td>\n",
       "      <td>macutek-etal-2017-menzerath</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  words    sections  \\\n",
       "115                            [Breaking, the, quality]    [[0, 3]]   \n",
       "199   [Calls, for, Participation, Calls, for, Partic...   [[0, 32]]   \n",
       "473   [A, Large, Rated, Lexicon, with, French, Medic...  [[0, 234]]   \n",
       "727                             [HimL, (Health, in, my]    [[0, 4]]   \n",
       "981   [``A, pessimist, sees, the, difficulty, in, ev...  [[0, 140]]   \n",
       "...                                                 ...         ...   \n",
       "6940  [Bilingual, Sign, Language, Dictionary, to, Le...   [[0, 15]]   \n",
       "7011  [日本學生學習華語的聲調偏誤分析, :, 以二字調為例, (Tonal, errors, o...  [[0, 457]]   \n",
       "7160  [Catching, the, news, :, two, key, cases, from...  [[0, 111]]   \n",
       "7320  [Building, Renewable, Language, Assets, in, Go...    [[0, 6]]   \n",
       "7847  [Menzerath, -, Altmann, Law, in, Syntactic, De...  [[0, 113]]   \n",
       "\n",
       "                                              sentences ner  \\\n",
       "115                                            [[0, 3]]  []   \n",
       "199                                           [[0, 32]]  []   \n",
       "473   [[0, 39], [39, 58], [58, 85], [85, 103], [103,...  []   \n",
       "727                                            [[0, 4]]  []   \n",
       "981   [[0, 46], [46, 64], [64, 88], [88, 114], [114,...  []   \n",
       "...                                                 ...  ..   \n",
       "6940                                          [[0, 15]]  []   \n",
       "7011                             [[0, 389], [389, 457]]  []   \n",
       "7160  [[0, 22], [22, 29], [29, 63], [63, 80], [80, 1...  []   \n",
       "7320                                           [[0, 6]]  []   \n",
       "7847  [[0, 33], [33, 49], [49, 51], [51, 66], [66, 8...  []   \n",
       "\n",
       "                               doc_id saliency spans clusters Method Task  \\\n",
       "115             farwell-1998-breaking     None   NaN     None     []   []   \n",
       "199       nn-1984-calls-participation     None   NaN     None     []   []   \n",
       "473           grabar-hamon-2016-large     None   NaN     None     []   []   \n",
       "727           haddow-2015-himl-health     None   NaN     None     []   []   \n",
       "981         kumar-etal-2017-pessimist     None   NaN     None     []   []   \n",
       "...                               ...      ...   ...      ...    ...  ...   \n",
       "6940       suzuki-etal-2004-bilingual     None   NaN     None     []   []   \n",
       "7011               zhang-chen-2005-ri     None   NaN     None     []   []   \n",
       "7160  margova-temnikova-2009-catching     None   NaN     None     []   []   \n",
       "7320  flaherty-johanson-2016-building     None   NaN     None     []   []   \n",
       "7847      macutek-etal-2017-menzerath     None   NaN     None     []   []   \n",
       "\n",
       "     Metric Dataset Material  \n",
       "115      []      []       []  \n",
       "199      []      []       []  \n",
       "473      []      []       []  \n",
       "727      []      []       []  \n",
       "981      []      []       []  \n",
       "...     ...     ...      ...  \n",
       "6940     []      []       []  \n",
       "7011     []      []       []  \n",
       "7160     []      []       []  \n",
       "7320     []      []       []  \n",
       "7847     []      []       []  \n",
       "\n",
       "[98 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salient.loc[(df_salient.saliency.isna()) & (df_salient.clusters.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsaliency=[]\n",
    "for i,row in df_salient.iterrows():\n",
    "    #print(row['ner'])\n",
    "    ner={}\n",
    "    score={}\n",
    "    saliency={}\n",
    "    doc_dict={}\n",
    "    ner_list=[]\n",
    "    \n",
    "    doc_ner={}\n",
    "    for n in row['ner']:\n",
    "        word=str.lower(' '.join(row['words'][n[0]:n[1]]))\n",
    "        if n[2] in doc_ner:\n",
    "            doc_ner[n[2]].append({'words':[word],'value':0,'top_word':word})\n",
    "        else:\n",
    "            doc_ner[n[2]]=list([{'words':[word],'value':0,'top_word':word}])\n",
    "    if (row['clusters']!=None):\n",
    "        for s,n in zip(row['saliency'],row['ner']):\n",
    "            ner[str(n[0])+\"_\"+str(n[1])]=n[2]\n",
    "            score[str(n[0])+\"_\"+str(n[1])]=s[3]\n",
    "            saliency[str(n[0])+\"_\"+str(n[1])]=s[2]\n",
    "        for k in row['clusters'].keys():\n",
    "            scores=[]\n",
    "            saliency_flag=0\n",
    "            ner_list=[]\n",
    "            word_list=[]\n",
    "            for entity in row['clusters'][k]:\n",
    "                #print((' '.join(row['words'][entity[0]:entity[1]])).lower())\n",
    "                #print(ner[str(entity[0])+\"_\"+str(entity[1])])\n",
    "                #print(score[str(entity[0])+\"_\"+str(entity[1])])\n",
    "                ner_list.append(ner[str(entity[0])+\"_\"+str(entity[1])])\n",
    "                scores.append(score[str(entity[0])+\"_\"+str(entity[1])])\n",
    "                word=(' '.join(row['words'][entity[0]:entity[1]])).lower()\n",
    "                word=re.sub('[^0-9a-zA-Z ]+', '', word)\n",
    "                word=re.sub(' +', ' ',word)\n",
    "                word_list.append(word)\n",
    "                if saliency[str(entity[0])+\"_\"+str(entity[1])]==1:\n",
    "                    saliency_flag=1\n",
    "            #print(word_list)\n",
    "            type_ent=set(ner_list).pop()\n",
    "            if (type_ent in doc_dict) and (type_ent in ['Method','Task','Metric','Dataset']):\n",
    "                doc_dict[type_ent].append({'words':word_list,'value':np.mean(scores),'top_word':word_list[scores.index(max(scores))]})\n",
    "            else:\n",
    "                doc_dict[type_ent]=list([{'words':word_list,'value':np.mean(scores),'top_word':word_list[scores.index(max(scores))]}])\n",
    "    #print(doc_ner)\n",
    "    #print()\n",
    "    #print(doc_dict)\n",
    "    #print(\"#########\")\n",
    "    \n",
    "    for key in doc_ner.keys():\n",
    "        if key not in doc_dict.keys():\n",
    "            doc_dict[key]=doc_ner[key]\n",
    "    for key in doc_dict.keys():\n",
    "        df_salient.at[i,key]=doc_dict[key]\n",
    "        \n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588bb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_task=df_salient.loc[:,['doc_id','Task','Method']].rename(columns={'doc_id':'ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa09ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_task=df_labels_task.rename(columns={'Task':'task_scirex','Method':'method_scirex'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7767b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7150, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_task.loc[(df_labels_task.method_scirex.apply(lambda x:len(x)!=0))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29e665c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task_scirex</th>\n",
       "      <th>method_scirex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>farwell-1998-breaking</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>nn-1984-calls-participation</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>grabar-hamon-2016-large</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>haddow-2015-himl-health</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>kumar-etal-2017-pessimist</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7058</th>\n",
       "      <td>price-etal-2020-six</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>margova-temnikova-2009-catching</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7231</th>\n",
       "      <td>casanellas-marg-2013-connectivity</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>flaherty-johanson-2016-building</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7847</th>\n",
       "      <td>macutek-etal-2017-menzerath</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID task_scirex method_scirex\n",
       "115               farwell-1998-breaking          []            []\n",
       "199         nn-1984-calls-participation          []            []\n",
       "473             grabar-hamon-2016-large          []            []\n",
       "727             haddow-2015-himl-health          []            []\n",
       "981           kumar-etal-2017-pessimist          []            []\n",
       "...                                 ...         ...           ...\n",
       "7058                price-etal-2020-six          []            []\n",
       "7160    margova-temnikova-2009-catching          []            []\n",
       "7231  casanellas-marg-2013-connectivity          []            []\n",
       "7320    flaherty-johanson-2016-building          []            []\n",
       "7847        macutek-etal-2017-menzerath          []            []\n",
       "\n",
       "[112 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_task.loc[(df_labels_task.method_scirex.apply(lambda x:len(x)==0)) & (df_labels_task.task_scirex.apply(lambda x:len(x)==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36fc3370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task_scirex</th>\n",
       "      <th>method_scirex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lin-etal-2006-generative</td>\n",
       "      <td>[{'words': ['natural language processing appli...</td>\n",
       "      <td>[{'words': ['generative models', 'generative a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ghosh-etal-2020-iitp</td>\n",
       "      <td>[{'words': ['offensive tweet identification', ...</td>\n",
       "      <td>[{'words': ['oth'], 'value': 0.080828607082366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grouin-2016-identification</td>\n",
       "      <td>[{'words': ['ner', 'ner', 'ner'], 'value': 0.3...</td>\n",
       "      <td>[{'words': ['machinelearning and rule based sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>volkova-etal-2014-inferring</td>\n",
       "      <td>[{'words': ['inferring user political preferen...</td>\n",
       "      <td>[{'words': ['social media personal analytics',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rayner-etal-2003-limited</td>\n",
       "      <td>[{'words': ['limited domain english to japanes...</td>\n",
       "      <td>[{'words': ['regulus 2', 'regulus 2 platform']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8128</th>\n",
       "      <td>landauer-2003-pasteurs</td>\n",
       "      <td>[{'words': ['optimal sequences of study materi...</td>\n",
       "      <td>[{'words': ['lsa', 'lsa', 'lsa', 'lsa'], 'valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8129</th>\n",
       "      <td>lee-etal-2020-counselling</td>\n",
       "      <td>[{'words': ['virtual counsellor', 'counselling...</td>\n",
       "      <td>[{'words': ['word movers distance'], 'value': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>widdows-etal-2003-unsupervised</td>\n",
       "      <td>[{'words': ['unsupervised monolingual and bili...</td>\n",
       "      <td>[{'words': ['monolingual techniques', 'bilingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8131</th>\n",
       "      <td>ye-etal-2018-interpretable</td>\n",
       "      <td>[{'words': ['text to text natural language gen...</td>\n",
       "      <td>[{'words': ['sequenceto sequence model', 'seq2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8132</th>\n",
       "      <td>otiefy-etal-2020-woli</td>\n",
       "      <td>[{'words': ['arabic offensive language identif...</td>\n",
       "      <td>[{'words': ['attention layers'], 'value': 0.16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8021 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "0           lin-etal-2006-generative   \n",
       "1               ghosh-etal-2020-iitp   \n",
       "2         grouin-2016-identification   \n",
       "3        volkova-etal-2014-inferring   \n",
       "4           rayner-etal-2003-limited   \n",
       "...                              ...   \n",
       "8128          landauer-2003-pasteurs   \n",
       "8129       lee-etal-2020-counselling   \n",
       "8130  widdows-etal-2003-unsupervised   \n",
       "8131      ye-etal-2018-interpretable   \n",
       "8132           otiefy-etal-2020-woli   \n",
       "\n",
       "                                            task_scirex  \\\n",
       "0     [{'words': ['natural language processing appli...   \n",
       "1     [{'words': ['offensive tweet identification', ...   \n",
       "2     [{'words': ['ner', 'ner', 'ner'], 'value': 0.3...   \n",
       "3     [{'words': ['inferring user political preferen...   \n",
       "4     [{'words': ['limited domain english to japanes...   \n",
       "...                                                 ...   \n",
       "8128  [{'words': ['optimal sequences of study materi...   \n",
       "8129  [{'words': ['virtual counsellor', 'counselling...   \n",
       "8130  [{'words': ['unsupervised monolingual and bili...   \n",
       "8131  [{'words': ['text to text natural language gen...   \n",
       "8132  [{'words': ['arabic offensive language identif...   \n",
       "\n",
       "                                          method_scirex  \n",
       "0     [{'words': ['generative models', 'generative a...  \n",
       "1     [{'words': ['oth'], 'value': 0.080828607082366...  \n",
       "2     [{'words': ['machinelearning and rule based sy...  \n",
       "3     [{'words': ['social media personal analytics',...  \n",
       "4     [{'words': ['regulus 2', 'regulus 2 platform']...  \n",
       "...                                                 ...  \n",
       "8128  [{'words': ['lsa', 'lsa', 'lsa', 'lsa'], 'valu...  \n",
       "8129  [{'words': ['word movers distance'], 'value': ...  \n",
       "8130  [{'words': ['monolingual techniques', 'bilingu...  \n",
       "8131  [{'words': ['sequenceto sequence model', 'seq2...  \n",
       "8132  [{'words': ['attention layers'], 'value': 0.16...  \n",
       "\n",
       "[8021 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_task.loc[(df_labels_task.method_scirex.apply(lambda x:len(x)!=0)) | (df_labels_task.task_scirex.apply(lambda x:len(x)!=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2847280b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task_scirex</th>\n",
       "      <th>method_scirex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lin-etal-2006-generative</td>\n",
       "      <td>[{'words': ['natural language processing appli...</td>\n",
       "      <td>[{'words': ['generative models', 'generative a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ghosh-etal-2020-iitp</td>\n",
       "      <td>[{'words': ['offensive tweet identification', ...</td>\n",
       "      <td>[{'words': ['oth'], 'value': 0.080828607082366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grouin-2016-identification</td>\n",
       "      <td>[{'words': ['ner', 'ner', 'ner'], 'value': 0.3...</td>\n",
       "      <td>[{'words': ['machinelearning and rule based sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>volkova-etal-2014-inferring</td>\n",
       "      <td>[{'words': ['inferring user political preferen...</td>\n",
       "      <td>[{'words': ['social media personal analytics',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rayner-etal-2003-limited</td>\n",
       "      <td>[{'words': ['limited domain english to japanes...</td>\n",
       "      <td>[{'words': ['regulus 2', 'regulus 2 platform']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8128</th>\n",
       "      <td>landauer-2003-pasteurs</td>\n",
       "      <td>[{'words': ['optimal sequences of study materi...</td>\n",
       "      <td>[{'words': ['lsa', 'lsa', 'lsa', 'lsa'], 'valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8129</th>\n",
       "      <td>lee-etal-2020-counselling</td>\n",
       "      <td>[{'words': ['virtual counsellor', 'counselling...</td>\n",
       "      <td>[{'words': ['word movers distance'], 'value': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>widdows-etal-2003-unsupervised</td>\n",
       "      <td>[{'words': ['unsupervised monolingual and bili...</td>\n",
       "      <td>[{'words': ['monolingual techniques', 'bilingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8131</th>\n",
       "      <td>ye-etal-2018-interpretable</td>\n",
       "      <td>[{'words': ['text to text natural language gen...</td>\n",
       "      <td>[{'words': ['sequenceto sequence model', 'seq2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8132</th>\n",
       "      <td>otiefy-etal-2020-woli</td>\n",
       "      <td>[{'words': ['arabic offensive language identif...</td>\n",
       "      <td>[{'words': ['attention layers'], 'value': 0.16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6944 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "0           lin-etal-2006-generative   \n",
       "1               ghosh-etal-2020-iitp   \n",
       "2         grouin-2016-identification   \n",
       "3        volkova-etal-2014-inferring   \n",
       "4           rayner-etal-2003-limited   \n",
       "...                              ...   \n",
       "8128          landauer-2003-pasteurs   \n",
       "8129       lee-etal-2020-counselling   \n",
       "8130  widdows-etal-2003-unsupervised   \n",
       "8131      ye-etal-2018-interpretable   \n",
       "8132           otiefy-etal-2020-woli   \n",
       "\n",
       "                                            task_scirex  \\\n",
       "0     [{'words': ['natural language processing appli...   \n",
       "1     [{'words': ['offensive tweet identification', ...   \n",
       "2     [{'words': ['ner', 'ner', 'ner'], 'value': 0.3...   \n",
       "3     [{'words': ['inferring user political preferen...   \n",
       "4     [{'words': ['limited domain english to japanes...   \n",
       "...                                                 ...   \n",
       "8128  [{'words': ['optimal sequences of study materi...   \n",
       "8129  [{'words': ['virtual counsellor', 'counselling...   \n",
       "8130  [{'words': ['unsupervised monolingual and bili...   \n",
       "8131  [{'words': ['text to text natural language gen...   \n",
       "8132  [{'words': ['arabic offensive language identif...   \n",
       "\n",
       "                                          method_scirex  \n",
       "0     [{'words': ['generative models', 'generative a...  \n",
       "1     [{'words': ['oth'], 'value': 0.080828607082366...  \n",
       "2     [{'words': ['machinelearning and rule based sy...  \n",
       "3     [{'words': ['social media personal analytics',...  \n",
       "4     [{'words': ['regulus 2', 'regulus 2 platform']...  \n",
       "...                                                 ...  \n",
       "8128  [{'words': ['lsa', 'lsa', 'lsa', 'lsa'], 'valu...  \n",
       "8129  [{'words': ['word movers distance'], 'value': ...  \n",
       "8130  [{'words': ['monolingual techniques', 'bilingu...  \n",
       "8131  [{'words': ['sequenceto sequence model', 'seq2...  \n",
       "8132  [{'words': ['attention layers'], 'value': 0.16...  \n",
       "\n",
       "[6944 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_task.loc[(df_labels_task.method_scirex.apply(lambda x:len(x)!=0)) & (df_labels_task.task_scirex.apply(lambda x:len(x)!=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "430087fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_task.to_json(output_path+\"sg_ie/positives_tasks_methods_clusters_final_f.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd8212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
