{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c536a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1817a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1066345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actual, predicted, k):\n",
    "    if len(predicted)==0:\n",
    "        return 0\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recall(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4407c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(test_set_labeled):\n",
    "    test_set_labeled=test_set_labeled.assign(task_annotation=test_set_labeled.task_annotation.str.lower().str.replace(\" ,\",\",\").str.replace(\", \",\",\").str.split(\",\"))\n",
    "\n",
    "    test_set_labeled=test_set_labeled.assign(method_annotation=test_set_labeled.method_annotation.str.lower().str.replace(\" ,\",\",\").str.replace(\", \",\",\").str.split(\",\"))\n",
    "\n",
    "    test_set_labeled=test_set_labeled.assign(org_annotation=test_set_labeled.org_annotation.str.lower().str.replace(\" ,\",\",\").str.replace(\", \",\",\").str.split(\",\"))\n",
    "\n",
    "    test_set_labeled=test_set_labeled.loc[:,['ID','text','task_annotation', 'method_annotation', 'org_annotation']]\n",
    "\n",
    "    return test_set_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff57629",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../data/\"\n",
    "output_path=\"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bc70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_labeled=pd.read_csv(output_path+\"general/test_set_final.csv\")\n",
    "\n",
    "methods_gpt3=pd.read_csv(output_path+\"sg_ie/gpt3/GPT3_responses_method2_cleaned_f.csv\")\n",
    "tasks_gpt3=pd.read_csv(output_path+\"sg_ie/gpt3/GPT3_responses_task_cleaned_f2.csv\")\n",
    "\n",
    "orgs=pd.read_csv(output_path+\"sg_ie/organizations_test_stanza_ontonotes_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f55bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_scirex=pd.read_json(output_path+\"sg_ie/test_PURE_tasks_methods.json\")\n",
    "#df_labels_scirex=pd.read_json(output_path+\"sg_ie/test_scirex_tasks_methods_clusters_final_f.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0998950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task_scirex</th>\n",
       "      <th>method_scirex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>miller-etal-2012-using</td>\n",
       "      <td>[knowledge - based word sense disambiguation, ...</td>\n",
       "      <td>[word overlap - based approaches, knowledge - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kim-etal-2019-unsupervised</td>\n",
       "      <td>[language modeling and parsing, language model...</td>\n",
       "      <td>[unsupervised learning of rnngs, amortized var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iyer-etal-2021-veealign</td>\n",
       "      <td>[data preparation, data integration, ontology ...</td>\n",
       "      <td>[naive domain - dependent approaches, multifac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patra-etal-2013-automatic</td>\n",
       "      <td>[automatic music mood classification, music in...</td>\n",
       "      <td>[automatic methods]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kazi-etal-2014-mitll</td>\n",
       "      <td>[english and italian asr tasks, hybrid and tan...</td>\n",
       "      <td>[neural network joint model rescoring, mitll -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>kuncham-etal-2015-statistical</td>\n",
       "      <td>[dialogue system, nlp applications, natural la...</td>\n",
       "      <td>[sss, statistical sandhi splitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>lux-etal-2020-truth</td>\n",
       "      <td>[]</td>\n",
       "      <td>[automatic summarization systems, summary eval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>islam-etal-2012-text</td>\n",
       "      <td>[readability classification, natural language ...</td>\n",
       "      <td>[readability classifier]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>wang-etal-2021-enhanced</td>\n",
       "      <td>[universal dependency parsing, enhanced univer...</td>\n",
       "      <td>[word representations, ace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>agirre-etal-2009-use</td>\n",
       "      <td>[]</td>\n",
       "      <td>[rule - based machine translation system, stat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5030 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0            miller-etal-2012-using   \n",
       "1        kim-etal-2019-unsupervised   \n",
       "2           iyer-etal-2021-veealign   \n",
       "3         patra-etal-2013-automatic   \n",
       "4              kazi-etal-2014-mitll   \n",
       "...                             ...   \n",
       "5025  kuncham-etal-2015-statistical   \n",
       "5026            lux-etal-2020-truth   \n",
       "5027           islam-etal-2012-text   \n",
       "5028        wang-etal-2021-enhanced   \n",
       "5029           agirre-etal-2009-use   \n",
       "\n",
       "                                            task_scirex  \\\n",
       "0     [knowledge - based word sense disambiguation, ...   \n",
       "1     [language modeling and parsing, language model...   \n",
       "2     [data preparation, data integration, ontology ...   \n",
       "3     [automatic music mood classification, music in...   \n",
       "4     [english and italian asr tasks, hybrid and tan...   \n",
       "...                                                 ...   \n",
       "5025  [dialogue system, nlp applications, natural la...   \n",
       "5026                                                 []   \n",
       "5027  [readability classification, natural language ...   \n",
       "5028  [universal dependency parsing, enhanced univer...   \n",
       "5029                                                 []   \n",
       "\n",
       "                                          method_scirex  \n",
       "0     [word overlap - based approaches, knowledge - ...  \n",
       "1     [unsupervised learning of rnngs, amortized var...  \n",
       "2     [naive domain - dependent approaches, multifac...  \n",
       "3                                   [automatic methods]  \n",
       "4     [neural network joint model rescoring, mitll -...  \n",
       "...                                                 ...  \n",
       "5025                 [sss, statistical sandhi splitter]  \n",
       "5026  [automatic summarization systems, summary eval...  \n",
       "5027                           [readability classifier]  \n",
       "5028                        [word representations, ace]  \n",
       "5029  [rule - based machine translation system, stat...  \n",
       "\n",
       "[5030 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_scirex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897296dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_labeled=test_set_labeled.loc[test_set_labeled.label==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f8c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_labeled=process_test_data(test_set_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66f15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_labels_scirex=df_labels_scirex.loc[:,['ID','task_scirex','method_scirex']]\n",
    "## organizations\n",
    "orgs=orgs.assign(organization=orgs.organization.str.lower())\n",
    "orgs=orgs.groupby(['ID'])['organization'].apply(list).reset_index()\n",
    "## gpt 3\n",
    "tasks_gpt3=tasks_gpt3.assign(tasks=tasks_gpt3.tasks.str.lower())\n",
    "methods_gpt3=methods_gpt3.assign(methods=methods_gpt3.methods.str.lower())\n",
    "tasks_gpt3=tasks_gpt3.groupby(['ID'])['tasks'].apply(list).reset_index()\n",
    "methods_gpt3=methods_gpt3.groupby(['ID'])['methods'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98987e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task_scirex</th>\n",
       "      <th>method_scirex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>miller-etal-2012-using</td>\n",
       "      <td>[knowledge - based word sense disambiguation, ...</td>\n",
       "      <td>[word overlap - based approaches, knowledge - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kim-etal-2019-unsupervised</td>\n",
       "      <td>[language modeling and parsing, language model...</td>\n",
       "      <td>[unsupervised learning of rnngs, amortized var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iyer-etal-2021-veealign</td>\n",
       "      <td>[data preparation, data integration, ontology ...</td>\n",
       "      <td>[naive domain - dependent approaches, multifac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patra-etal-2013-automatic</td>\n",
       "      <td>[automatic music mood classification, music in...</td>\n",
       "      <td>[automatic methods]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kazi-etal-2014-mitll</td>\n",
       "      <td>[english and italian asr tasks, hybrid and tan...</td>\n",
       "      <td>[neural network joint model rescoring, mitll -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>kuncham-etal-2015-statistical</td>\n",
       "      <td>[dialogue system, nlp applications, natural la...</td>\n",
       "      <td>[sss, statistical sandhi splitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>lux-etal-2020-truth</td>\n",
       "      <td>[]</td>\n",
       "      <td>[automatic summarization systems, summary eval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>islam-etal-2012-text</td>\n",
       "      <td>[readability classification, natural language ...</td>\n",
       "      <td>[readability classifier]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>wang-etal-2021-enhanced</td>\n",
       "      <td>[universal dependency parsing, enhanced univer...</td>\n",
       "      <td>[word representations, ace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>agirre-etal-2009-use</td>\n",
       "      <td>[]</td>\n",
       "      <td>[rule - based machine translation system, stat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5030 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0            miller-etal-2012-using   \n",
       "1        kim-etal-2019-unsupervised   \n",
       "2           iyer-etal-2021-veealign   \n",
       "3         patra-etal-2013-automatic   \n",
       "4              kazi-etal-2014-mitll   \n",
       "...                             ...   \n",
       "5025  kuncham-etal-2015-statistical   \n",
       "5026            lux-etal-2020-truth   \n",
       "5027           islam-etal-2012-text   \n",
       "5028        wang-etal-2021-enhanced   \n",
       "5029           agirre-etal-2009-use   \n",
       "\n",
       "                                            task_scirex  \\\n",
       "0     [knowledge - based word sense disambiguation, ...   \n",
       "1     [language modeling and parsing, language model...   \n",
       "2     [data preparation, data integration, ontology ...   \n",
       "3     [automatic music mood classification, music in...   \n",
       "4     [english and italian asr tasks, hybrid and tan...   \n",
       "...                                                 ...   \n",
       "5025  [dialogue system, nlp applications, natural la...   \n",
       "5026                                                 []   \n",
       "5027  [readability classification, natural language ...   \n",
       "5028  [universal dependency parsing, enhanced univer...   \n",
       "5029                                                 []   \n",
       "\n",
       "                                          method_scirex  \n",
       "0     [word overlap - based approaches, knowledge - ...  \n",
       "1     [unsupervised learning of rnngs, amortized var...  \n",
       "2     [naive domain - dependent approaches, multifac...  \n",
       "3                                   [automatic methods]  \n",
       "4     [neural network joint model rescoring, mitll -...  \n",
       "...                                                 ...  \n",
       "5025                 [sss, statistical sandhi splitter]  \n",
       "5026  [automatic summarization systems, summary eval...  \n",
       "5027                           [readability classifier]  \n",
       "5028                        [word representations, ace]  \n",
       "5029  [rule - based machine translation system, stat...  \n",
       "\n",
       "[5030 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_scirex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb1af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_labeled=test_set_labeled.loc[(~test_set_labeled.task_annotation.isna())&(~test_set_labeled.method_annotation.isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55caac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577758ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set_labeled.merge(tasks_gpt3,on=['ID'],how='left').merge(methods_gpt3,on=['ID'],how='left').merge(df_labels_scirex,on=['ID'],how='left').merge(orgs,on=['ID'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aea1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.loc[~test_set.task_scirex.isna(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acaa9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.assign(organization2=test_set.organization)\n",
    "\n",
    "test_set['organization']=[list() for _ in range(test_set.shape[0])]\n",
    "\n",
    "test_set=test_set.assign(organization=np.where(test_set.organization2.isna(),test_set.organization,test_set.organization2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "937d671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['task_scirex'] = test_set['task_scirex'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "\n",
    "test_set['method_scirex'] = test_set['method_scirex'].apply(lambda d: d if isinstance(d, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2e8e6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>task_annotation</th>\n",
       "      <th>method_annotation</th>\n",
       "      <th>org_annotation</th>\n",
       "      <th>tasks</th>\n",
       "      <th>methods</th>\n",
       "      <th>task_scirex</th>\n",
       "      <th>method_scirex</th>\n",
       "      <th>organization</th>\n",
       "      <th>organization2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wandji-tchami-grabar-2014-towards</td>\n",
       "      <td>Towards Automatic Distinction between Speciali...</td>\n",
       "      <td>[contrastive automatic analysis of verbs]</td>\n",
       "      <td>[semantic annotation]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[semantic annotation, part-of-speech tagging, ...</td>\n",
       "      <td>[semantic annotation, contrastive analysis, me...</td>\n",
       "      <td>[contrastive automatic analysis of verbs, medi...</td>\n",
       "      <td>[semantic annotation]</td>\n",
       "      <td>[anr, dga]</td>\n",
       "      <td>[anr, dga]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soh-etal-2019-legal</td>\n",
       "      <td>Legal Area Classification: A Comparative Study...</td>\n",
       "      <td>[legal area classification]</td>\n",
       "      <td>[topic model, word embedding, language model]</td>\n",
       "      <td>[singapore academy of law, singapore supreme c...</td>\n",
       "      <td>[text classification, topic modeling, word emb...</td>\n",
       "      <td>[topic modeling, word embedding, language mode...</td>\n",
       "      <td>[legal area classification]</td>\n",
       "      <td>[text classifiers, machine learning (\"ml\") app...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huang-bai-2021-team</td>\n",
       "      <td>TEAM HUB@LT-EDI-EACL2021: Hope Speech Detectio...</td>\n",
       "      <td>[hope speech detection, text classification]</td>\n",
       "      <td>[language model, xlm-roberta, tf-idf]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[text classification, pre-trained language model]</td>\n",
       "      <td>[xlm-roberta, tf-idf]</td>\n",
       "      <td>[text classification, hope speech detection, v...</td>\n",
       "      <td>[xlm - roberta pre - trained language model]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sikdar-etal-2018-flytxt</td>\n",
       "      <td>Flytxt\\_NTNU at SemEval-2018 Task 8: Identifyi...</td>\n",
       "      <td>[identifying and classifying malware text]</td>\n",
       "      <td>[supervised learning, conditional random fields]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[identifying malware sentences, classifying ma...</td>\n",
       "      <td>[conditional random fields, naive bayes classi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[supervised learning approach, na\\\"\\ive bayes ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rio-2002-compiling</td>\n",
       "      <td>Compiling an Interactive Literary Translation ...</td>\n",
       "      <td>[compiling an interactive literary translation...</td>\n",
       "      <td>[web site]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[translation, interpretation, debate]</td>\n",
       "      <td>[web resources, english-spanish literary trans...</td>\n",
       "      <td>[higher education, literary translation]</td>\n",
       "      <td>[course methodology]</td>\n",
       "      <td>[uma, dev, sci]</td>\n",
       "      <td>[uma, dev, sci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yang-heeman-2007-avoiding</td>\n",
       "      <td>Avoiding and Resolving Initiative Conflicts in...</td>\n",
       "      <td>[avoiding and resolving initiative conflicts]</td>\n",
       "      <td>[empirical study]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[dialogue act recognition, dialogue act classi...</td>\n",
       "      <td>[empirical study, corpus analysis, linguistic ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[linguistic devices]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>del-tredici-fernandez-2020-words</td>\n",
       "      <td>Words are the Window to the Soul: Language-bas...</td>\n",
       "      <td>[fake news detection]</td>\n",
       "      <td>[language-based user representations]</td>\n",
       "      <td>[netherlands organisation for scientific resea...</td>\n",
       "      <td>[fake news detection, language-based user repr...</td>\n",
       "      <td>[text representation, text classification, soc...</td>\n",
       "      <td>[fake news detection, language of fake news sp...</td>\n",
       "      <td>[language - based user representations]</td>\n",
       "      <td>[netherlands organisation for scientific resea...</td>\n",
       "      <td>[netherlands organisation for scientific resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>akhlaghi-etal-2020-constructing</td>\n",
       "      <td>Constructing Multimodal Language Learner Texts...</td>\n",
       "      <td>[conversion of plain texts]</td>\n",
       "      <td>[learning and reading assistant]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text annotation, audio recording, language le...</td>\n",
       "      <td>[text tagging, audio recording, annotation]</td>\n",
       "      <td>[conversion task]</td>\n",
       "      <td>[crowdsourcing techniques, lara]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stajner-etal-2017-effects</td>\n",
       "      <td>Effects of Lexical Properties on Viewing Time ...</td>\n",
       "      <td>[eye-tracking]</td>\n",
       "      <td>[lexical properties,  parallel gaze data]</td>\n",
       "      <td>[university of mannheim political economy of r...</td>\n",
       "      <td>[text classification, text processing, text un...</td>\n",
       "      <td>[lexical analysis, online processing, gaze-bas...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[online processing techniques]</td>\n",
       "      <td>[political economy of reforms, university of m...</td>\n",
       "      <td>[political economy of reforms, university of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>du-etal-2019-extracting</td>\n",
       "      <td>Extracting Symptoms and their Status from Clin...</td>\n",
       "      <td>[extracting symptoms]</td>\n",
       "      <td>[curriculum learning, sequence-to-sequence, hi...</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[text classification, sequence labeling, seque...</td>\n",
       "      <td>[hierarchical span-attribute tagging, sequence...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[deep learning approaches, curriculum learning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>guo-etal-2021-pre</td>\n",
       "      <td>Pre-trained Transformer-based Classification a...</td>\n",
       "      <td>[span detection, classification]</td>\n",
       "      <td>[pre-trained transformer, classifier ensembling]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[classification, span detection]</td>\n",
       "      <td>[pre-trained transformer-based models, oversam...</td>\n",
       "      <td>[classification, classification tasks, social ...</td>\n",
       "      <td>[oversampling, span detection models, classifi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nguyen-2019-question</td>\n",
       "      <td>Question Answering in the Biomedical Domain. Q...</td>\n",
       "      <td>[question answering]</td>\n",
       "      <td>[question answering system]</td>\n",
       "      <td>[australian research training program, csiro]</td>\n",
       "      <td>[question answering, information retrieval, te...</td>\n",
       "      <td>[lexical gap, quality of answer spans, limited...</td>\n",
       "      <td>[question answering, patient question answering]</td>\n",
       "      <td>[question answering system, question answering...</td>\n",
       "      <td>[australian research training program]</td>\n",
       "      <td>[australian research training program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>polat-saraclar-2020-unsupervised</td>\n",
       "      <td>Unsupervised Term Discovery for Continuous Sig...</td>\n",
       "      <td>[sign language recognition]</td>\n",
       "      <td>[unsupervised term discovery]</td>\n",
       "      <td>[scientific and technological research council...</td>\n",
       "      <td>[unsupervised term discovery, sign language re...</td>\n",
       "      <td>[unsupervised learning, spoken term discovery,...</td>\n",
       "      <td>[annotation process, sign language processing,...</td>\n",
       "      <td>[sign language recognition (slr) systems]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bagheri-garakani-etal-2022-improving</td>\n",
       "      <td>Improving Relevance Quality in Product Search ...</td>\n",
       "      <td>[relevance quality in product search]</td>\n",
       "      <td>[bert]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[semantic similarity, ranking]</td>\n",
       "      <td>[cross-encoding, bert, semantic similarity]</td>\n",
       "      <td>[estimating relevance quality impact, ranking ...</td>\n",
       "      <td>[high - precision crossencoder bert model, hig...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doddington-1989-initial</td>\n",
       "      <td>Initial Draft Guidelines for the Development o...</td>\n",
       "      <td>[spoken language systems]</td>\n",
       "      <td>[guidelines]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[database creation, speech research]</td>\n",
       "      <td>[tokenization, part-of-speech tagging, parsing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[darpa]</td>\n",
       "      <td>[darpa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>maegaard-etal-2008-medar</td>\n",
       "      <td>MEDAR: Collaboration between European and Medi...</td>\n",
       "      <td>[machine translation, information retrieval]</td>\n",
       "      <td>[surveys, questionnaires]</td>\n",
       "      <td>[european comission]</td>\n",
       "      <td>[machine translation, information retrieval, l...</td>\n",
       "      <td>[machine translation, information retrieval, b...</td>\n",
       "      <td>[machine translation, information retrieval]</td>\n",
       "      <td>[human language technologies, language technol...</td>\n",
       "      <td>[european commission, nemlar, medar, medar, eu...</td>\n",
       "      <td>[european commission, nemlar, medar, medar, eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>flekova-etal-2016-exploring</td>\n",
       "      <td>Exploring Stylistic Variation with Age and Inc...</td>\n",
       "      <td>[exploring stylistic variation]</td>\n",
       "      <td>[regression, stylistic features]</td>\n",
       "      <td>[templeton religion trust]</td>\n",
       "      <td>[stylistic variation, syntactic features, pred...</td>\n",
       "      <td>[stylistic variation, syntactic features, writ...</td>\n",
       "      <td>[exploring stylistic variation, social media, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[templeton religion trust]</td>\n",
       "      <td>[templeton religion trust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hieu-etal-2020-reintel</td>\n",
       "      <td>ReINTEL Challenge 2020: Vietnamese Fake News D...</td>\n",
       "      <td>[fake news detection]</td>\n",
       "      <td>[phobert embeddings, ensemble method]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[fake news detection, linguistic feature extra...</td>\n",
       "      <td>[phobert, ensemble model, linguistic features]</td>\n",
       "      <td>[vietnamese fake news detection, fake news det...</td>\n",
       "      <td>[ensemble method]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>da-san-martino-etal-2020-prta</td>\n",
       "      <td>Prta: A System to Support the Analysis of Prop...</td>\n",
       "      <td>[analysis of propaganda techniques]</td>\n",
       "      <td>[propaganda persuasion techniques analyzer]</td>\n",
       "      <td>[qatar computing research institute, mit-csail]</td>\n",
       "      <td>[information retrieval, text classification, t...</td>\n",
       "      <td>[tokenization, part-of-speech tagging, depende...</td>\n",
       "      <td>[factchecking, online disinformation, analysis...</td>\n",
       "      <td>[prta, rhetorical and psychological techniques...</td>\n",
       "      <td>[propaganda analysis project 7, qatar computin...</td>\n",
       "      <td>[propaganda analysis project 7, qatar computin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>akasaki-kaji-2019-conversation</td>\n",
       "      <td>Conversation Initiation by Diverse News Conten...</td>\n",
       "      <td>[conversation systems]</td>\n",
       "      <td>[information retrieval, generation models]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[conversation initiation, information retrieva...</td>\n",
       "      <td>[information retrieval, generation, crowd-sour...</td>\n",
       "      <td>[conversation systems, automatic and manual ev...</td>\n",
       "      <td>[conversation systems]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tang-shen-2020-categorizing</td>\n",
       "      <td>Categorizing Offensive Language in Social Netw...</td>\n",
       "      <td>[categorizing offensive language]</td>\n",
       "      <td>[hierarchical attention capsule network, integ...</td>\n",
       "      <td>[national language commission key research pro...</td>\n",
       "      <td>[text classification, offensive language detec...</td>\n",
       "      <td>[text classification, hierarchical attention, ...</td>\n",
       "      <td>[offensive classification, automatically ident...</td>\n",
       "      <td>[explanation tool, capsule system, hierarchica...</td>\n",
       "      <td>[national language commission key research pro...</td>\n",
       "      <td>[national language commission key research pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pergola-etal-2021-boosting</td>\n",
       "      <td>Boosting Low-Resource Biomedical QA via Entity...</td>\n",
       "      <td>[question answering, domain adaptaion, transfe...</td>\n",
       "      <td>[masked language models, language models, enti...</td>\n",
       "      <td>[uk research and innovation, epsrc]</td>\n",
       "      <td>[question answering, information retrieval, te...</td>\n",
       "      <td>[masked language models, entity-aware masking,...</td>\n",
       "      <td>[domain adaptation, biomedical question - answ...</td>\n",
       "      <td>[masked lms, masked language models, neural ar...</td>\n",
       "      <td>[epsrc, yh, a turing ai fellowship, uk researc...</td>\n",
       "      <td>[epsrc, yh, a turing ai fellowship, uk researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kim-park-2015-statistical</td>\n",
       "      <td>A Statistical Modeling of the Correlation betw...</td>\n",
       "      <td>[statistical modeling]</td>\n",
       "      <td>[grammatical analysis, working-memory-based pr...</td>\n",
       "      <td>[national research foundation of korea]</td>\n",
       "      <td>[syntax, working-memory]</td>\n",
       "      <td>[statistical modeling, working-memory capacity...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[l2 learners, wm - based processing analysis]</td>\n",
       "      <td>[national research foundation of korea grant, ...</td>\n",
       "      <td>[national research foundation of korea grant, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>salawu-etal-2021-large</td>\n",
       "      <td>A Large-Scale English Multi-Label Twitter Data...</td>\n",
       "      <td>[online abuse detection]</td>\n",
       "      <td>[transformers, dataset]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[cyberbullying detection, online abuse detecti...</td>\n",
       "      <td>[data collection, data analysis, deep learning]</td>\n",
       "      <td>[cyberbullying]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>finlayson-etal-2014-n2</td>\n",
       "      <td>The N2 corpus: A semantically annotated collec...</td>\n",
       "      <td>[corpus]</td>\n",
       "      <td>[multi-layed annotation, annotation procedure]</td>\n",
       "      <td>[u.s. defense advanced research project agency...</td>\n",
       "      <td>[text classification, information extraction, ...</td>\n",
       "      <td>[syntax, semantics, co-reference, events, time...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[u.s. defense advanced research project agency...</td>\n",
       "      <td>[u.s. defense advanced research project agency...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>shreevastava-foltz-2021-detecting</td>\n",
       "      <td>Detecting Cognitive Distortions from Patient-T...</td>\n",
       "      <td>[cognitive distortion detection]</td>\n",
       "      <td>[pretrained sentence-bert embeddings, svm clas...</td>\n",
       "      <td>[university of colorado, boulder computational...</td>\n",
       "      <td>[text classification, feature engineering, dat...</td>\n",
       "      <td>[text classification, feature engineering, nat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[classification algorithms, natural language p...</td>\n",
       "      <td>[computational linguistics, search and informa...</td>\n",
       "      <td>[computational linguistics, search and informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kirk-etal-2021-memes</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>[assessing dataset generalizability, hateful m...</td>\n",
       "      <td>[ocr, multimodal models]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[text classification, multimodal classification]</td>\n",
       "      <td>[ocr, multimodal models]</td>\n",
       "      <td>[hateful meme detection, detecting real world ...</td>\n",
       "      <td>[multimodal models, machine learning systems, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>marinelli-etal-2008-encoding</td>\n",
       "      <td>Encoding Terms from a Scientific Domain in a T...</td>\n",
       "      <td>[terminological database]</td>\n",
       "      <td>[encoding terms]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[terminology extraction, term selection, synon...</td>\n",
       "      <td>[term selection and extraction, synset compari...</td>\n",
       "      <td>[extraction]</td>\n",
       "      <td>[iwn model, eurowordnet/italwordnet model]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>oard-2007-invited</td>\n",
       "      <td>Invited Talk: Lessons from the MALACH Project:...</td>\n",
       "      <td>[intellectual access to large oral history col...</td>\n",
       "      <td>[automated clustering, automatic speech recogn...</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[automatic speech recognition, boundary detect...</td>\n",
       "      <td>[automatic speech recognition, machine learnin...</td>\n",
       "      <td>[boundary detection, interactive search, topic...</td>\n",
       "      <td>[automatic speech recognition techniques, mach...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>berzak-etal-2015-contrastive</td>\n",
       "      <td>Contrastive Analysis with Predictive Power: Ty...</td>\n",
       "      <td>[contrastive analysis, typology driven estimat...</td>\n",
       "      <td>[bootstrapping]</td>\n",
       "      <td>[center for brains, minds, and machines]</td>\n",
       "      <td>[computational linguistics, contrastive analys...</td>\n",
       "      <td>[contrastive analysis, predictive power, typol...</td>\n",
       "      <td>[linguistic inquiry]</td>\n",
       "      <td>[contrastive analysis, bootstrapping approach,...</td>\n",
       "      <td>[center for brains, minds, machines, cbmm, nsf...</td>\n",
       "      <td>[center for brains, minds, machines, cbmm, nsf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>huang-bai-2021-hub</td>\n",
       "      <td>HUB@DravidianLangTech-EACL2021: Identify and C...</td>\n",
       "      <td>[offensive language identification]</td>\n",
       "      <td>[multilingual bert]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[offensive language identification, code-mixin...</td>\n",
       "      <td>[bert, multilingualism, code-mixing]</td>\n",
       "      <td>[comment/post - level classification tasks, of...</td>\n",
       "      <td>[multilingual bert model, fine - tuning methods]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pyysalo-etal-2007-unification</td>\n",
       "      <td>On the unification of syntactic annotations un...</td>\n",
       "      <td>[biomedical information extraction]</td>\n",
       "      <td>[dependency schemes, bioinfer, genia treebank]</td>\n",
       "      <td>[academy of finland]</td>\n",
       "      <td>[parsing, syntactic annotation, information ex...</td>\n",
       "      <td>[parsing, dependency grammar, treebank]</td>\n",
       "      <td>[biomedical information extraction, informatio...</td>\n",
       "      <td>[unifying syntax formalism, syntactic annotati...</td>\n",
       "      <td>[lll, academy of finland]</td>\n",
       "      <td>[lll, academy of finland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>shen-etal-2013-participant</td>\n",
       "      <td>A Participant-based Approach for Event Summari...</td>\n",
       "      <td>[event summarization]</td>\n",
       "      <td>[participant-based approach]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[event summarization, tweet classification]</td>\n",
       "      <td>[twitter event summarization, mixture model, b...</td>\n",
       "      <td>[event summarization, summarizing the twitter ...</td>\n",
       "      <td>[participantbased approach, mixture model, par...</td>\n",
       "      <td>[bosch research and technology center, natural...</td>\n",
       "      <td>[bosch research and technology center, natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>li-etal-2020-adviser</td>\n",
       "      <td>ADVISER: A Toolkit for Developing Multi-modal,...</td>\n",
       "      <td>[multi-domain dialog system]</td>\n",
       "      <td>[python]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[dialog system, speech recognition, text recog...</td>\n",
       "      <td>[speech recognition, text processing, vision p...</td>\n",
       "      <td>[collaborative research, emotion recognition]</td>\n",
       "      <td>[adviser 1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cardellino-etal-2017-legal</td>\n",
       "      <td>Legal NERC with ontologies, Wikipedia and curr...</td>\n",
       "      <td>[legal domain ontology]</td>\n",
       "      <td>[named entity recognizer, classifier]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[named entity recognition, text classification...</td>\n",
       "      <td>[named entity recognition, classifier, curricu...</td>\n",
       "      <td>[legal nerc, human annotation, curriculum lear...</td>\n",
       "      <td>[wikipediabased approach, lkif, curriculum lea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>neubig-etal-2011-safety</td>\n",
       "      <td>Safety Information Mining --- What can NLP do ...</td>\n",
       "      <td>[information mining]</td>\n",
       "      <td>[robust and effective systems]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[word segmentation, named entity recognition, ...</td>\n",
       "      <td>[word segmentation, named entity recognition, ...</td>\n",
       "      <td>[safety information mining, relief efforts, wo...</td>\n",
       "      <td>[nlp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>trajanovski-etal-2021-text</td>\n",
       "      <td>When does text prediction benefit from additio...</td>\n",
       "      <td>[text prediction]</td>\n",
       "      <td>[large language models]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text prediction, phrase completion, language ...</td>\n",
       "      <td>[text prediction, language modeling, contextua...</td>\n",
       "      <td>[text prediction, chat scenarios, real - time ...</td>\n",
       "      <td>[text prediction algorithms, large language mo...</td>\n",
       "      <td>[microsoft search, intelligence, msai]</td>\n",
       "      <td>[microsoft search, intelligence, msai]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>kulkarni-etal-2018-annotated</td>\n",
       "      <td>An Annotated Corpus for Machine Reading of Ins...</td>\n",
       "      <td>[conversion of protocols into a machine-readab...</td>\n",
       "      <td>[annotated corpus]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[shallow semantic parsing, natural language pr...</td>\n",
       "      <td>[shallow semantic parsing, machine learning, n...</td>\n",
       "      <td>[biological research]</td>\n",
       "      <td>[machine learning approaches]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>mayfield-black-2019-stance</td>\n",
       "      <td>Stance Classification, Outcome Prediction, and...</td>\n",
       "      <td>[stance classification, outcome prediction, an...</td>\n",
       "      <td>[corpus, bert]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stance classification, outcome prediction, im...</td>\n",
       "      <td>[stance classification, outcome prediction, im...</td>\n",
       "      <td>[group decision - making, nlp tasks, outcome p...</td>\n",
       "      <td>[bert contextualized word embeddings, language...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ma-etal-2017-detect</td>\n",
       "      <td>Detect Rumors in Microblog Posts Using Propaga...</td>\n",
       "      <td>[identifying rumors]</td>\n",
       "      <td>[propogation trees, kernel-based method]</td>\n",
       "      <td>[general research fund of hong kong]</td>\n",
       "      <td>[identifying rumors, modeling microblog posts ...</td>\n",
       "      <td>[kernel learning, tree-based models, propagati...</td>\n",
       "      <td>[identifying rumors, microblog posts diffusion]</td>\n",
       "      <td>[rumor detection models, propagation tree kern...</td>\n",
       "      <td>[general research fund]</td>\n",
       "      <td>[general research fund]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>hopkins-etal-2017-beyond</td>\n",
       "      <td>Beyond Sentential Semantic Parsing: Tackling t...</td>\n",
       "      <td>[semantic parsing]</td>\n",
       "      <td>[tree transducer cascade]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[semantic parsing, coreference resolution, ver...</td>\n",
       "      <td>[tree transducers, coreference resolution, ver...</td>\n",
       "      <td>[sat algebra word problems, verb interpretation)]</td>\n",
       "      <td>[tree transducer cascade, cascade of tree tran...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>asgari-etal-2020-topic</td>\n",
       "      <td>Topic-Based Measures of Conversation for Detec...</td>\n",
       "      <td>[detecting mild cognitiveimpairment]</td>\n",
       "      <td>[lexical coherence of consecutive utterances]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[topic detection, lexical coherence, conversat...</td>\n",
       "      <td>[lexical coherence, topic modeling, conversati...</td>\n",
       "      <td>[detecting mild cognitiveimpairment, conversat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[oregon roybal center for aging and technology...</td>\n",
       "      <td>[oregon roybal center for aging and technology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>alexeeva-etal-2020-mathalign</td>\n",
       "      <td>MathAlign: Linking Formula Identifiers to thei...</td>\n",
       "      <td>[machine reading]</td>\n",
       "      <td>[rule-based approach]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[information retrieval, accessibility]</td>\n",
       "      <td>[rule-based approach, natural language process...</td>\n",
       "      <td>[mathematical information retrieval]</td>\n",
       "      <td>[machine reading approaches]</td>\n",
       "      <td>[defense advanced research projects agency, da...</td>\n",
       "      <td>[defense advanced research projects agency, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>barreiro-cabral-2009-reescreve</td>\n",
       "      <td>ReEscreve: a Translator-friendly Multi-purpose...</td>\n",
       "      <td>[paraphrasing]</td>\n",
       "      <td>[multi-purpose paraphrasing software tool]</td>\n",
       "      <td>[linguateca (portugese government and european...</td>\n",
       "      <td>[text generation, text summarization, text sim...</td>\n",
       "      <td>[tokenization, part-of-speech tagging, parsing]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[reescreve]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>moore-etal-1997-commandtalk</td>\n",
       "      <td>CommandTalk: A Spoken-Language Interface for B...</td>\n",
       "      <td>[battlefield simulations]</td>\n",
       "      <td>[nuance speech recognition system, gemini natu...</td>\n",
       "      <td>[defense advanced research projects agency, na...</td>\n",
       "      <td>[speech recognition, natural language parsing,...</td>\n",
       "      <td>[speech recognition, natural language parsing,...</td>\n",
       "      <td>[battlefield simulations]</td>\n",
       "      <td>[open agent architecture, modsaf battlefield s...</td>\n",
       "      <td>[nuance, government, darpa]</td>\n",
       "      <td>[nuance, government, darpa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>chakraborti-tendulkar-2013-parallels</td>\n",
       "      <td>Parallels between Linguistics and Biology. In ...</td>\n",
       "      <td>[parallels between linguistics and biology]</td>\n",
       "      <td>[parallel construction, analogies]</td>\n",
       "      <td>[government of india department of biotechnology]</td>\n",
       "      <td>[cross-disciplinary research, new research ave...</td>\n",
       "      <td>[method1, method2, method3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[avt, iyba, department of biotechnology, gover...</td>\n",
       "      <td>[avt, iyba, department of biotechnology, gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>li-hovy-2014-sentiment</td>\n",
       "      <td>Sentiment Analysis on the People's Daily. We p...</td>\n",
       "      <td>[sentiment analysis]</td>\n",
       "      <td>[semi-supervised bootstrapping algorithm]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sentiment target clustering, subjective lexic...</td>\n",
       "      <td>[semi-supervised bootstrapping, hierarchical b...</td>\n",
       "      <td>[subjective lexicons extraction, sentiment pre...</td>\n",
       "      <td>[semi - supervised bootstrapping algorithm, bo...</td>\n",
       "      <td>[emnlp]</td>\n",
       "      <td>[emnlp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>wang-etal-2019-bigodm</td>\n",
       "      <td>BIGODM System in the Social Media Mining for H...</td>\n",
       "      <td>[social media mining]</td>\n",
       "      <td>[support vector machines, word embedding, line...</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[classification, information extraction, socia...</td>\n",
       "      <td>[vote-based undersampling, linear support vect...</td>\n",
       "      <td>[adr classification task]</td>\n",
       "      <td>[ensemble, linear support vector machine, vote...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sotnikova-etal-2021-analyzing</td>\n",
       "      <td>Analyzing Stereotypes in Generative Text Infer...</td>\n",
       "      <td>[analyzing stereotypes]</td>\n",
       "      <td>[annotation, human judgement]</td>\n",
       "      <td>[university of maryland clip lab]</td>\n",
       "      <td>[stereotype identification, natural language i...</td>\n",
       "      <td>[stereotype analysis, natural language inferen...</td>\n",
       "      <td>[analyzing stereotypes, generative text infere...</td>\n",
       "      <td>[nlp systems]</td>\n",
       "      <td>[clip, university of maryland]</td>\n",
       "      <td>[clip, university of maryland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>muti-barron-cedeno-2022-checkpoint</td>\n",
       "      <td>A Checkpoint on Multilingual Misogyny Identifi...</td>\n",
       "      <td>[misogyny identification]</td>\n",
       "      <td>[transformers, bert]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[identifying misogyny in tweets, zero-shot cla...</td>\n",
       "      <td>[pre-training, transfer learning, zero-shot cl...</td>\n",
       "      <td>[detecting misogyny, identifying misogyny, mul...</td>\n",
       "      <td>[single - language bert models, error analysis...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>hu-etal-2021-collaborative</td>\n",
       "      <td>Collaborative Data Relabeling for Robust and D...</td>\n",
       "      <td>[recommendation system]</td>\n",
       "      <td>[collaborative data relabeling]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[automatic speech recognition, natural languag...</td>\n",
       "      <td>[collaborative data relabeling, automatic spee...</td>\n",
       "      <td>[online a/b testing, intelligent personal assi...</td>\n",
       "      <td>[collaborative data relabeling, cdr, skill rec...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>u-etal-2008-statistical</td>\n",
       "      <td>Statistical Machine Translation Models for Per...</td>\n",
       "      <td>[personalized search web]</td>\n",
       "      <td>[statistical machine translation models, relev...</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[statistical machine translation, relevance fe...</td>\n",
       "      <td>[statistical machine translation, relevance fe...</td>\n",
       "      <td>[personalization of web search, web search per...</td>\n",
       "      <td>[search engine, noisy channel model, statistic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>aggarwal-etal-2019-ltl</td>\n",
       "      <td>LTL-UDE at SemEval-2019 Task 6: BERT and Two-V...</td>\n",
       "      <td>[categorizing offensiveness]</td>\n",
       "      <td>[embedding representation, multi-layer percept...</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[text classification, offensive language detec...</td>\n",
       "      <td>[bert, multi-layer perceptron, two-vote classi...</td>\n",
       "      <td>[categorizing offensiveness]</td>\n",
       "      <td>[bert, ltl - ude's systems, embedding represen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>rehm-etal-2019-developing</td>\n",
       "      <td>Developing and Orchestrating a Portfolio of Na...</td>\n",
       "      <td>[natural legal language processing and documen...</td>\n",
       "      <td>[content and document curation workflow manager]</td>\n",
       "      <td>[european unionâ€™s horizon 2020 research and in...</td>\n",
       "      <td>[text processing, document curation, microserv...</td>\n",
       "      <td>[tokenization, part-of-speech tagging, parsing]</td>\n",
       "      <td>[natural legal language processing, prototype ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[lynx, european union's, horizon 2020]</td>\n",
       "      <td>[lynx, european union's, horizon 2020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>dernoncourt-etal-2017-neural</td>\n",
       "      <td>Neural Networks for Joint Sentence Classificat...</td>\n",
       "      <td>[sentence classification]</td>\n",
       "      <td>[artificial neural networks]</td>\n",
       "      <td>[philips research]</td>\n",
       "      <td>[sentence classification, joint sentence class...</td>\n",
       "      <td>[neural networks, artificial neural networks, ...</td>\n",
       "      <td>[joint sentence classification, sequential sen...</td>\n",
       "      <td>[neural networks, ann models, sentence classif...</td>\n",
       "      <td>[philips research, philips research]</td>\n",
       "      <td>[philips research, philips research]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>dimov-etal-2020-nopropaganda</td>\n",
       "      <td>NoPropaganda at SemEval-2020 Task 11: A Borrow...</td>\n",
       "      <td>[detection of propaganda techniques, propagand...</td>\n",
       "      <td>[lstm, autoregressive transformer decoder]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sequence tagging, text classification]</td>\n",
       "      <td>[lstm, transformer, relation extraction]</td>\n",
       "      <td>[text classification, relation extraction, det...</td>\n",
       "      <td>[lstm baselines, propaganda technique classifi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>yu-etal-2021-interpretable</td>\n",
       "      <td>Interpretable Propaganda Detection in News Art...</td>\n",
       "      <td>[propaganda detection]</td>\n",
       "      <td>[qualitatively descriptive features, pre-train...</td>\n",
       "      <td>[qatar computing research institute, hbku, com...</td>\n",
       "      <td>[text classification, information extraction, ...</td>\n",
       "      <td>[text classification, feature engineering, pre...</td>\n",
       "      <td>[detecting deception techniques]</td>\n",
       "      <td>[automatic systems, deception techniques]</td>\n",
       "      <td>[qatar computing research institute, hbku, mit...</td>\n",
       "      <td>[qatar computing research institute, hbku, mit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>lee-etal-2021-unifying</td>\n",
       "      <td>On Unifying Misinformation Detection. In this ...</td>\n",
       "      <td>[misinformation detection, detecting news bias...</td>\n",
       "      <td>[few-shot learning, unifiedm2]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[news bias detection, clickbait detection, fak...</td>\n",
       "      <td>[unification, joint modeling, representation l...</td>\n",
       "      <td>[detecting news bias, verifying rumors, unifyi...</td>\n",
       "      <td>[unifiedm2, general - purpose misinformation m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>kiritchenko-cherry-2011-lexically</td>\n",
       "      <td>Lexically-Triggered Hidden Markov Models for C...</td>\n",
       "      <td>[clinical document coding, document classifica...</td>\n",
       "      <td>[ lexically-triggered hidden markov model]</td>\n",
       "      <td>[acl-hlt]</td>\n",
       "      <td>[document classification, phrase detection, co...</td>\n",
       "      <td>[lexical matching, hidden markov models, term ...</td>\n",
       "      <td>[automatic coding of clinical documents, clini...</td>\n",
       "      <td>[discriminative hmm]</td>\n",
       "      <td>[acl-hlt]</td>\n",
       "      <td>[acl-hlt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>zeng-etal-2019-faceted</td>\n",
       "      <td>Faceted Hierarchy: A New Graph Type to Organiz...</td>\n",
       "      <td>[organize scientific concepts]</td>\n",
       "      <td>[information extraction techniques]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[information extraction, hypernym detection, t...</td>\n",
       "      <td>[hypernym detection, information extraction, h...</td>\n",
       "      <td>[classification]</td>\n",
       "      <td>[construction method, hypernym detection, hier...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>somers-etal-1997-multilingual</td>\n",
       "      <td>Multilingual Generation and Summarization of J...</td>\n",
       "      <td>[multilingual generation, summarization]</td>\n",
       "      <td>[query engine]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[text analysis, text generation, text summariz...</td>\n",
       "      <td>[example-based learning, pattern matching, cas...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[query engine, generation module, example - ba...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>maxwelll-smith-etal-2020-applications</td>\n",
       "      <td>Applications of Natural Language Processing in...</td>\n",
       "      <td>[bilingual language teaching]</td>\n",
       "      <td>[speech recognition, corpus]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[automated speech recognition, corpus building...</td>\n",
       "      <td>[automated speech recognition, computer-assist...</td>\n",
       "      <td>[data collection, fine - grained error analysis]</td>\n",
       "      <td>[natural language processing, automated speech...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>sun-etal-2021-medai</td>\n",
       "      <td>MedAI at SemEval-2021 Task 10: Negation-aware ...</td>\n",
       "      <td>[source-free negation detection]</td>\n",
       "      <td>[negationaware pre-training]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[negation detection, unsupervised domain adapt...</td>\n",
       "      <td>[unsupervised domain adaptation, pseudo labeli...</td>\n",
       "      <td>[semantic processing, adaptation, source - fre...</td>\n",
       "      <td>[negationaware pre - training]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>mcinnes-2008-unsupervised</td>\n",
       "      <td>An Unsupervised Vector Approach to Biomedical ...</td>\n",
       "      <td>[biomedical term disambiguation]</td>\n",
       "      <td>[contextual information]</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[term disambiguation, all-word disambiguation,...</td>\n",
       "      <td>[vector space model, unsupervised learning, st...</td>\n",
       "      <td>[all - word disambiguation, biomedical term di...</td>\n",
       "      <td>[senseclusters, unsupervised vector approach]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>jacobs-hoste-2020-extracting</td>\n",
       "      <td>Extracting Fine-Grained Economic Events from B...</td>\n",
       "      <td>[supervised economic event extraction]</td>\n",
       "      <td>[pilot study]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[event extraction, trigger identification, arg...</td>\n",
       "      <td>[supervised learning, event extraction, trigge...</td>\n",
       "      <td>[classification, event extraction, trigger ide...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[research foundation flanders, fwo, sentivent]</td>\n",
       "      <td>[research foundation flanders, fwo, sentivent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>v-hahn-vertan-2002-architectures</td>\n",
       "      <td>Architectures of ``toy'' systems for teaching ...</td>\n",
       "      <td>[academic teaching]</td>\n",
       "      <td>[describing two possible architectures]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[machine translation, teaching, evaluation]</td>\n",
       "      <td>[method1, method2, method3]</td>\n",
       "      <td>[mt, machine translation, teaching machine tra...</td>\n",
       "      <td>[educational toy systems, architectures of ``t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>rinaldi-etal-2008-dependency</td>\n",
       "      <td>Dependency-Based Relation Mining for Biomedica...</td>\n",
       "      <td>[dependency-based relation mining, text mining]</td>\n",
       "      <td>[dependency parser]</td>\n",
       "      <td>[swiss national science foundation]</td>\n",
       "      <td>[relationship detection, parsing, sentence sel...</td>\n",
       "      <td>[syntax-based filtering, dependency parsing]</td>\n",
       "      <td>[interaction detection]</td>\n",
       "      <td>[dependency parser, demonstrator]</td>\n",
       "      <td>[swiss national science foundation]</td>\n",
       "      <td>[swiss national science foundation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>meng-etal-2018-automatic</td>\n",
       "      <td>Automatic Labeling of Problem-Solving Dialogue...</td>\n",
       "      <td>[automatic labeling]</td>\n",
       "      <td>[sentence embeddings, linear chain crf model, ...</td>\n",
       "      <td>[no organization]</td>\n",
       "      <td>[dialogue segmentation, computational thinking...</td>\n",
       "      <td>[recurrent neural networks, long short-term me...</td>\n",
       "      <td>[microgenetic analysis of cognitive interactio...</td>\n",
       "      <td>[linear chain crf model, linear chain crfs, re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tanaka-etal-2014-linguistic</td>\n",
       "      <td>Linguistic and Acoustic Features for Automatic...</td>\n",
       "      <td>[automatic identification of autism spectrum d...</td>\n",
       "      <td>[linguistic and acoustic features]</td>\n",
       "      <td>[university of southern california signal anal...</td>\n",
       "      <td>[classification, feature engineering, explorat...</td>\n",
       "      <td>[automatic classification, word categories, pr...</td>\n",
       "      <td>[automatic classifiers, automatic identification]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[signal analysis and interpretation laboratory...</td>\n",
       "      <td>[signal analysis and interpretation laboratory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>wu-etal-2019-wtmed</td>\n",
       "      <td>WTMED at MEDIQA 2019: A Hybrid Approach to Bio...</td>\n",
       "      <td>[natural language inference]</td>\n",
       "      <td>[pre-trained text encoder, syntax encoder, ens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[natural language inference, text encoding, sy...</td>\n",
       "      <td>[pre-trained text encoder, syntax encoder, fea...</td>\n",
       "      <td>[biomedical natural language inference, biomed...</td>\n",
       "      <td>[feature encoder, ensemble models, syntax enco...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>bestgen-2019-tintin</td>\n",
       "      <td>Tintin at SemEval-2019 Task 4: Detecting Hyper...</td>\n",
       "      <td>[hyperpartisan news detection]</td>\n",
       "      <td>[simple tokens]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[hyperpartisan news detection, text classifica...</td>\n",
       "      <td>[supervised learning, tokenization]</td>\n",
       "      <td>[detecting hyperpartisan news article]</td>\n",
       "      <td>[tintin, supervised learning procedure, superv...</td>\n",
       "      <td>[fonds de la recherche scientifique -fnrs]</td>\n",
       "      <td>[fonds de la recherche scientifique -fnrs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>prost-etal-2019-debiasing</td>\n",
       "      <td>Debiasing Embeddings for Reduced Gender Bias i...</td>\n",
       "      <td>[text classification]</td>\n",
       "      <td>[classifier, embeddings]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text classification, debiasing word embeddings]</td>\n",
       "      <td>[word embeddings, debiasing, classification]</td>\n",
       "      <td>[text classification, reduced gender bias, occ...</td>\n",
       "      <td>[debiasing embeddings, classifier]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>perez-rosas-etal-2014-multimodal</td>\n",
       "      <td>A Multimodal Dataset for Deception Detection. ...</td>\n",
       "      <td>[deception detection]</td>\n",
       "      <td>[multimodal dataset, statistical analysis]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deception detection, multimodal dataset const...</td>\n",
       "      <td>[tokenization, part-of-speech tagging, parsing]</td>\n",
       "      <td>[deception detection]</td>\n",
       "      <td>[statistical analysis]</td>\n",
       "      <td>[national science foundation, national science...</td>\n",
       "      <td>[national science foundation, national science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>marge-etal-2019-research</td>\n",
       "      <td>A Research Platform for Multi-Robot Dialogue w...</td>\n",
       "      <td>[multi-robot dialogue with humans]</td>\n",
       "      <td>[research platform]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[spoken dialogue interaction, speech recogniti...</td>\n",
       "      <td>[speech recognition, dialogue management, inte...</td>\n",
       "      <td>[spoken dialogue interaction, dialogue managem...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[u.s. army research laboratory]</td>\n",
       "      <td>[u.s. army research laboratory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>desilets-etal-2008-evaluating</td>\n",
       "      <td>Evaluating productivity gains of hybrid ASR-MT...</td>\n",
       "      <td>[translation dictation, automatic speech recog...</td>\n",
       "      <td>[evaluation]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[translation dictation, automatic speech recog...</td>\n",
       "      <td>[automatic speech recognition, statistical mac...</td>\n",
       "      <td>[translation, french asr, translation dictatio...</td>\n",
       "      <td>[asr, human translator, smt, asr language mode...</td>\n",
       "      <td>[nrc, translation bureau of canada]</td>\n",
       "      <td>[nrc, translation bureau of canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>rudinger-etal-2017-social</td>\n",
       "      <td>Social Bias in Elicited Natural Language Infer...</td>\n",
       "      <td>[natural language inference]</td>\n",
       "      <td>[statistical analysis]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[bias and stereotyping in nlp data]</td>\n",
       "      <td>[stereotyping, bias, human-elicitation]</td>\n",
       "      <td>[bias and stereotyping]</td>\n",
       "      <td>[human - elicitation protocol]</td>\n",
       "      <td>[jhu human language technology center of excel...</td>\n",
       "      <td>[jhu human language technology center of excel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>coppersmith-etal-2015-adhd</td>\n",
       "      <td>From ADHD to SAD: Analyzing the Language of Me...</td>\n",
       "      <td>[mental health]</td>\n",
       "      <td>[language analysis]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text classification, topic modeling, lexical ...</td>\n",
       "      <td>[identifying self-reported statements of diagn...</td>\n",
       "      <td>[mental health]</td>\n",
       "      <td>[linguistic components]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>zhang-patrick-2006-extracting</td>\n",
       "      <td>Extracting Patient Clinical Profiles from Case...</td>\n",
       "      <td>[extracting patient clinical profiles]</td>\n",
       "      <td>[sentence classification system]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text classification, information extraction, ...</td>\n",
       "      <td>[text classification, part-of-speech tagging, ...</td>\n",
       "      <td>[extracting patient clinical profiles]</td>\n",
       "      <td>[sentence classification system]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>xie-etal-2021-humorhunter</td>\n",
       "      <td>HumorHunter at SemEval-2021 Task 7: Humor and ...</td>\n",
       "      <td>[humor and offense recognition]</td>\n",
       "      <td>[deberta, disentangled attention]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[humor recognition, offense recognition, text ...</td>\n",
       "      <td>[deberta, attention, bert]</td>\n",
       "      <td>[detecting and rating humor and offense]</td>\n",
       "      <td>[deberta model, language models, hahackathon]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>economou-etal-2000-lexiploigissi</td>\n",
       "      <td>LEXIPLOIGISSI: An Educational Platform for the...</td>\n",
       "      <td>[teaching of terminology]</td>\n",
       "      <td>[educational platform]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text processing, information retrieval, machi...</td>\n",
       "      <td>[lexical analysis, part-of-speech tagging, par...</td>\n",
       "      <td>[lexiploigissi *, lexiploigissi, educational p...</td>\n",
       "      <td>[educational platform, lexiploigissi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>gavankar-etal-2012-enriching</td>\n",
       "      <td>Enriching An Academic knowledge base using Lin...</td>\n",
       "      <td>[domain ontology]</td>\n",
       "      <td>[querying, extracting]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[information extraction, ontology mapping, lin...</td>\n",
       "      <td>[ontology mapping, linked open data, semantic ...</td>\n",
       "      <td>[semantic web context, ontology mapping]</td>\n",
       "      <td>[bootstrapping paradigm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>mosallanezhad-etal-2019-deep</td>\n",
       "      <td>Deep Reinforcement Learning-based Text Anonymi...</td>\n",
       "      <td>[text anonymization]</td>\n",
       "      <td>[deep reinforcement learning]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[textual data anonymization, deep reinforcemen...</td>\n",
       "      <td>[reinforcement learning, deep learning, text r...</td>\n",
       "      <td>[manipulating text representations, user behav...</td>\n",
       "      <td>[rlta, deep reinforcement learning, text repre...</td>\n",
       "      <td>[onr]</td>\n",
       "      <td>[onr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>risch-etal-2021-toxic</td>\n",
       "      <td>Data Integration for Toxic Comment Classificat...</td>\n",
       "      <td>[toxic comment classification]</td>\n",
       "      <td>[data integration]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[toxic comment classification, data integratio...</td>\n",
       "      <td>[data integration, toxic comment classificatio...</td>\n",
       "      <td>[toxic comment classification]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>senda-etal-2004-support</td>\n",
       "      <td>A Support System for Revising Titles to Stimul...</td>\n",
       "      <td>[revising titles]</td>\n",
       "      <td>[support system, questionnaire survey]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text generation, text revision, text simplifi...</td>\n",
       "      <td>[text generation, text revision, question gene...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[title revision wizard, support system]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>chen-etal-2020-analyzing</td>\n",
       "      <td>Analyzing Political Bias and Unfairness in New...</td>\n",
       "      <td>[analyzing political bias]</td>\n",
       "      <td>[corpus, neural model]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text classification, bias detection, politica...</td>\n",
       "      <td>[neural networks, text classification, corpus ...</td>\n",
       "      <td>[automatic detection of bias, bias assessment]</td>\n",
       "      <td>[neural model]</td>\n",
       "      <td>[german research foundation, dfg, collaborativ...</td>\n",
       "      <td>[german research foundation, dfg, collaborativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>jagannatha-yu-2016-structured</td>\n",
       "      <td>Structured prediction models for RNN based seq...</td>\n",
       "      <td>[sequence labeling, named entity recognition, ...</td>\n",
       "      <td>[conditional random field, recurrent neural ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sequence labeling, named entity recognition, ...</td>\n",
       "      <td>[sequence labeling, named entity recognition, ...</td>\n",
       "      <td>[rnn based sequence labeling, phrase detection...</td>\n",
       "      <td>[structured prediction models, crf - lstm mode...</td>\n",
       "      <td>[umassmed, national institutes of health, nih,...</td>\n",
       "      <td>[umassmed, national institutes of health, nih,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>lin-etal-2019-enhancing</td>\n",
       "      <td>Enhancing Dialogue Symptom Diagnosis with Glob...</td>\n",
       "      <td>[symptom diagnosis]</td>\n",
       "      <td>[global attention mechanism, symptom graph]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[dialogue symptom diagnosis, global attention,...</td>\n",
       "      <td>[global attention, symptom graph]</td>\n",
       "      <td>[natural language processing, dialogue symptom...</td>\n",
       "      <td>[global attention mechanism]</td>\n",
       "      <td>[national natural science foundation of china,...</td>\n",
       "      <td>[national natural science foundation of china,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>hoang-kan-2010-towards</td>\n",
       "      <td>Towards Automated Related Work Summarization. ...</td>\n",
       "      <td>[automatic related work summarization]</td>\n",
       "      <td>[summarization system]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[automatic summarization, extractive summariza...</td>\n",
       "      <td>[textual entailment, information extraction, s...</td>\n",
       "      <td>[automated related work summarization, automat...</td>\n",
       "      <td>[rewos, prototype related work summarization s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>sizov-ozturk-2013-automatic</td>\n",
       "      <td>Automatic Extraction of Reasoning Chains from ...</td>\n",
       "      <td>[automatic extraction]</td>\n",
       "      <td>[graph-based text representation, natural lang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[textual entailment, information extraction, p...</td>\n",
       "      <td>[syntactic parsing, discourse parsing]</td>\n",
       "      <td>[automatic extraction of reasoning chains]</td>\n",
       "      <td>[syntactic and discourse parsers, graph - base...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>brekke-etal-2006-automatic</td>\n",
       "      <td>Automatic Term Extraction from Knowledge Bank ...</td>\n",
       "      <td>[automatic term extraction]</td>\n",
       "      <td>[statistical filtering]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[term extraction, word sense disambiguation]</td>\n",
       "      <td>[automatic term extraction, linguistic filteri...</td>\n",
       "      <td>[automatic term extraction]</td>\n",
       "      <td>[linguistic , lexical and statistical filtering]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>li-etal-2020-multi-task</td>\n",
       "      <td>Multi-task Peer-Review Score Prediction. Autom...</td>\n",
       "      <td>[peer-review score prediction]</td>\n",
       "      <td>[multi-task shared structure encoding approach...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[task1, task2, task3, task4, task5, task6, tas...</td>\n",
       "      <td>[multi-task learning, shared structure encoding]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[naÃ¯ve multi - task methods]</td>\n",
       "      <td>[kddi foundation research grant program]</td>\n",
       "      <td>[kddi foundation research grant program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>diaz-etal-2010-development</td>\n",
       "      <td>Development and Use of an Evaluation Collectio...</td>\n",
       "      <td>[personalisation system]</td>\n",
       "      <td>[evaluation collection]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[information retrieval, information extraction...</td>\n",
       "      <td>[information retrieval, text classification, t...</td>\n",
       "      <td>[personalisation of digital newspapers]</td>\n",
       "      <td>[user model, personalisation system, personali...</td>\n",
       "      <td>[yahoo]</td>\n",
       "      <td>[yahoo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>kim-etal-2019-textbook</td>\n",
       "      <td>Textbook Question Answering with Multi-modal C...</td>\n",
       "      <td>[textbook question answering]</td>\n",
       "      <td>[multi-modal context graph understanding, self...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[textbook question answering, multimodal conte...</td>\n",
       "      <td>[graph convolutional networks, self-supervised...</td>\n",
       "      <td>[textbook question answering (tqa) task, tqa p...</td>\n",
       "      <td>[graph convolutional networks]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>inan-etal-2022-modeling</td>\n",
       "      <td>Modeling Intensification for Sign Language Gen...</td>\n",
       "      <td>[sign language generation]</td>\n",
       "      <td>[supervised intensity tagger, transformer]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sign language generation, data annotation, su...</td>\n",
       "      <td>[supervised learning, data-driven modeling, tr...</td>\n",
       "      <td>[sign language generation, linguistics of sign...</td>\n",
       "      <td>[modeling intensification, transformer models,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>boytcheva-etal-2009-extraction</td>\n",
       "      <td>Extraction and Exploration of Correlations in ...</td>\n",
       "      <td>[information extraction]</td>\n",
       "      <td>[algorithm for exploring the correlations]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[information extraction, text processing, shal...</td>\n",
       "      <td>[information extraction, clustering, metrics]</td>\n",
       "      <td>[automatic processing of hospital patient reco...</td>\n",
       "      <td>[ie, information extraction approach, ie proto...</td>\n",
       "      <td>[bulgarian national science fund]</td>\n",
       "      <td>[bulgarian national science fund]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>falenska-cetinoglu-2021-assessing</td>\n",
       "      <td>Assessing Gender Bias in Wikipedia: Inequaliti...</td>\n",
       "      <td>[gender bias detection]</td>\n",
       "      <td>[analysis]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text classification, information extraction, ...</td>\n",
       "      <td>[data collection, data analysis, natural langu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[wikipedia components]</td>\n",
       "      <td>[dfg]</td>\n",
       "      <td>[dfg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>levi-etal-2019-identifying</td>\n",
       "      <td>Identifying Nuances in Fake News vs. Satire: U...</td>\n",
       "      <td>[identifying nuances in fake news]</td>\n",
       "      <td>[semantic representation, contextual language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[classification, semantic representation, lang...</td>\n",
       "      <td>[semantic representation, contextual language ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[semantic representation, machine learning met...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sellami-etal-2013-exploiting</td>\n",
       "      <td>Exploiting multiple resources for Japanese to ...</td>\n",
       "      <td>[patent translation]</td>\n",
       "      <td>[mt system, data collection]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[machine translation, data collection, data ex...</td>\n",
       "      <td>[machine translation, data extraction, languag...</td>\n",
       "      <td>[japanese to english patent translation]</td>\n",
       "      <td>[japanese to english translation system, mt sy...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>li-etal-2019-detecting</td>\n",
       "      <td>Detecting dementia in Mandarin Chinese using t...</td>\n",
       "      <td>[dementia detection]</td>\n",
       "      <td>[transfer learning]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[transfer learning, feature engineering, demen...</td>\n",
       "      <td>[transfer learning, parallel corpus, lexicosyn...</td>\n",
       "      <td>[automatic detection of alzheimer's disease (a...</td>\n",
       "      <td>[machine learning, transfer learning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>schumaker-2010-analysis</td>\n",
       "      <td>An Analysis of Verbs in Financial News Article...</td>\n",
       "      <td>[analysis of verbs]</td>\n",
       "      <td>[discrete machine learning algorithm]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[identifying the sentiment of a text, identify...</td>\n",
       "      <td>[part-of-speech tagging, dependency parsing, v...</td>\n",
       "      <td>[stock price movement]</td>\n",
       "      <td>[discrete machine learning algorithm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>cavicchio-2009-modulation</td>\n",
       "      <td>The Modulation of Cooperation and Emotion in D...</td>\n",
       "      <td>[emotion in dialogue]</td>\n",
       "      <td>[emotive corpus, kappa statistic, coding scheme]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[emotion recognition, annotation of blended em...</td>\n",
       "      <td>[emotion recognition, multimodal annotation, k...</td>\n",
       "      <td>[bimodal display of emotion), cooperation, emo...</td>\n",
       "      <td>[multimodal annotation scheme, logistic regres...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>khanna-etal-2022-idiap</td>\n",
       "      <td>IDIAP\\_TIET@LT-EDI-ACL2022 : Hope Speech Detec...</td>\n",
       "      <td>[hope speech detection]</td>\n",
       "      <td>[bert embeddings, attention mechanism]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[text classification, hate speech detection]</td>\n",
       "      <td>[bert, attention mechanism, contextualized bert]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[modular pipeline classifier, attention mechan...</td>\n",
       "      <td>[european union's, horizon 2020]</td>\n",
       "      <td>[european union's, horizon 2020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>solorio-etal-2013-case</td>\n",
       "      <td>A Case Study of Sockpuppet Detection in Wikipe...</td>\n",
       "      <td>[detection of sockpuppeteering]</td>\n",
       "      <td>[authorship attribution, voting scheme]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[authorship attribution, text classification]</td>\n",
       "      <td>[authorship attribution, voting scheme, short ...</td>\n",
       "      <td>[sockpuppet detection, sockpuppeteering, detec...</td>\n",
       "      <td>[authorship attribution methods, human process...</td>\n",
       "      <td>[onr]</td>\n",
       "      <td>[onr]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID  \\\n",
       "0        wandji-tchami-grabar-2014-towards   \n",
       "1                      soh-etal-2019-legal   \n",
       "2                      huang-bai-2021-team   \n",
       "3                  sikdar-etal-2018-flytxt   \n",
       "4                       rio-2002-compiling   \n",
       "5                yang-heeman-2007-avoiding   \n",
       "6         del-tredici-fernandez-2020-words   \n",
       "7          akhlaghi-etal-2020-constructing   \n",
       "8                stajner-etal-2017-effects   \n",
       "9                  du-etal-2019-extracting   \n",
       "10                       guo-etal-2021-pre   \n",
       "11                    nguyen-2019-question   \n",
       "12        polat-saraclar-2020-unsupervised   \n",
       "13    bagheri-garakani-etal-2022-improving   \n",
       "14                 doddington-1989-initial   \n",
       "15                maegaard-etal-2008-medar   \n",
       "16             flekova-etal-2016-exploring   \n",
       "17                  hieu-etal-2020-reintel   \n",
       "18           da-san-martino-etal-2020-prta   \n",
       "19          akasaki-kaji-2019-conversation   \n",
       "20             tang-shen-2020-categorizing   \n",
       "21              pergola-etal-2021-boosting   \n",
       "22               kim-park-2015-statistical   \n",
       "23                  salawu-etal-2021-large   \n",
       "24                  finlayson-etal-2014-n2   \n",
       "25       shreevastava-foltz-2021-detecting   \n",
       "26                    kirk-etal-2021-memes   \n",
       "27            marinelli-etal-2008-encoding   \n",
       "28                       oard-2007-invited   \n",
       "29            berzak-etal-2015-contrastive   \n",
       "30                      huang-bai-2021-hub   \n",
       "31           pyysalo-etal-2007-unification   \n",
       "32              shen-etal-2013-participant   \n",
       "33                    li-etal-2020-adviser   \n",
       "34              cardellino-etal-2017-legal   \n",
       "35                 neubig-etal-2011-safety   \n",
       "36              trajanovski-etal-2021-text   \n",
       "37            kulkarni-etal-2018-annotated   \n",
       "38              mayfield-black-2019-stance   \n",
       "39                     ma-etal-2017-detect   \n",
       "40                hopkins-etal-2017-beyond   \n",
       "41                  asgari-etal-2020-topic   \n",
       "42            alexeeva-etal-2020-mathalign   \n",
       "43          barreiro-cabral-2009-reescreve   \n",
       "44             moore-etal-1997-commandtalk   \n",
       "45    chakraborti-tendulkar-2013-parallels   \n",
       "46                  li-hovy-2014-sentiment   \n",
       "47                   wang-etal-2019-bigodm   \n",
       "48           sotnikova-etal-2021-analyzing   \n",
       "49      muti-barron-cedeno-2022-checkpoint   \n",
       "50              hu-etal-2021-collaborative   \n",
       "51                 u-etal-2008-statistical   \n",
       "52                  aggarwal-etal-2019-ltl   \n",
       "53               rehm-etal-2019-developing   \n",
       "54            dernoncourt-etal-2017-neural   \n",
       "55            dimov-etal-2020-nopropaganda   \n",
       "56              yu-etal-2021-interpretable   \n",
       "57                  lee-etal-2021-unifying   \n",
       "58       kiritchenko-cherry-2011-lexically   \n",
       "59                  zeng-etal-2019-faceted   \n",
       "60           somers-etal-1997-multilingual   \n",
       "61   maxwelll-smith-etal-2020-applications   \n",
       "62                     sun-etal-2021-medai   \n",
       "63               mcinnes-2008-unsupervised   \n",
       "64            jacobs-hoste-2020-extracting   \n",
       "65        v-hahn-vertan-2002-architectures   \n",
       "66            rinaldi-etal-2008-dependency   \n",
       "67                meng-etal-2018-automatic   \n",
       "68             tanaka-etal-2014-linguistic   \n",
       "69                      wu-etal-2019-wtmed   \n",
       "70                     bestgen-2019-tintin   \n",
       "71               prost-etal-2019-debiasing   \n",
       "72        perez-rosas-etal-2014-multimodal   \n",
       "73                marge-etal-2019-research   \n",
       "74           desilets-etal-2008-evaluating   \n",
       "75               rudinger-etal-2017-social   \n",
       "76              coppersmith-etal-2015-adhd   \n",
       "77           zhang-patrick-2006-extracting   \n",
       "78               xie-etal-2021-humorhunter   \n",
       "79        economou-etal-2000-lexiploigissi   \n",
       "80            gavankar-etal-2012-enriching   \n",
       "81            mosallanezhad-etal-2019-deep   \n",
       "82                   risch-etal-2021-toxic   \n",
       "83                 senda-etal-2004-support   \n",
       "84                chen-etal-2020-analyzing   \n",
       "85           jagannatha-yu-2016-structured   \n",
       "86                 lin-etal-2019-enhancing   \n",
       "87                  hoang-kan-2010-towards   \n",
       "88             sizov-ozturk-2013-automatic   \n",
       "89              brekke-etal-2006-automatic   \n",
       "90                 li-etal-2020-multi-task   \n",
       "91              diaz-etal-2010-development   \n",
       "92                  kim-etal-2019-textbook   \n",
       "93                 inan-etal-2022-modeling   \n",
       "94          boytcheva-etal-2009-extraction   \n",
       "95       falenska-cetinoglu-2021-assessing   \n",
       "96              levi-etal-2019-identifying   \n",
       "97            sellami-etal-2013-exploiting   \n",
       "98                  li-etal-2019-detecting   \n",
       "99                 schumaker-2010-analysis   \n",
       "100              cavicchio-2009-modulation   \n",
       "101                 khanna-etal-2022-idiap   \n",
       "102                 solorio-etal-2013-case   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Towards Automatic Distinction between Speciali...   \n",
       "1    Legal Area Classification: A Comparative Study...   \n",
       "2    TEAM HUB@LT-EDI-EACL2021: Hope Speech Detectio...   \n",
       "3    Flytxt\\_NTNU at SemEval-2018 Task 8: Identifyi...   \n",
       "4    Compiling an Interactive Literary Translation ...   \n",
       "5    Avoiding and Resolving Initiative Conflicts in...   \n",
       "6    Words are the Window to the Soul: Language-bas...   \n",
       "7    Constructing Multimodal Language Learner Texts...   \n",
       "8    Effects of Lexical Properties on Viewing Time ...   \n",
       "9    Extracting Symptoms and their Status from Clin...   \n",
       "10   Pre-trained Transformer-based Classification a...   \n",
       "11   Question Answering in the Biomedical Domain. Q...   \n",
       "12   Unsupervised Term Discovery for Continuous Sig...   \n",
       "13   Improving Relevance Quality in Product Search ...   \n",
       "14   Initial Draft Guidelines for the Development o...   \n",
       "15   MEDAR: Collaboration between European and Medi...   \n",
       "16   Exploring Stylistic Variation with Age and Inc...   \n",
       "17   ReINTEL Challenge 2020: Vietnamese Fake News D...   \n",
       "18   Prta: A System to Support the Analysis of Prop...   \n",
       "19   Conversation Initiation by Diverse News Conten...   \n",
       "20   Categorizing Offensive Language in Social Netw...   \n",
       "21   Boosting Low-Resource Biomedical QA via Entity...   \n",
       "22   A Statistical Modeling of the Correlation betw...   \n",
       "23   A Large-Scale English Multi-Label Twitter Data...   \n",
       "24   The N2 corpus: A semantically annotated collec...   \n",
       "25   Detecting Cognitive Distortions from Patient-T...   \n",
       "26   Memes in the Wild: Assessing the Generalizabil...   \n",
       "27   Encoding Terms from a Scientific Domain in a T...   \n",
       "28   Invited Talk: Lessons from the MALACH Project:...   \n",
       "29   Contrastive Analysis with Predictive Power: Ty...   \n",
       "30   HUB@DravidianLangTech-EACL2021: Identify and C...   \n",
       "31   On the unification of syntactic annotations un...   \n",
       "32   A Participant-based Approach for Event Summari...   \n",
       "33   ADVISER: A Toolkit for Developing Multi-modal,...   \n",
       "34   Legal NERC with ontologies, Wikipedia and curr...   \n",
       "35   Safety Information Mining --- What can NLP do ...   \n",
       "36   When does text prediction benefit from additio...   \n",
       "37   An Annotated Corpus for Machine Reading of Ins...   \n",
       "38   Stance Classification, Outcome Prediction, and...   \n",
       "39   Detect Rumors in Microblog Posts Using Propaga...   \n",
       "40   Beyond Sentential Semantic Parsing: Tackling t...   \n",
       "41   Topic-Based Measures of Conversation for Detec...   \n",
       "42   MathAlign: Linking Formula Identifiers to thei...   \n",
       "43   ReEscreve: a Translator-friendly Multi-purpose...   \n",
       "44   CommandTalk: A Spoken-Language Interface for B...   \n",
       "45   Parallels between Linguistics and Biology. In ...   \n",
       "46   Sentiment Analysis on the People's Daily. We p...   \n",
       "47   BIGODM System in the Social Media Mining for H...   \n",
       "48   Analyzing Stereotypes in Generative Text Infer...   \n",
       "49   A Checkpoint on Multilingual Misogyny Identifi...   \n",
       "50   Collaborative Data Relabeling for Robust and D...   \n",
       "51   Statistical Machine Translation Models for Per...   \n",
       "52   LTL-UDE at SemEval-2019 Task 6: BERT and Two-V...   \n",
       "53   Developing and Orchestrating a Portfolio of Na...   \n",
       "54   Neural Networks for Joint Sentence Classificat...   \n",
       "55   NoPropaganda at SemEval-2020 Task 11: A Borrow...   \n",
       "56   Interpretable Propaganda Detection in News Art...   \n",
       "57   On Unifying Misinformation Detection. In this ...   \n",
       "58   Lexically-Triggered Hidden Markov Models for C...   \n",
       "59   Faceted Hierarchy: A New Graph Type to Organiz...   \n",
       "60   Multilingual Generation and Summarization of J...   \n",
       "61   Applications of Natural Language Processing in...   \n",
       "62   MedAI at SemEval-2021 Task 10: Negation-aware ...   \n",
       "63   An Unsupervised Vector Approach to Biomedical ...   \n",
       "64   Extracting Fine-Grained Economic Events from B...   \n",
       "65   Architectures of ``toy'' systems for teaching ...   \n",
       "66   Dependency-Based Relation Mining for Biomedica...   \n",
       "67   Automatic Labeling of Problem-Solving Dialogue...   \n",
       "68   Linguistic and Acoustic Features for Automatic...   \n",
       "69   WTMED at MEDIQA 2019: A Hybrid Approach to Bio...   \n",
       "70   Tintin at SemEval-2019 Task 4: Detecting Hyper...   \n",
       "71   Debiasing Embeddings for Reduced Gender Bias i...   \n",
       "72   A Multimodal Dataset for Deception Detection. ...   \n",
       "73   A Research Platform for Multi-Robot Dialogue w...   \n",
       "74   Evaluating productivity gains of hybrid ASR-MT...   \n",
       "75   Social Bias in Elicited Natural Language Infer...   \n",
       "76   From ADHD to SAD: Analyzing the Language of Me...   \n",
       "77   Extracting Patient Clinical Profiles from Case...   \n",
       "78   HumorHunter at SemEval-2021 Task 7: Humor and ...   \n",
       "79   LEXIPLOIGISSI: An Educational Platform for the...   \n",
       "80   Enriching An Academic knowledge base using Lin...   \n",
       "81   Deep Reinforcement Learning-based Text Anonymi...   \n",
       "82   Data Integration for Toxic Comment Classificat...   \n",
       "83   A Support System for Revising Titles to Stimul...   \n",
       "84   Analyzing Political Bias and Unfairness in New...   \n",
       "85   Structured prediction models for RNN based seq...   \n",
       "86   Enhancing Dialogue Symptom Diagnosis with Glob...   \n",
       "87   Towards Automated Related Work Summarization. ...   \n",
       "88   Automatic Extraction of Reasoning Chains from ...   \n",
       "89   Automatic Term Extraction from Knowledge Bank ...   \n",
       "90   Multi-task Peer-Review Score Prediction. Autom...   \n",
       "91   Development and Use of an Evaluation Collectio...   \n",
       "92   Textbook Question Answering with Multi-modal C...   \n",
       "93   Modeling Intensification for Sign Language Gen...   \n",
       "94   Extraction and Exploration of Correlations in ...   \n",
       "95   Assessing Gender Bias in Wikipedia: Inequaliti...   \n",
       "96   Identifying Nuances in Fake News vs. Satire: U...   \n",
       "97   Exploiting multiple resources for Japanese to ...   \n",
       "98   Detecting dementia in Mandarin Chinese using t...   \n",
       "99   An Analysis of Verbs in Financial News Article...   \n",
       "100  The Modulation of Cooperation and Emotion in D...   \n",
       "101  IDIAP\\_TIET@LT-EDI-ACL2022 : Hope Speech Detec...   \n",
       "102  A Case Study of Sockpuppet Detection in Wikipe...   \n",
       "\n",
       "                                       task_annotation  \\\n",
       "0            [contrastive automatic analysis of verbs]   \n",
       "1                          [legal area classification]   \n",
       "2         [hope speech detection, text classification]   \n",
       "3           [identifying and classifying malware text]   \n",
       "4    [compiling an interactive literary translation...   \n",
       "5        [avoiding and resolving initiative conflicts]   \n",
       "6                                [fake news detection]   \n",
       "7                          [conversion of plain texts]   \n",
       "8                                       [eye-tracking]   \n",
       "9                                [extracting symptoms]   \n",
       "10                    [span detection, classification]   \n",
       "11                                [question answering]   \n",
       "12                         [sign language recognition]   \n",
       "13               [relevance quality in product search]   \n",
       "14                           [spoken language systems]   \n",
       "15        [machine translation, information retrieval]   \n",
       "16                     [exploring stylistic variation]   \n",
       "17                               [fake news detection]   \n",
       "18                 [analysis of propaganda techniques]   \n",
       "19                              [conversation systems]   \n",
       "20                   [categorizing offensive language]   \n",
       "21   [question answering, domain adaptaion, transfe...   \n",
       "22                              [statistical modeling]   \n",
       "23                            [online abuse detection]   \n",
       "24                                            [corpus]   \n",
       "25                    [cognitive distortion detection]   \n",
       "26   [assessing dataset generalizability, hateful m...   \n",
       "27                           [terminological database]   \n",
       "28   [intellectual access to large oral history col...   \n",
       "29   [contrastive analysis, typology driven estimat...   \n",
       "30                 [offensive language identification]   \n",
       "31                 [biomedical information extraction]   \n",
       "32                               [event summarization]   \n",
       "33                        [multi-domain dialog system]   \n",
       "34                             [legal domain ontology]   \n",
       "35                                [information mining]   \n",
       "36                                   [text prediction]   \n",
       "37   [conversion of protocols into a machine-readab...   \n",
       "38   [stance classification, outcome prediction, an...   \n",
       "39                                [identifying rumors]   \n",
       "40                                  [semantic parsing]   \n",
       "41                [detecting mild cognitiveimpairment]   \n",
       "42                                   [machine reading]   \n",
       "43                                      [paraphrasing]   \n",
       "44                           [battlefield simulations]   \n",
       "45         [parallels between linguistics and biology]   \n",
       "46                                [sentiment analysis]   \n",
       "47                               [social media mining]   \n",
       "48                             [analyzing stereotypes]   \n",
       "49                           [misogyny identification]   \n",
       "50                             [recommendation system]   \n",
       "51                           [personalized search web]   \n",
       "52                        [categorizing offensiveness]   \n",
       "53   [natural legal language processing and documen...   \n",
       "54                           [sentence classification]   \n",
       "55   [detection of propaganda techniques, propagand...   \n",
       "56                              [propaganda detection]   \n",
       "57   [misinformation detection, detecting news bias...   \n",
       "58   [clinical document coding, document classifica...   \n",
       "59                      [organize scientific concepts]   \n",
       "60            [multilingual generation, summarization]   \n",
       "61                       [bilingual language teaching]   \n",
       "62                    [source-free negation detection]   \n",
       "63                    [biomedical term disambiguation]   \n",
       "64              [supervised economic event extraction]   \n",
       "65                                 [academic teaching]   \n",
       "66     [dependency-based relation mining, text mining]   \n",
       "67                                [automatic labeling]   \n",
       "68   [automatic identification of autism spectrum d...   \n",
       "69                        [natural language inference]   \n",
       "70                      [hyperpartisan news detection]   \n",
       "71                               [text classification]   \n",
       "72                               [deception detection]   \n",
       "73                  [multi-robot dialogue with humans]   \n",
       "74   [translation dictation, automatic speech recog...   \n",
       "75                        [natural language inference]   \n",
       "76                                     [mental health]   \n",
       "77              [extracting patient clinical profiles]   \n",
       "78                     [humor and offense recognition]   \n",
       "79                           [teaching of terminology]   \n",
       "80                                   [domain ontology]   \n",
       "81                                [text anonymization]   \n",
       "82                      [toxic comment classification]   \n",
       "83                                   [revising titles]   \n",
       "84                          [analyzing political bias]   \n",
       "85   [sequence labeling, named entity recognition, ...   \n",
       "86                                 [symptom diagnosis]   \n",
       "87              [automatic related work summarization]   \n",
       "88                              [automatic extraction]   \n",
       "89                         [automatic term extraction]   \n",
       "90                      [peer-review score prediction]   \n",
       "91                            [personalisation system]   \n",
       "92                       [textbook question answering]   \n",
       "93                          [sign language generation]   \n",
       "94                            [information extraction]   \n",
       "95                             [gender bias detection]   \n",
       "96                  [identifying nuances in fake news]   \n",
       "97                                [patent translation]   \n",
       "98                                [dementia detection]   \n",
       "99                                 [analysis of verbs]   \n",
       "100                              [emotion in dialogue]   \n",
       "101                            [hope speech detection]   \n",
       "102                    [detection of sockpuppeteering]   \n",
       "\n",
       "                                     method_annotation  \\\n",
       "0                                [semantic annotation]   \n",
       "1        [topic model, word embedding, language model]   \n",
       "2                [language model, xlm-roberta, tf-idf]   \n",
       "3     [supervised learning, conditional random fields]   \n",
       "4                                           [web site]   \n",
       "5                                    [empirical study]   \n",
       "6                [language-based user representations]   \n",
       "7                     [learning and reading assistant]   \n",
       "8            [lexical properties,  parallel gaze data]   \n",
       "9    [curriculum learning, sequence-to-sequence, hi...   \n",
       "10    [pre-trained transformer, classifier ensembling]   \n",
       "11                         [question answering system]   \n",
       "12                       [unsupervised term discovery]   \n",
       "13                                              [bert]   \n",
       "14                                        [guidelines]   \n",
       "15                           [surveys, questionnaires]   \n",
       "16                    [regression, stylistic features]   \n",
       "17               [phobert embeddings, ensemble method]   \n",
       "18         [propaganda persuasion techniques analyzer]   \n",
       "19          [information retrieval, generation models]   \n",
       "20   [hierarchical attention capsule network, integ...   \n",
       "21   [masked language models, language models, enti...   \n",
       "22   [grammatical analysis, working-memory-based pr...   \n",
       "23                             [transformers, dataset]   \n",
       "24      [multi-layed annotation, annotation procedure]   \n",
       "25   [pretrained sentence-bert embeddings, svm clas...   \n",
       "26                            [ocr, multimodal models]   \n",
       "27                                    [encoding terms]   \n",
       "28   [automated clustering, automatic speech recogn...   \n",
       "29                                     [bootstrapping]   \n",
       "30                                 [multilingual bert]   \n",
       "31      [dependency schemes, bioinfer, genia treebank]   \n",
       "32                        [participant-based approach]   \n",
       "33                                            [python]   \n",
       "34               [named entity recognizer, classifier]   \n",
       "35                      [robust and effective systems]   \n",
       "36                             [large language models]   \n",
       "37                                  [annotated corpus]   \n",
       "38                                      [corpus, bert]   \n",
       "39            [propogation trees, kernel-based method]   \n",
       "40                           [tree transducer cascade]   \n",
       "41       [lexical coherence of consecutive utterances]   \n",
       "42                               [rule-based approach]   \n",
       "43          [multi-purpose paraphrasing software tool]   \n",
       "44   [nuance speech recognition system, gemini natu...   \n",
       "45                  [parallel construction, analogies]   \n",
       "46           [semi-supervised bootstrapping algorithm]   \n",
       "47   [support vector machines, word embedding, line...   \n",
       "48                       [annotation, human judgement]   \n",
       "49                                [transformers, bert]   \n",
       "50                     [collaborative data relabeling]   \n",
       "51   [statistical machine translation models, relev...   \n",
       "52   [embedding representation, multi-layer percept...   \n",
       "53    [content and document curation workflow manager]   \n",
       "54                        [artificial neural networks]   \n",
       "55          [lstm, autoregressive transformer decoder]   \n",
       "56   [qualitatively descriptive features, pre-train...   \n",
       "57                      [few-shot learning, unifiedm2]   \n",
       "58          [ lexically-triggered hidden markov model]   \n",
       "59                 [information extraction techniques]   \n",
       "60                                      [query engine]   \n",
       "61                        [speech recognition, corpus]   \n",
       "62                        [negationaware pre-training]   \n",
       "63                            [contextual information]   \n",
       "64                                       [pilot study]   \n",
       "65             [describing two possible architectures]   \n",
       "66                                 [dependency parser]   \n",
       "67   [sentence embeddings, linear chain crf model, ...   \n",
       "68                  [linguistic and acoustic features]   \n",
       "69   [pre-trained text encoder, syntax encoder, ens...   \n",
       "70                                     [simple tokens]   \n",
       "71                            [classifier, embeddings]   \n",
       "72          [multimodal dataset, statistical analysis]   \n",
       "73                                 [research platform]   \n",
       "74                                        [evaluation]   \n",
       "75                              [statistical analysis]   \n",
       "76                                 [language analysis]   \n",
       "77                    [sentence classification system]   \n",
       "78                   [deberta, disentangled attention]   \n",
       "79                              [educational platform]   \n",
       "80                              [querying, extracting]   \n",
       "81                       [deep reinforcement learning]   \n",
       "82                                  [data integration]   \n",
       "83              [support system, questionnaire survey]   \n",
       "84                              [corpus, neural model]   \n",
       "85   [conditional random field, recurrent neural ne...   \n",
       "86         [global attention mechanism, symptom graph]   \n",
       "87                              [summarization system]   \n",
       "88   [graph-based text representation, natural lang...   \n",
       "89                             [statistical filtering]   \n",
       "90   [multi-task shared structure encoding approach...   \n",
       "91                             [evaluation collection]   \n",
       "92   [multi-modal context graph understanding, self...   \n",
       "93          [supervised intensity tagger, transformer]   \n",
       "94          [algorithm for exploring the correlations]   \n",
       "95                                          [analysis]   \n",
       "96   [semantic representation, contextual language ...   \n",
       "97                        [mt system, data collection]   \n",
       "98                                 [transfer learning]   \n",
       "99               [discrete machine learning algorithm]   \n",
       "100   [emotive corpus, kappa statistic, coding scheme]   \n",
       "101             [bert embeddings, attention mechanism]   \n",
       "102            [authorship attribution, voting scheme]   \n",
       "\n",
       "                                        org_annotation  \\\n",
       "0                                    [no organization]   \n",
       "1    [singapore academy of law, singapore supreme c...   \n",
       "2                                    [no organization]   \n",
       "3                                    [no organization]   \n",
       "4                                    [no organization]   \n",
       "5                                                  NaN   \n",
       "6    [netherlands organisation for scientific resea...   \n",
       "7                                                  NaN   \n",
       "8    [university of mannheim political economy of r...   \n",
       "9                                    [no organization]   \n",
       "10                                   [no organization]   \n",
       "11       [australian research training program, csiro]   \n",
       "12   [scientific and technological research council...   \n",
       "13                                                 NaN   \n",
       "14                                                 NaN   \n",
       "15                                [european comission]   \n",
       "16                          [templeton religion trust]   \n",
       "17                                   [no organization]   \n",
       "18     [qatar computing research institute, mit-csail]   \n",
       "19                                   [no organization]   \n",
       "20   [national language commission key research pro...   \n",
       "21                 [uk research and innovation, epsrc]   \n",
       "22             [national research foundation of korea]   \n",
       "23                                   [no organization]   \n",
       "24   [u.s. defense advanced research project agency...   \n",
       "25   [university of colorado, boulder computational...   \n",
       "26                                   [no organization]   \n",
       "27                                                 NaN   \n",
       "28                                   [no organization]   \n",
       "29            [center for brains, minds, and machines]   \n",
       "30                                   [no organization]   \n",
       "31                                [academy of finland]   \n",
       "32                                                 NaN   \n",
       "33                                   [no organization]   \n",
       "34                                                 NaN   \n",
       "35                                                 NaN   \n",
       "36                                                 NaN   \n",
       "37                                                 NaN   \n",
       "38                                                 NaN   \n",
       "39                [general research fund of hong kong]   \n",
       "40                                                 NaN   \n",
       "41                                                 NaN   \n",
       "42                                                 NaN   \n",
       "43   [linguateca (portugese government and european...   \n",
       "44   [defense advanced research projects agency, na...   \n",
       "45   [government of india department of biotechnology]   \n",
       "46                                                 NaN   \n",
       "47                                   [no organization]   \n",
       "48                   [university of maryland clip lab]   \n",
       "49                                   [no organization]   \n",
       "50                                   [no organization]   \n",
       "51                                   [no organization]   \n",
       "52                                   [no organization]   \n",
       "53   [european unionâ€™s horizon 2020 research and in...   \n",
       "54                                  [philips research]   \n",
       "55                                                 NaN   \n",
       "56   [qatar computing research institute, hbku, com...   \n",
       "57                                   [no organization]   \n",
       "58                                           [acl-hlt]   \n",
       "59                                                 NaN   \n",
       "60                                   [no organization]   \n",
       "61                                   [no organization]   \n",
       "62                                                 NaN   \n",
       "63                                   [no organization]   \n",
       "64                                                 NaN   \n",
       "65                                                 NaN   \n",
       "66                 [swiss national science foundation]   \n",
       "67                                   [no organization]   \n",
       "68   [university of southern california signal anal...   \n",
       "69                                                 NaN   \n",
       "70                                                 NaN   \n",
       "71                                                 NaN   \n",
       "72                                                 NaN   \n",
       "73                                                 NaN   \n",
       "74                                                 NaN   \n",
       "75                                                 NaN   \n",
       "76                                                 NaN   \n",
       "77                                                 NaN   \n",
       "78                                                 NaN   \n",
       "79                                                 NaN   \n",
       "80                                                 NaN   \n",
       "81                                                 NaN   \n",
       "82                                                 NaN   \n",
       "83                                                 NaN   \n",
       "84                                                 NaN   \n",
       "85                                                 NaN   \n",
       "86                                                 NaN   \n",
       "87                                                 NaN   \n",
       "88                                                 NaN   \n",
       "89                                                 NaN   \n",
       "90                                                 NaN   \n",
       "91                                                 NaN   \n",
       "92                                                 NaN   \n",
       "93                                                 NaN   \n",
       "94                                                 NaN   \n",
       "95                                                 NaN   \n",
       "96                                                 NaN   \n",
       "97                                                 NaN   \n",
       "98                                                 NaN   \n",
       "99                                                 NaN   \n",
       "100                                                NaN   \n",
       "101                                                NaN   \n",
       "102                                                NaN   \n",
       "\n",
       "                                                 tasks  \\\n",
       "0    [semantic annotation, part-of-speech tagging, ...   \n",
       "1    [text classification, topic modeling, word emb...   \n",
       "2    [text classification, pre-trained language model]   \n",
       "3    [identifying malware sentences, classifying ma...   \n",
       "4                [translation, interpretation, debate]   \n",
       "5    [dialogue act recognition, dialogue act classi...   \n",
       "6    [fake news detection, language-based user repr...   \n",
       "7    [text annotation, audio recording, language le...   \n",
       "8    [text classification, text processing, text un...   \n",
       "9    [text classification, sequence labeling, seque...   \n",
       "10                    [classification, span detection]   \n",
       "11   [question answering, information retrieval, te...   \n",
       "12   [unsupervised term discovery, sign language re...   \n",
       "13                      [semantic similarity, ranking]   \n",
       "14                [database creation, speech research]   \n",
       "15   [machine translation, information retrieval, l...   \n",
       "16   [stylistic variation, syntactic features, pred...   \n",
       "17   [fake news detection, linguistic feature extra...   \n",
       "18   [information retrieval, text classification, t...   \n",
       "19   [conversation initiation, information retrieva...   \n",
       "20   [text classification, offensive language detec...   \n",
       "21   [question answering, information retrieval, te...   \n",
       "22                            [syntax, working-memory]   \n",
       "23   [cyberbullying detection, online abuse detecti...   \n",
       "24   [text classification, information extraction, ...   \n",
       "25   [text classification, feature engineering, dat...   \n",
       "26    [text classification, multimodal classification]   \n",
       "27   [terminology extraction, term selection, synon...   \n",
       "28   [automatic speech recognition, boundary detect...   \n",
       "29   [computational linguistics, contrastive analys...   \n",
       "30   [offensive language identification, code-mixin...   \n",
       "31   [parsing, syntactic annotation, information ex...   \n",
       "32         [event summarization, tweet classification]   \n",
       "33   [dialog system, speech recognition, text recog...   \n",
       "34   [named entity recognition, text classification...   \n",
       "35   [word segmentation, named entity recognition, ...   \n",
       "36   [text prediction, phrase completion, language ...   \n",
       "37   [shallow semantic parsing, natural language pr...   \n",
       "38   [stance classification, outcome prediction, im...   \n",
       "39   [identifying rumors, modeling microblog posts ...   \n",
       "40   [semantic parsing, coreference resolution, ver...   \n",
       "41   [topic detection, lexical coherence, conversat...   \n",
       "42              [information retrieval, accessibility]   \n",
       "43   [text generation, text summarization, text sim...   \n",
       "44   [speech recognition, natural language parsing,...   \n",
       "45   [cross-disciplinary research, new research ave...   \n",
       "46   [sentiment target clustering, subjective lexic...   \n",
       "47   [classification, information extraction, socia...   \n",
       "48   [stereotype identification, natural language i...   \n",
       "49   [identifying misogyny in tweets, zero-shot cla...   \n",
       "50   [automatic speech recognition, natural languag...   \n",
       "51   [statistical machine translation, relevance fe...   \n",
       "52   [text classification, offensive language detec...   \n",
       "53   [text processing, document curation, microserv...   \n",
       "54   [sentence classification, joint sentence class...   \n",
       "55             [sequence tagging, text classification]   \n",
       "56   [text classification, information extraction, ...   \n",
       "57   [news bias detection, clickbait detection, fak...   \n",
       "58   [document classification, phrase detection, co...   \n",
       "59   [information extraction, hypernym detection, t...   \n",
       "60   [text analysis, text generation, text summariz...   \n",
       "61   [automated speech recognition, corpus building...   \n",
       "62   [negation detection, unsupervised domain adapt...   \n",
       "63   [term disambiguation, all-word disambiguation,...   \n",
       "64   [event extraction, trigger identification, arg...   \n",
       "65         [machine translation, teaching, evaluation]   \n",
       "66   [relationship detection, parsing, sentence sel...   \n",
       "67   [dialogue segmentation, computational thinking...   \n",
       "68   [classification, feature engineering, explorat...   \n",
       "69   [natural language inference, text encoding, sy...   \n",
       "70   [hyperpartisan news detection, text classifica...   \n",
       "71    [text classification, debiasing word embeddings]   \n",
       "72   [deception detection, multimodal dataset const...   \n",
       "73   [spoken dialogue interaction, speech recogniti...   \n",
       "74   [translation dictation, automatic speech recog...   \n",
       "75                 [bias and stereotyping in nlp data]   \n",
       "76   [text classification, topic modeling, lexical ...   \n",
       "77   [text classification, information extraction, ...   \n",
       "78   [humor recognition, offense recognition, text ...   \n",
       "79   [text processing, information retrieval, machi...   \n",
       "80   [information extraction, ontology mapping, lin...   \n",
       "81   [textual data anonymization, deep reinforcemen...   \n",
       "82   [toxic comment classification, data integratio...   \n",
       "83   [text generation, text revision, text simplifi...   \n",
       "84   [text classification, bias detection, politica...   \n",
       "85   [sequence labeling, named entity recognition, ...   \n",
       "86   [dialogue symptom diagnosis, global attention,...   \n",
       "87   [automatic summarization, extractive summariza...   \n",
       "88   [textual entailment, information extraction, p...   \n",
       "89        [term extraction, word sense disambiguation]   \n",
       "90   [task1, task2, task3, task4, task5, task6, tas...   \n",
       "91   [information retrieval, information extraction...   \n",
       "92   [textbook question answering, multimodal conte...   \n",
       "93   [sign language generation, data annotation, su...   \n",
       "94   [information extraction, text processing, shal...   \n",
       "95   [text classification, information extraction, ...   \n",
       "96   [classification, semantic representation, lang...   \n",
       "97   [machine translation, data collection, data ex...   \n",
       "98   [transfer learning, feature engineering, demen...   \n",
       "99   [identifying the sentiment of a text, identify...   \n",
       "100  [emotion recognition, annotation of blended em...   \n",
       "101       [text classification, hate speech detection]   \n",
       "102      [authorship attribution, text classification]   \n",
       "\n",
       "                                               methods  \\\n",
       "0    [semantic annotation, contrastive analysis, me...   \n",
       "1    [topic modeling, word embedding, language mode...   \n",
       "2                                [xlm-roberta, tf-idf]   \n",
       "3    [conditional random fields, naive bayes classi...   \n",
       "4    [web resources, english-spanish literary trans...   \n",
       "5    [empirical study, corpus analysis, linguistic ...   \n",
       "6    [text representation, text classification, soc...   \n",
       "7          [text tagging, audio recording, annotation]   \n",
       "8    [lexical analysis, online processing, gaze-bas...   \n",
       "9    [hierarchical span-attribute tagging, sequence...   \n",
       "10   [pre-trained transformer-based models, oversam...   \n",
       "11   [lexical gap, quality of answer spans, limited...   \n",
       "12   [unsupervised learning, spoken term discovery,...   \n",
       "13         [cross-encoding, bert, semantic similarity]   \n",
       "14     [tokenization, part-of-speech tagging, parsing]   \n",
       "15   [machine translation, information retrieval, b...   \n",
       "16   [stylistic variation, syntactic features, writ...   \n",
       "17      [phobert, ensemble model, linguistic features]   \n",
       "18   [tokenization, part-of-speech tagging, depende...   \n",
       "19   [information retrieval, generation, crowd-sour...   \n",
       "20   [text classification, hierarchical attention, ...   \n",
       "21   [masked language models, entity-aware masking,...   \n",
       "22   [statistical modeling, working-memory capacity...   \n",
       "23     [data collection, data analysis, deep learning]   \n",
       "24   [syntax, semantics, co-reference, events, time...   \n",
       "25   [text classification, feature engineering, nat...   \n",
       "26                            [ocr, multimodal models]   \n",
       "27   [term selection and extraction, synset compari...   \n",
       "28   [automatic speech recognition, machine learnin...   \n",
       "29   [contrastive analysis, predictive power, typol...   \n",
       "30                [bert, multilingualism, code-mixing]   \n",
       "31             [parsing, dependency grammar, treebank]   \n",
       "32   [twitter event summarization, mixture model, b...   \n",
       "33   [speech recognition, text processing, vision p...   \n",
       "34   [named entity recognition, classifier, curricu...   \n",
       "35   [word segmentation, named entity recognition, ...   \n",
       "36   [text prediction, language modeling, contextua...   \n",
       "37   [shallow semantic parsing, machine learning, n...   \n",
       "38   [stance classification, outcome prediction, im...   \n",
       "39   [kernel learning, tree-based models, propagati...   \n",
       "40   [tree transducers, coreference resolution, ver...   \n",
       "41   [lexical coherence, topic modeling, conversati...   \n",
       "42   [rule-based approach, natural language process...   \n",
       "43     [tokenization, part-of-speech tagging, parsing]   \n",
       "44   [speech recognition, natural language parsing,...   \n",
       "45                         [method1, method2, method3]   \n",
       "46   [semi-supervised bootstrapping, hierarchical b...   \n",
       "47   [vote-based undersampling, linear support vect...   \n",
       "48   [stereotype analysis, natural language inferen...   \n",
       "49   [pre-training, transfer learning, zero-shot cl...   \n",
       "50   [collaborative data relabeling, automatic spee...   \n",
       "51   [statistical machine translation, relevance fe...   \n",
       "52   [bert, multi-layer perceptron, two-vote classi...   \n",
       "53     [tokenization, part-of-speech tagging, parsing]   \n",
       "54   [neural networks, artificial neural networks, ...   \n",
       "55            [lstm, transformer, relation extraction]   \n",
       "56   [text classification, feature engineering, pre...   \n",
       "57   [unification, joint modeling, representation l...   \n",
       "58   [lexical matching, hidden markov models, term ...   \n",
       "59   [hypernym detection, information extraction, h...   \n",
       "60   [example-based learning, pattern matching, cas...   \n",
       "61   [automated speech recognition, computer-assist...   \n",
       "62   [unsupervised domain adaptation, pseudo labeli...   \n",
       "63   [vector space model, unsupervised learning, st...   \n",
       "64   [supervised learning, event extraction, trigge...   \n",
       "65                         [method1, method2, method3]   \n",
       "66        [syntax-based filtering, dependency parsing]   \n",
       "67   [recurrent neural networks, long short-term me...   \n",
       "68   [automatic classification, word categories, pr...   \n",
       "69   [pre-trained text encoder, syntax encoder, fea...   \n",
       "70                 [supervised learning, tokenization]   \n",
       "71        [word embeddings, debiasing, classification]   \n",
       "72     [tokenization, part-of-speech tagging, parsing]   \n",
       "73   [speech recognition, dialogue management, inte...   \n",
       "74   [automatic speech recognition, statistical mac...   \n",
       "75             [stereotyping, bias, human-elicitation]   \n",
       "76   [identifying self-reported statements of diagn...   \n",
       "77   [text classification, part-of-speech tagging, ...   \n",
       "78                          [deberta, attention, bert]   \n",
       "79   [lexical analysis, part-of-speech tagging, par...   \n",
       "80   [ontology mapping, linked open data, semantic ...   \n",
       "81   [reinforcement learning, deep learning, text r...   \n",
       "82   [data integration, toxic comment classificatio...   \n",
       "83   [text generation, text revision, question gene...   \n",
       "84   [neural networks, text classification, corpus ...   \n",
       "85   [sequence labeling, named entity recognition, ...   \n",
       "86                   [global attention, symptom graph]   \n",
       "87   [textual entailment, information extraction, s...   \n",
       "88              [syntactic parsing, discourse parsing]   \n",
       "89   [automatic term extraction, linguistic filteri...   \n",
       "90    [multi-task learning, shared structure encoding]   \n",
       "91   [information retrieval, text classification, t...   \n",
       "92   [graph convolutional networks, self-supervised...   \n",
       "93   [supervised learning, data-driven modeling, tr...   \n",
       "94       [information extraction, clustering, metrics]   \n",
       "95   [data collection, data analysis, natural langu...   \n",
       "96   [semantic representation, contextual language ...   \n",
       "97   [machine translation, data extraction, languag...   \n",
       "98   [transfer learning, parallel corpus, lexicosyn...   \n",
       "99   [part-of-speech tagging, dependency parsing, v...   \n",
       "100  [emotion recognition, multimodal annotation, k...   \n",
       "101   [bert, attention mechanism, contextualized bert]   \n",
       "102  [authorship attribution, voting scheme, short ...   \n",
       "\n",
       "                                           task_scirex  \\\n",
       "0    [contrastive automatic analysis of verbs, medi...   \n",
       "1                          [legal area classification]   \n",
       "2    [text classification, hope speech detection, v...   \n",
       "3                                                   []   \n",
       "4             [higher education, literary translation]   \n",
       "5                                                   []   \n",
       "6    [fake news detection, language of fake news sp...   \n",
       "7                                    [conversion task]   \n",
       "8                                                   []   \n",
       "9                                                   []   \n",
       "10   [classification, classification tasks, social ...   \n",
       "11    [question answering, patient question answering]   \n",
       "12   [annotation process, sign language processing,...   \n",
       "13   [estimating relevance quality impact, ranking ...   \n",
       "14                                                  []   \n",
       "15        [machine translation, information retrieval]   \n",
       "16   [exploring stylistic variation, social media, ...   \n",
       "17   [vietnamese fake news detection, fake news det...   \n",
       "18   [factchecking, online disinformation, analysis...   \n",
       "19   [conversation systems, automatic and manual ev...   \n",
       "20   [offensive classification, automatically ident...   \n",
       "21   [domain adaptation, biomedical question - answ...   \n",
       "22                                                  []   \n",
       "23                                     [cyberbullying]   \n",
       "24                                                  []   \n",
       "25                                                  []   \n",
       "26   [hateful meme detection, detecting real world ...   \n",
       "27                                        [extraction]   \n",
       "28   [boundary detection, interactive search, topic...   \n",
       "29                                [linguistic inquiry]   \n",
       "30   [comment/post - level classification tasks, of...   \n",
       "31   [biomedical information extraction, informatio...   \n",
       "32   [event summarization, summarizing the twitter ...   \n",
       "33       [collaborative research, emotion recognition]   \n",
       "34   [legal nerc, human annotation, curriculum lear...   \n",
       "35   [safety information mining, relief efforts, wo...   \n",
       "36   [text prediction, chat scenarios, real - time ...   \n",
       "37                               [biological research]   \n",
       "38   [group decision - making, nlp tasks, outcome p...   \n",
       "39     [identifying rumors, microblog posts diffusion]   \n",
       "40   [sat algebra word problems, verb interpretation)]   \n",
       "41   [detecting mild cognitiveimpairment, conversat...   \n",
       "42                [mathematical information retrieval]   \n",
       "43                                                  []   \n",
       "44                           [battlefield simulations]   \n",
       "45                                                  []   \n",
       "46   [subjective lexicons extraction, sentiment pre...   \n",
       "47                           [adr classification task]   \n",
       "48   [analyzing stereotypes, generative text infere...   \n",
       "49   [detecting misogyny, identifying misogyny, mul...   \n",
       "50   [online a/b testing, intelligent personal assi...   \n",
       "51   [personalization of web search, web search per...   \n",
       "52                        [categorizing offensiveness]   \n",
       "53   [natural legal language processing, prototype ...   \n",
       "54   [joint sentence classification, sequential sen...   \n",
       "55   [text classification, relation extraction, det...   \n",
       "56                    [detecting deception techniques]   \n",
       "57   [detecting news bias, verifying rumors, unifyi...   \n",
       "58   [automatic coding of clinical documents, clini...   \n",
       "59                                    [classification]   \n",
       "60                                                  []   \n",
       "61    [data collection, fine - grained error analysis]   \n",
       "62   [semantic processing, adaptation, source - fre...   \n",
       "63   [all - word disambiguation, biomedical term di...   \n",
       "64   [classification, event extraction, trigger ide...   \n",
       "65   [mt, machine translation, teaching machine tra...   \n",
       "66                             [interaction detection]   \n",
       "67   [microgenetic analysis of cognitive interactio...   \n",
       "68   [automatic classifiers, automatic identification]   \n",
       "69   [biomedical natural language inference, biomed...   \n",
       "70              [detecting hyperpartisan news article]   \n",
       "71   [text classification, reduced gender bias, occ...   \n",
       "72                               [deception detection]   \n",
       "73   [spoken dialogue interaction, dialogue managem...   \n",
       "74   [translation, french asr, translation dictatio...   \n",
       "75                             [bias and stereotyping]   \n",
       "76                                     [mental health]   \n",
       "77              [extracting patient clinical profiles]   \n",
       "78            [detecting and rating humor and offense]   \n",
       "79   [lexiploigissi *, lexiploigissi, educational p...   \n",
       "80            [semantic web context, ontology mapping]   \n",
       "81   [manipulating text representations, user behav...   \n",
       "82                      [toxic comment classification]   \n",
       "83                                                  []   \n",
       "84      [automatic detection of bias, bias assessment]   \n",
       "85   [rnn based sequence labeling, phrase detection...   \n",
       "86   [natural language processing, dialogue symptom...   \n",
       "87   [automated related work summarization, automat...   \n",
       "88          [automatic extraction of reasoning chains]   \n",
       "89                         [automatic term extraction]   \n",
       "90                                                  []   \n",
       "91             [personalisation of digital newspapers]   \n",
       "92   [textbook question answering (tqa) task, tqa p...   \n",
       "93   [sign language generation, linguistics of sign...   \n",
       "94   [automatic processing of hospital patient reco...   \n",
       "95                                                  []   \n",
       "96                                                  []   \n",
       "97            [japanese to english patent translation]   \n",
       "98   [automatic detection of alzheimer's disease (a...   \n",
       "99                              [stock price movement]   \n",
       "100  [bimodal display of emotion), cooperation, emo...   \n",
       "101                                                 []   \n",
       "102  [sockpuppet detection, sockpuppeteering, detec...   \n",
       "\n",
       "                                         method_scirex  \\\n",
       "0                                [semantic annotation]   \n",
       "1    [text classifiers, machine learning (\"ml\") app...   \n",
       "2         [xlm - roberta pre - trained language model]   \n",
       "3    [supervised learning approach, na\\\"\\ive bayes ...   \n",
       "4                                 [course methodology]   \n",
       "5                                 [linguistic devices]   \n",
       "6              [language - based user representations]   \n",
       "7                     [crowdsourcing techniques, lara]   \n",
       "8                       [online processing techniques]   \n",
       "9      [deep learning approaches, curriculum learning]   \n",
       "10   [oversampling, span detection models, classifi...   \n",
       "11   [question answering system, question answering...   \n",
       "12           [sign language recognition (slr) systems]   \n",
       "13   [high - precision crossencoder bert model, hig...   \n",
       "14                                                  []   \n",
       "15   [human language technologies, language technol...   \n",
       "16                                                  []   \n",
       "17                                   [ensemble method]   \n",
       "18   [prta, rhetorical and psychological techniques...   \n",
       "19                              [conversation systems]   \n",
       "20   [explanation tool, capsule system, hierarchica...   \n",
       "21   [masked lms, masked language models, neural ar...   \n",
       "22       [l2 learners, wm - based processing analysis]   \n",
       "23                                                  []   \n",
       "24                                                  []   \n",
       "25   [classification algorithms, natural language p...   \n",
       "26   [multimodal models, machine learning systems, ...   \n",
       "27          [iwn model, eurowordnet/italwordnet model]   \n",
       "28   [automatic speech recognition techniques, mach...   \n",
       "29   [contrastive analysis, bootstrapping approach,...   \n",
       "30    [multilingual bert model, fine - tuning methods]   \n",
       "31   [unifying syntax formalism, syntactic annotati...   \n",
       "32   [participantbased approach, mixture model, par...   \n",
       "33                                         [adviser 1]   \n",
       "34   [wikipediabased approach, lkif, curriculum lea...   \n",
       "35                                               [nlp]   \n",
       "36   [text prediction algorithms, large language mo...   \n",
       "37                       [machine learning approaches]   \n",
       "38   [bert contextualized word embeddings, language...   \n",
       "39   [rumor detection models, propagation tree kern...   \n",
       "40   [tree transducer cascade, cascade of tree tran...   \n",
       "41                                                  []   \n",
       "42                        [machine reading approaches]   \n",
       "43                                         [reescreve]   \n",
       "44   [open agent architecture, modsaf battlefield s...   \n",
       "45                                                  []   \n",
       "46   [semi - supervised bootstrapping algorithm, bo...   \n",
       "47   [ensemble, linear support vector machine, vote...   \n",
       "48                                       [nlp systems]   \n",
       "49   [single - language bert models, error analysis...   \n",
       "50   [collaborative data relabeling, cdr, skill rec...   \n",
       "51   [search engine, noisy channel model, statistic...   \n",
       "52   [bert, ltl - ude's systems, embedding represen...   \n",
       "53                                                  []   \n",
       "54   [neural networks, ann models, sentence classif...   \n",
       "55   [lstm baselines, propaganda technique classifi...   \n",
       "56           [automatic systems, deception techniques]   \n",
       "57   [unifiedm2, general - purpose misinformation m...   \n",
       "58                                [discriminative hmm]   \n",
       "59   [construction method, hypernym detection, hier...   \n",
       "60   [query engine, generation module, example - ba...   \n",
       "61   [natural language processing, automated speech...   \n",
       "62                      [negationaware pre - training]   \n",
       "63       [senseclusters, unsupervised vector approach]   \n",
       "64                                                  []   \n",
       "65   [educational toy systems, architectures of ``t...   \n",
       "66                   [dependency parser, demonstrator]   \n",
       "67   [linear chain crf model, linear chain crfs, re...   \n",
       "68                                                  []   \n",
       "69   [feature encoder, ensemble models, syntax enco...   \n",
       "70   [tintin, supervised learning procedure, superv...   \n",
       "71                  [debiasing embeddings, classifier]   \n",
       "72                              [statistical analysis]   \n",
       "73                                                  []   \n",
       "74   [asr, human translator, smt, asr language mode...   \n",
       "75                      [human - elicitation protocol]   \n",
       "76                             [linguistic components]   \n",
       "77                    [sentence classification system]   \n",
       "78       [deberta model, language models, hahackathon]   \n",
       "79               [educational platform, lexiploigissi]   \n",
       "80                            [bootstrapping paradigm]   \n",
       "81   [rlta, deep reinforcement learning, text repre...   \n",
       "82                                                  []   \n",
       "83             [title revision wizard, support system]   \n",
       "84                                      [neural model]   \n",
       "85   [structured prediction models, crf - lstm mode...   \n",
       "86                        [global attention mechanism]   \n",
       "87   [rewos, prototype related work summarization s...   \n",
       "88   [syntactic and discourse parsers, graph - base...   \n",
       "89    [linguistic , lexical and statistical filtering]   \n",
       "90                        [naÃ¯ve multi - task methods]   \n",
       "91   [user model, personalisation system, personali...   \n",
       "92                      [graph convolutional networks]   \n",
       "93   [modeling intensification, transformer models,...   \n",
       "94   [ie, information extraction approach, ie proto...   \n",
       "95                              [wikipedia components]   \n",
       "96   [semantic representation, machine learning met...   \n",
       "97   [japanese to english translation system, mt sy...   \n",
       "98               [machine learning, transfer learning]   \n",
       "99               [discrete machine learning algorithm]   \n",
       "100  [multimodal annotation scheme, logistic regres...   \n",
       "101  [modular pipeline classifier, attention mechan...   \n",
       "102  [authorship attribution methods, human process...   \n",
       "\n",
       "                                          organization  \\\n",
       "0                                           [anr, dga]   \n",
       "1                                                   []   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                                      [uma, dev, sci]   \n",
       "5                                                   []   \n",
       "6    [netherlands organisation for scientific resea...   \n",
       "7                                                   []   \n",
       "8    [political economy of reforms, university of m...   \n",
       "9                                                   []   \n",
       "10                                                  []   \n",
       "11              [australian research training program]   \n",
       "12                                                  []   \n",
       "13                                                  []   \n",
       "14                                             [darpa]   \n",
       "15   [european commission, nemlar, medar, medar, eu...   \n",
       "16                          [templeton religion trust]   \n",
       "17                                                  []   \n",
       "18   [propaganda analysis project 7, qatar computin...   \n",
       "19                                                  []   \n",
       "20   [national language commission key research pro...   \n",
       "21   [epsrc, yh, a turing ai fellowship, uk researc...   \n",
       "22   [national research foundation of korea grant, ...   \n",
       "23                                                  []   \n",
       "24   [u.s. defense advanced research project agency...   \n",
       "25   [computational linguistics, search and informa...   \n",
       "26                                                  []   \n",
       "27                                                  []   \n",
       "28                                                  []   \n",
       "29   [center for brains, minds, machines, cbmm, nsf...   \n",
       "30                                                  []   \n",
       "31                           [lll, academy of finland]   \n",
       "32   [bosch research and technology center, natural...   \n",
       "33                                                  []   \n",
       "34                                                  []   \n",
       "35                                                  []   \n",
       "36              [microsoft search, intelligence, msai]   \n",
       "37                                                  []   \n",
       "38                                                  []   \n",
       "39                             [general research fund]   \n",
       "40                                                  []   \n",
       "41   [oregon roybal center for aging and technology...   \n",
       "42   [defense advanced research projects agency, da...   \n",
       "43                                                  []   \n",
       "44                         [nuance, government, darpa]   \n",
       "45   [avt, iyba, department of biotechnology, gover...   \n",
       "46                                             [emnlp]   \n",
       "47                                                  []   \n",
       "48                      [clip, university of maryland]   \n",
       "49                                                  []   \n",
       "50                                                  []   \n",
       "51                                                  []   \n",
       "52                                                  []   \n",
       "53              [lynx, european union's, horizon 2020]   \n",
       "54                [philips research, philips research]   \n",
       "55                                                  []   \n",
       "56   [qatar computing research institute, hbku, mit...   \n",
       "57                                                  []   \n",
       "58                                           [acl-hlt]   \n",
       "59                                                  []   \n",
       "60                                                  []   \n",
       "61                                                  []   \n",
       "62                                                  []   \n",
       "63                                                  []   \n",
       "64      [research foundation flanders, fwo, sentivent]   \n",
       "65                                                  []   \n",
       "66                 [swiss national science foundation]   \n",
       "67                                                  []   \n",
       "68   [signal analysis and interpretation laboratory...   \n",
       "69                                                  []   \n",
       "70          [fonds de la recherche scientifique -fnrs]   \n",
       "71                                                  []   \n",
       "72   [national science foundation, national science...   \n",
       "73                     [u.s. army research laboratory]   \n",
       "74                 [nrc, translation bureau of canada]   \n",
       "75   [jhu human language technology center of excel...   \n",
       "76                                                  []   \n",
       "77                                                  []   \n",
       "78                                                  []   \n",
       "79                                                  []   \n",
       "80                                                  []   \n",
       "81                                               [onr]   \n",
       "82                                                  []   \n",
       "83                                                  []   \n",
       "84   [german research foundation, dfg, collaborativ...   \n",
       "85   [umassmed, national institutes of health, nih,...   \n",
       "86   [national natural science foundation of china,...   \n",
       "87                                                  []   \n",
       "88                                                  []   \n",
       "89                                                  []   \n",
       "90            [kddi foundation research grant program]   \n",
       "91                                             [yahoo]   \n",
       "92                                                  []   \n",
       "93                                                  []   \n",
       "94                   [bulgarian national science fund]   \n",
       "95                                               [dfg]   \n",
       "96                                                  []   \n",
       "97                                                  []   \n",
       "98                                                  []   \n",
       "99                                                  []   \n",
       "100                                                 []   \n",
       "101                   [european union's, horizon 2020]   \n",
       "102                                              [onr]   \n",
       "\n",
       "                                         organization2  \n",
       "0                                           [anr, dga]  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                      [uma, dev, sci]  \n",
       "5                                                  NaN  \n",
       "6    [netherlands organisation for scientific resea...  \n",
       "7                                                  NaN  \n",
       "8    [political economy of reforms, university of m...  \n",
       "9                                                  NaN  \n",
       "10                                                 NaN  \n",
       "11              [australian research training program]  \n",
       "12                                                 NaN  \n",
       "13                                                 NaN  \n",
       "14                                             [darpa]  \n",
       "15   [european commission, nemlar, medar, medar, eu...  \n",
       "16                          [templeton religion trust]  \n",
       "17                                                 NaN  \n",
       "18   [propaganda analysis project 7, qatar computin...  \n",
       "19                                                 NaN  \n",
       "20   [national language commission key research pro...  \n",
       "21   [epsrc, yh, a turing ai fellowship, uk researc...  \n",
       "22   [national research foundation of korea grant, ...  \n",
       "23                                                 NaN  \n",
       "24   [u.s. defense advanced research project agency...  \n",
       "25   [computational linguistics, search and informa...  \n",
       "26                                                 NaN  \n",
       "27                                                 NaN  \n",
       "28                                                 NaN  \n",
       "29   [center for brains, minds, machines, cbmm, nsf...  \n",
       "30                                                 NaN  \n",
       "31                           [lll, academy of finland]  \n",
       "32   [bosch research and technology center, natural...  \n",
       "33                                                 NaN  \n",
       "34                                                 NaN  \n",
       "35                                                 NaN  \n",
       "36              [microsoft search, intelligence, msai]  \n",
       "37                                                 NaN  \n",
       "38                                                 NaN  \n",
       "39                             [general research fund]  \n",
       "40                                                 NaN  \n",
       "41   [oregon roybal center for aging and technology...  \n",
       "42   [defense advanced research projects agency, da...  \n",
       "43                                                 NaN  \n",
       "44                         [nuance, government, darpa]  \n",
       "45   [avt, iyba, department of biotechnology, gover...  \n",
       "46                                             [emnlp]  \n",
       "47                                                 NaN  \n",
       "48                      [clip, university of maryland]  \n",
       "49                                                 NaN  \n",
       "50                                                 NaN  \n",
       "51                                                 NaN  \n",
       "52                                                 NaN  \n",
       "53              [lynx, european union's, horizon 2020]  \n",
       "54                [philips research, philips research]  \n",
       "55                                                 NaN  \n",
       "56   [qatar computing research institute, hbku, mit...  \n",
       "57                                                 NaN  \n",
       "58                                           [acl-hlt]  \n",
       "59                                                 NaN  \n",
       "60                                                 NaN  \n",
       "61                                                 NaN  \n",
       "62                                                 NaN  \n",
       "63                                                 NaN  \n",
       "64      [research foundation flanders, fwo, sentivent]  \n",
       "65                                                 NaN  \n",
       "66                 [swiss national science foundation]  \n",
       "67                                                 NaN  \n",
       "68   [signal analysis and interpretation laboratory...  \n",
       "69                                                 NaN  \n",
       "70          [fonds de la recherche scientifique -fnrs]  \n",
       "71                                                 NaN  \n",
       "72   [national science foundation, national science...  \n",
       "73                     [u.s. army research laboratory]  \n",
       "74                 [nrc, translation bureau of canada]  \n",
       "75   [jhu human language technology center of excel...  \n",
       "76                                                 NaN  \n",
       "77                                                 NaN  \n",
       "78                                                 NaN  \n",
       "79                                                 NaN  \n",
       "80                                                 NaN  \n",
       "81                                               [onr]  \n",
       "82                                                 NaN  \n",
       "83                                                 NaN  \n",
       "84   [german research foundation, dfg, collaborativ...  \n",
       "85   [umassmed, national institutes of health, nih,...  \n",
       "86   [national natural science foundation of china,...  \n",
       "87                                                 NaN  \n",
       "88                                                 NaN  \n",
       "89                                                 NaN  \n",
       "90            [kddi foundation research grant program]  \n",
       "91                                             [yahoo]  \n",
       "92                                                 NaN  \n",
       "93                                                 NaN  \n",
       "94                   [bulgarian national science fund]  \n",
       "95                                               [dfg]  \n",
       "96                                                 NaN  \n",
       "97                                                 NaN  \n",
       "98                                                 NaN  \n",
       "99                                                 NaN  \n",
       "100                                                NaN  \n",
       "101                   [european union's, horizon 2020]  \n",
       "102                                              [onr]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c9137b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Automatic Distinction between Specialized and Non-Specialized Occurrences of Verbs in Medical Corpora. The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with.\n",
      "['semantic annotation']\n",
      "['semantic annotation', 'contrastive analysis', 'medical terminology']\n",
      "0.5\n",
      "Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments. This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain.\n",
      "['topic model', 'word embedding', 'language model']\n",
      "['topic modeling', 'word embedding', 'language modeling']\n",
      "0.5\n",
      "TEAM HUB@LT-EDI-EACL2021: Hope Speech Detection Based On Pre-trained Language Model. This article introduces the system description of TEAM HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.\n",
      "['language model', 'xlm-roberta', 'tf-idf']\n",
      "['xlm-roberta', 'tf-idf']\n",
      "1.0\n",
      "Flytxt\\_NTNU at SemEval-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers. Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57% for subTask1 and 28% for subTask2.\n",
      "['supervised learning', 'conditional random fields']\n",
      "['conditional random fields', 'naive bayes classifiers']\n",
      "0.5\n",
      "Compiling an Interactive Literary Translation Web Site for Education Purposes. The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de MÃ¡laga are offered the possibility to take an English-Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation-aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.\n",
      "['web site']\n",
      "['web resources', 'english-spanish literary translation', 'humanities', 'mailing list', 'debate forum', 'chat rooms']\n",
      "0.0\n",
      "Avoiding and Resolving Initiative Conflicts in Dialogue. In this paper, we report on an empirical study on initiative conflicts in human-human conversation. We examined these conflicts in two corpora of task-oriented dialogues. The results show that conversants try to avoid initiative conflicts, but when these conflicts occur, they are efficiently resolved by linguistic devices, such as volume.\n",
      "['empirical study']\n",
      "['empirical study', 'corpus analysis', 'linguistic analysis']\n",
      "0.5\n",
      "Words are the Window to the Soul: Language-based User Representations for Fake News Detection. Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.\n",
      "['language-based user representations']\n",
      "['text representation', 'text classification', 'social network analysis']\n",
      "0.0\n",
      "Constructing Multimodal Language Learner Texts Using LARA: Experiences with Nine Languages. LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by language learners. This involves semi-automatically tagging the text, adding other annotations and recording audio. The platform is suitable for creating texts in multiple languages via crowdsourcing techniques that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in Dutch, English, Farsi, French, German, Icelandic, Irish, Swedish and Turkish. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form.\n",
      "['learning and reading assistant']\n",
      "['text tagging', 'audio recording', 'annotation']\n",
      "0.0\n",
      "Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers. Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).\n",
      "['lexical properties', ' parallel gaze data']\n",
      "['lexical analysis', 'online processing', 'gaze-based measures']\n",
      "0.0\n",
      "Extracting Symptoms and their Status from Clinical Conversations. This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.\n",
      "['curriculum learning', 'sequence-to-sequence', 'hierarchical span-attribute tagging (sa-t) model']\n",
      "['hierarchical span-attribute tagging', 'sequence-to-sequence model', 'deep learning']\n",
      "0.0\n",
      "Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications. This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1-score than the median in Task 1b.\n",
      "['pre-trained transformer', 'classifier ensembling']\n",
      "['pre-trained transformer-based models', 'oversampling', 'classifier ensembling']\n",
      "0.0\n",
      "Question Answering in the Biomedical Domain. Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low.\n",
      "['question answering system']\n",
      "['lexical gap', 'quality of answer spans', 'limited annotated data']\n",
      "0.0\n",
      "Unsupervised Term Discovery for Continuous Sign Language. Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before.\n",
      "['unsupervised term discovery']\n",
      "['unsupervised learning', 'spoken term discovery', 'hand shape and pose features']\n",
      "0.0\n",
      "Improving Relevance Quality in Product Search using High-Precision Query-Product Semantic Similarity. Ensuring relevance quality in product search is a critical task as it impacts the customer's ability to find intended products in the short-term as well as the general perception and trust of the e-commerce system in the long term. In this work we leverage a high-precision crossencoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline-generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re-ranking feature covering head/torso queries, and (3) as a training objective for optimization. We present results on effectiveness of this strategy for the large e-commerce setting, which has general applicability for choice of other high-precision models and tasks in ranking.\n",
      "['bert']\n",
      "['cross-encoding', 'bert', 'semantic similarity']\n",
      "0.5\n",
      "Initial Draft Guidelines for the Development of the Next-Generation Spoken Language Systems Speech Research Database. To best serve the strategic needs of the DARPA SLS research program by creating the next-generation speech database(s).\n",
      "['guidelines']\n",
      "['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "0.0\n",
      "MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic. After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.\n",
      "['surveys', 'questionnaires']\n",
      "['machine translation', 'information retrieval', 'blark']\n",
      "0.0\n",
      "Exploring Stylistic Variation with Age and Income on Twitter. Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.\n",
      "['regression', 'stylistic features']\n",
      "['stylistic variation', 'syntactic features', 'writing style']\n",
      "0.0\n",
      "ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings. Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https://gitlab.com/thuan.\n",
      "['phobert embeddings', 'ensemble method']\n",
      "['phobert', 'ensemble model', 'linguistic features']\n",
      "0.0\n",
      "Prta: A System to Support the Analysis of Propaganda Techniques in the News. Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https://www.tanbih.org/prta.\n",
      "['propaganda persuasion techniques analyzer']\n",
      "['tokenization', 'part-of-speech tagging', 'dependency parsing']\n",
      "0.0\n",
      "Conversation Initiation by Diverse News Contents Introduction. In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open-domain nontask-oriented conversations and chitchat .\n",
      "['information retrieval', 'generation models']\n",
      "['information retrieval', 'generation', 'crowd-sourcing', 'automatic evaluation', 'manual evaluation']\n",
      "0.5\n",
      "Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool. Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.\n",
      "['hierarchical attention capsule network', 'integrated gradients']\n",
      "['text classification', 'hierarchical attention', 'capsule networks', 'explanation tool']\n",
      "0.0\n",
      "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies. Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.\n",
      "['masked language models', 'language models', 'entity-aware masking']\n",
      "['masked language models', 'entity-aware masking', 'biomedical entity-aware masking']\n",
      "1.0\n",
      "A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for L2 Learners. The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.\n",
      "['grammatical analysis', 'working-memory-based processing']\n",
      "['statistical modeling', 'working-memory capacity', 'island effects']\n",
      "0.0\n",
      "A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection. In this paper, we introduce a new English Twitter-based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.\n",
      "['transformers', 'dataset']\n",
      "['data collection', 'data analysis', 'deep learning']\n",
      "0.0\n",
      "The N2 corpus: A semantically annotated collection of Islamist extremist stories. We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually doubleannotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure.\n",
      "['multi-layed annotation', 'annotation procedure']\n",
      "['syntax', 'semantics', 'co-reference', 'events', 'time expressions', 'temporal relationships', 'semantic roles', 'word senses']\n",
      "0.0\n",
      "Detecting Cognitive Distortions from Patient-Therapist Interactions. An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.\n",
      "['pretrained sentence-bert embeddings', 'svm classifier']\n",
      "['text classification', 'feature engineering', 'natural language processing']\n",
      "0.0\n",
      "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text-and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\n",
      "['ocr', 'multimodal models']\n",
      "['ocr', 'multimodal models']\n",
      "1.0\n",
      "Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria. This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy/hyponymy relation.\n",
      "['encoding terms']\n",
      "['term selection and extraction', 'synset comparison and evaluation', 'coding of new terms']\n",
      "0.0\n",
      "Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections. In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.\n",
      "['automated clustering', 'automatic speech recognition techniques']\n",
      "['automatic speech recognition', 'machine learning', 'word error rates']\n",
      "0.0\n",
      "Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL. This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.\n",
      "['bootstrapping']\n",
      "['contrastive analysis', 'predictive power', 'typology']\n",
      "0.0\n",
      "HUB@DravidianLangTech-EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media. This paper introduces the system description of the HUB team participating in Dravidian-LangTech-EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results.\n",
      "['multilingual bert']\n",
      "['bert', 'multilingualism', 'code-mixing']\n",
      "0.0\n",
      "On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA. Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction.\n",
      "['dependency schemes', 'bioinfer', 'genia treebank']\n",
      "['parsing', 'dependency grammar', 'treebank']\n",
      "0.0\n",
      "A Participant-based Approach for Event Summarization Using Twitter Streams. Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant-based event summarization approach that \"zooms-in\" the Twitter event streams to the participant level, detects the important sub-events associated with each participant using a novel mixture model that combines the \"burstiness\" and \"cohesiveness\" properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art.\n",
      "['participant-based approach']\n",
      "['twitter event summarization', 'mixture model', 'burstiness', 'cohesiveness']\n",
      "0.0\n",
      "ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents. We present ADVISER 1-an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.\n",
      "['python']\n",
      "['speech recognition', 'text processing', 'vision processing']\n",
      "0.0\n",
      "Legal NERC with ontologies, Wikipedia and curriculum learning. In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.\n",
      "['named entity recognizer', 'classifier']\n",
      "['named entity recognition', 'classifier', 'curriculum learning']\n",
      "0.5\n",
      "Safety Information Mining --- What can NLP do in a disaster---. This paper describes efforts of NLP researchers to create a system to aid the relief efforts during the 2011 East Japan Earthquake. Specifically, we created a system to mine information regarding the safety of people in the disaster-stricken area from Twitter, a massive yet highly unorganized information source. We describe the large scale collaborative effort to rapidly create robust and effective systems for word segmentation, named entity recognition, and tweet classification. As a result of our efforts, we were able to effectively deliver new information about the safety of over 100 people in the disasterstricken area to a central repository for safety information.\n",
      "['robust and effective systems']\n",
      "['word segmentation', 'named entity recognition', 'tweet classification']\n",
      "0.0\n",
      "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages. Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.\n",
      "['large language models']\n",
      "['text prediction', 'language modeling', 'contextual signal']\n",
      "0.0\n",
      "An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols. We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community. 1 1 The dataset is available on the authors' websites.\n",
      "['annotated corpus']\n",
      "['shallow semantic parsing', 'machine learning', 'natural language processing']\n",
      "0.0\n",
      "Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision-Making. In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.\n",
      "['corpus', 'bert']\n",
      "['stance classification', 'outcome prediction', 'impact assessment']\n",
      "0.0\n",
      "Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning. How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.\n",
      "['propogation trees', 'kernel-based method']\n",
      "['kernel learning', 'tree-based models', 'propagation tree kernel']\n",
      "0.0\n",
      "Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers. We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions-the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system (called EU-CLID) propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results (43% recall and 91% precision) on SAT algebra word problems. We also apply EUCLID to the public Dolphin algebra question set, and improve the state-of-the-art F 1-score from 73.9% to 77.0%.\n",
      "['tree transducer cascade']\n",
      "['tree transducers', 'coreference resolution', 'verb interpretation']\n",
      "0.0\n",
      "Topic-Based Measures of Conversation for Detecting Mild CognitiveImpairment. Conversation is a complex cognitive task that engages multiple aspects of cognitive functions to remember the discussed topics, monitor the semantic and linguistic elements, and recognize others' emotions. In this paper, we propose a computational method based on the lexical coherence of consecutive utterances to quantify topical variations in semistructured conversations of older adults with cognitive impairments. Extracting the lexical knowledge of conversational utterances, our method generates a set of novel conversational measures that indicate underlying cognitive deficits among subjects with mild cognitive impairment (MCI). Our preliminary results verify the utility of the proposed conversation-based measures in distinguishing MCI from healthy controls.\n",
      "['lexical coherence of consecutive utterances']\n",
      "['lexical coherence', 'topic modeling', 'conversation analysis']\n",
      "0.0\n",
      "MathAlign: Linking Formula Identifiers to their Contextual Natural Language Descriptions. Extending machine reading approaches to extract mathematical concepts and their descriptions is useful for a variety of tasks, ranging from mathematical information retrieval to increasing accessibility of scientific documents for the visually impaired. This entails segmenting mathematical formulae into identifiers and linking them to their natural language descriptions. We propose a rule-based approach for this task, which extracts L A T E X representations of formula identifiers and links them to their in-text descriptions, given only the original PDF and the location of the formula of interest. We also present a novel evaluation dataset for this task, as well as the tool used to create it.\n",
      "['rule-based approach']\n",
      "['rule-based approach', 'natural language processing', 'machine reading']\n",
      "0.5\n",
      "ReEscreve: a Translator-friendly Multi-purpose Paraphrasing Software Tool. \n",
      "['multi-purpose paraphrasing software tool']\n",
      "['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "0.0\n",
      "CommandTalk: A Spoken-Language Interface for Battlefield Simulations. CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual-interpretation modhle, a \"push-to-talk\" agent, the ModSAF battlefield simulator, and \"Start-It\" (a graphical processing-spawning agent). Com-mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod-SAF system functions include the following: Show contour lines. Center on M1 platoon.\n",
      "['nuance speech recognition system', 'gemini natural language parsing', 'contextual-interpretation module', 'spoken-language interface']\n",
      "['speech recognition', 'natural language parsing', 'contextual interpretation']\n",
      "0.0\n",
      "Parallels between Linguistics and Biology. In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues.\n",
      "['parallel construction', 'analogies']\n",
      "['method1', 'method2', 'method3']\n",
      "0.0\n",
      "Sentiment Analysis on the People's Daily. We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.\n",
      "['semi-supervised bootstrapping algorithm']\n",
      "['semi-supervised bootstrapping', 'hierarchical bayesian model']\n",
      "0.0\n",
      "BIGODM System in the Social Media Mining for Health Applications Shared Task 2019. In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.\n",
      "['support vector machines', 'word embedding', 'linear kernel', 'bag-of-word', 'domain-knowledge', 'negation']\n",
      "['vote-based undersampling', 'linear support vector machine', 'bag-of-word', 'domain knowledge', 'negation', 'word embedding']\n",
      "0.0\n",
      "Analyzing Stereotypes in Generative Text Inference Tasks. Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non-binary person, trans man, trans woman, cis man, cis woman\n",
      "['annotation', 'human judgement']\n",
      "['stereotype analysis', 'natural language inference', 'commonsense inference']\n",
      "0.0\n",
      "A Checkpoint on Multilingual Misogyny Identification. We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.\n",
      "['transformers', 'bert']\n",
      "['pre-training', 'transfer learning', 'zero-shot classification']\n",
      "0.0\n",
      "Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants. Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A/B testing results show significant gain of customer experience metrics.\n",
      "['collaborative data relabeling']\n",
      "['collaborative data relabeling', 'automatic speech recognition', 'natural language understanding']\n",
      "0.5\n",
      "Statistical Machine Translation Models for Personalized Search. Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.\n",
      "['statistical machine translation models', 'relevance feedback']\n",
      "['statistical machine translation', 'relevance feedback', 'noisy channel model']\n",
      "0.5\n",
      "LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness. This paper describes LTL-UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).\n",
      "['embedding representation', 'multi-layer perceptron', 'bert']\n",
      "['bert', 'multi-layer perceptron', 'two-vote classification']\n",
      "1.0\n",
      "Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services. We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.\n",
      "['content and document curation workflow manager']\n",
      "['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "0.0\n",
      "Neural Networks for Joint Sentence Classification in Medical Paper Abstracts. Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-ofthe-art results on two different datasets for sequential sentence classification in medical abstracts.\n",
      "['artificial neural networks']\n",
      "['neural networks', 'artificial neural networks', 'sentence classification']\n",
      "0.5\n",
      "NoPropaganda at SemEval-2020 Task 11: A Borrowed Approach to Sequence Tagging and Text Classification. This paper describes our contribution to SemEval-2020 Task 11: Detection Of Propaganda Techniques In News Articles. We start with simple LSTM baselines and move to an autoregressive transformer decoder to predict long continuous propaganda spans for the first subtask. We also adopt an approach from relation extraction by enveloping spans mentioned above with special tokens for the second subtask of propaganda technique classification. Our models report an F-score of 44.6% and a micro-averaged F-score of 58.2% for those tasks accordingly.\n",
      "['lstm', 'autoregressive transformer decoder']\n",
      "['lstm', 'transformer', 'relation extraction']\n",
      "0.5\n",
      "Interpretable Propaganda Detection in News Articles. Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.\n",
      "['qualitatively descriptive features', 'pre-trained language models']\n",
      "['text classification', 'feature engineering', 'pre-trained language models']\n",
      "0.0\n",
      "On Unifying Misinformation Detection. In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. â€  Work partially done while working at Facebook AI.\n",
      "['few-shot learning', 'unifiedm2']\n",
      "['unification', 'joint modeling', 'representation learning']\n",
      "0.0\n",
      "Lexically-Triggered Hidden Markov Models for Clinical Document Coding. The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.\n",
      "[' lexically-triggered hidden markov model']\n",
      "['lexical matching', 'hidden markov models', 'term dictionary']\n",
      "0.0\n",
      "Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method. On a scientific concept hierarchy, a parent concept may have a few attributes, each of which has multiple values being a group of child concepts. We call these attributes facets: classification has a few facets such as application (e.g., face recognition), model (e.g., svm, knn), and metric (e.g., precision). In this work, we aim at building faceted concept hierarchies from scientific literature. Hierarchy construction methods heavily rely on hypernym detection, however, the faceted relations are parent-to-child links but the hypernym relation is a multi-hop, i.e., ancestor-todescendent link with a specific facet \"type-of\". We use information extraction techniques to find synonyms, sibling concepts, and ancestordescendent relations from a data science corpus. And we propose a hierarchy growth algorithm to infer the parent-child links from the three types of relationships. It resolves conflicts by maintaining the acyclic structure of a hierarchy.\n",
      "['information extraction techniques']\n",
      "['hypernym detection', 'information extraction', 'hierarchy growth algorithm']\n",
      "0.0\n",
      "Multilingual Generation and Summarization of Job Adverts: the TREE Project. A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.\n",
      "['query engine']\n",
      "['example-based learning', 'pattern matching', 'case-based reasoning', 'text generation', 'text summarization']\n",
      "0.0\n",
      "Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian-English Case Study. Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally.\n",
      "['speech recognition', 'corpus']\n",
      "['automated speech recognition', 'computer-assisted process', 'error analysis']\n",
      "0.0\n",
      "MedAI at SemEval-2021 Task 10: Negation-aware Pre-training for Source-free Negation Detection Domain Adaptation. Due to the increasing concerns for data privacy, source-free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remains private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source-Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negationaware Pre-training (NAP) to incorporate negation knowledge into model. Our method wins the 1st place with F1-score of 0.822 on the official blind test set of Negation Detection Track.\n",
      "['negationaware pre-training']\n",
      "['unsupervised domain adaptation', 'pseudo labeling', 'negation-aware pre-training']\n",
      "0.0\n",
      "An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline. This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.\n",
      "['contextual information']\n",
      "['vector space model', 'unsupervised learning', 'stoplist']\n",
      "0.0\n",
      "Extracting Fine-Grained Economic Events from Business News. Based on a recently developed fine-grained event extraction dataset for the economic domain, we present in a pilot study for supervised economic event extraction. We investigate how a stateof-the-art model for event extraction performs on the trigger and argument identification and classification. While F 1-scores of above 50% are obtained on the task of trigger identification, we observe a large gap in performance compared to results on the benchmark ACE05 dataset. We show that single-token triggers do not provide sufficient discriminative information for a finegrained event detection setup in a closed domain such as economics, since many classes have a large degree of lexico-semantic and contextual overlap.\n",
      "['pilot study']\n",
      "['supervised learning', 'event extraction', 'trigger identification', 'argument identification', 'classification']\n",
      "0.0\n",
      "Architectures of ``toy'' systems for teaching machine translation. This paper addresses the advantages of practical academic teaching of machine translation by implementations of \"toy\" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success.\n",
      "['describing two possible architectures']\n",
      "['method1', 'method2', 'method3']\n",
      "0.0\n",
      "Dependency-Based Relation Mining for Biomedical Literature. We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.\n",
      "['dependency parser']\n",
      "['syntax-based filtering', 'dependency parsing']\n",
      "0.0\n",
      "Automatic Labeling of Problem-Solving Dialogues for Computational Microgenetic Learning Analytics. This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem-solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM-CRF). Both the linear chain CRF model and the LSTM-CRF model outperform the naÃ¯ve baselines by a large margin, and LSTM-CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping-stone to automating the microgenetic analysis of cognitive interactions between students.\n",
      "['sentence embeddings', 'linear chain crf model', 'rnns', 'lstm-crf', 'microgenetic learning analytics']\n",
      "['recurrent neural networks', 'long short-term memory', 'crfs']\n",
      "0.0\n",
      "Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children's Narrative. Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non-verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues.\n",
      "['linguistic and acoustic features']\n",
      "['automatic classification', 'word categories', 'prosody', 'voice quality']\n",
      "0.0\n",
      "WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference. Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.\n",
      "['pre-trained text encoder', 'syntax encoder', 'ensemble models']\n",
      "['pre-trained text encoder', 'syntax encoder', 'feature encoder']\n",
      "1.0\n",
      "Tintin at SemEval-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens. Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se-mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.\n",
      "['simple tokens']\n",
      "['supervised learning', 'tokenization']\n",
      "0.0\n",
      "Debiasing Embeddings for Reduced Gender Bias in Text Classification. Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.\n",
      "['classifier', 'embeddings']\n",
      "['word embeddings', 'debiasing', 'classification']\n",
      "0.0\n",
      "A Multimodal Dataset for Deception Detection. This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states.\n",
      "['multimodal dataset', 'statistical analysis']\n",
      "['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "0.0\n",
      "A Research Platform for Multi-Robot Dialogue with Humans. This paper presents a research platform that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates: a simulated ground robot and an aerial robot. This flexible language and robotic platform takes advantage of existing tools for speech recognition and dialogue management that are compatible with new domains, and implements an inter-agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.\n",
      "['research platform']\n",
      "['speech recognition', 'dialogue management', 'inter-agent communication']\n",
      "0.0\n",
      "Evaluating productivity gains of hybrid ASR-MT systems for translation dictation.. This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces Nbest translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line offthe-shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7%, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4% or less would have resulted in statistically significant (p < 0.6) productivity gains in the order of 25.1% to 44.9% Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.\n",
      "['evaluation']\n",
      "['automatic speech recognition', 'statistical machine translation']\n",
      "0.0\n",
      "Social Bias in Elicited Natural Language Inferences. We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human-elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.\n",
      "['statistical analysis']\n",
      "['stereotyping', 'bias', 'human-elicitation']\n",
      "0.0\n",
      "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses. Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health-much existing work and therapy have strong linguistic components, so the creation of a large, varied, language-centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self-reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful.\n",
      "['language analysis']\n",
      "['identifying self-reported statements of diagnosis', 'exploring language differences']\n",
      "0.0\n",
      "Extracting Patient Clinical Profiles from Case Reports. This research aims to extract detailed clinical profiles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel markup tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classification system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports.\n",
      "['sentence classification system']\n",
      "['text classification', 'part-of-speech tagging', 'named entity recognition']\n",
      "0.0\n",
      "HumorHunter at SemEval-2021 Task 7: Humor and Offense Recognition with Disentangled Attention. In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).\n",
      "['deberta', 'disentangled attention']\n",
      "['deberta', 'attention', 'bert']\n",
      "0.5\n",
      "LEXIPLOIGISSI: An Educational Platform for the Teaching of Terminology in Greece. This paper introduces a project, LEXIPLOIGISSI * , which involves use of language resources for educational purposes. More particularly, the aim of the project is to develop written corpora, electronic dictionaries and exercises to enhance students' reading and writing abilities in six different school subjects. It is the product of a small-scale pilot program that will be part of the school curriculum in the three grades of Upper Secondary Education in Greece. The application seeks to create exploratory learning environments in which digital sound, image, text and video are fully integrated through the educational platform and placed under the direct control of users who are able to follow individual pathways through data stores. * The Institute for Language and Speech Processing has undertaken this project as the leading contractor and Kastaniotis Publications as a subcontractor. The first partner was responsible for the design, development and implementation of the educational platform, as well as for the provision of pedagogic scenarios of use; the second partner provided the resources (texts and multimedia material). The starting date of the project was June 1999, the development of the software and the collection of material lasted nine months.\n",
      "['educational platform']\n",
      "['lexical analysis', 'part-of-speech tagging', 'parsing']\n",
      "0.0\n",
      "Enriching An Academic knowledge base using Linked Open Data. In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter-operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm.\n",
      "['querying', 'extracting']\n",
      "['ontology mapping', 'linked open data', 'semantic web standards']\n",
      "0.0\n",
      "Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference. User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.\n",
      "['deep reinforcement learning']\n",
      "['reinforcement learning', 'deep learning', 'text representation']\n",
      "0.0\n",
      "Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format. With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects.\n",
      "['data integration']\n",
      "['data integration', 'toxic comment classification', 'web scraping']\n",
      "0.5\n",
      "A Support System for Revising Titles to Stimulate the Lay Reader's Interest in Technical Achievements. When we write a report or an explanation on a newly-developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it \"title revision wizard\". The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves.\n",
      "['support system', 'questionnaire survey']\n",
      "['text generation', 'text revision', 'question generation']\n",
      "0.0\n",
      "Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity. Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.\n",
      "['corpus', 'neural model']\n",
      "['neural networks', 'text classification', 'corpus linguistics']\n",
      "0.0\n",
      "Structured prediction models for RNN based sequence labeling in clinical text. Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side-effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF-LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities.\n",
      "['conditional random field', 'recurrent neural networks', 'crf-lstm model']\n",
      "['sequence labeling', 'named entity recognition', 'information extraction']\n",
      "0.0\n",
      "Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph. Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset.\n",
      "['global attention mechanism', 'symptom graph']\n",
      "['global attention', 'symptom graph']\n",
      "0.5\n",
      "Towards Automated Related Work Summarization. We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference/journal papers) as input, a related work summarization system creates a topic-biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.\n",
      "['summarization system']\n",
      "['textual entailment', 'information extraction', 'sentence selection']\n",
      "0.0\n",
      "Automatic Extraction of Reasoning Chains from Textual Reports. Many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. medical patient records, industrial accident reports, lawsuit records and investigation reports. Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. In this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. For this purpose, we developed a graph-based text representation that makes the relations between textual units explicit. This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. When applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes.\n",
      "['graph-based text representation', 'natural language processing tools', 'syntactic and discourse parsers']\n",
      "['syntactic parsing', 'discourse parsing']\n",
      "0.0\n",
      "Automatic Term Extraction from Knowledge Bank of Economics. KB-N is a web-accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed.\n",
      "['statistical filtering']\n",
      "['automatic term extraction', 'linguistic filtering', 'lexical filtering', 'statistical filtering']\n",
      "0.0\n",
      "Multi-task Peer-Review Score Prediction. Automatic prediction of the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target aspect. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures, we thus propose a multi-task shared structure encoding approach that automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naÃ¯ve multi-task methods.\n",
      "['multi-task shared structure encoding approach', 'peer-review datasets']\n",
      "['multi-task learning', 'shared structure encoding']\n",
      "0.0\n",
      "Development and Use of an Evaluation Collection for Personalisation of Digital Newspapers. This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross-related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross-indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations-adapting to different users and adapting to particular users over time-providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.\n",
      "['evaluation collection']\n",
      "['information retrieval', 'text classification', 'text clustering', 'text summarization']\n",
      "0.0\n",
      "Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension. In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multimodal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called 'out-of-domain' issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.\n",
      "['multi-modal context graph understanding', 'self-supervised open-set comprehension', 'graph convolutional networks (gcn)']\n",
      "['graph convolutional networks', 'self-supervised learning']\n",
      "0.0\n",
      "Modeling Intensification for Sign Language Generation: A Computational Approach. End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.\n",
      "['supervised intensity tagger', 'transformer']\n",
      "['supervised learning', 'data-driven modeling', 'transformer models']\n",
      "0.0\n",
      "Extraction and Exploration of Correlations in Patient Status Data. The paper discusses an Information Extraction approach, which is applied for the automatic processing of hospital Patient Records (PRs) in Bulgarian language. The main task reported here is retrieval of status descriptions related to anatomical organs. Due to the specific telegraphic PR style, the approach is focused on shallow analysis. Missing text descriptions and default values are another obstacle. To overcome it, we propose an algorithm for exploring the correlations between patient status data and the corresponding diagnosis. Rules for interdependencies of the patient status data are generated by clustering according to chosen metrics. In this way it becomes possible to fill in status templates for each patient when explicit descriptions are unavailable in the text. The article summarises evaluation results which concern the performance of the current IE prototype.\n",
      "['algorithm for exploring the correlations']\n",
      "['information extraction', 'clustering', 'metrics']\n",
      "0.0\n",
      "Assessing Gender Bias in Wikipedia: Inequalities in Article Titles. Potential gender biases existing in Wikipedia's content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on biographies, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender-related asymmetries in Wikipedia titles from all domains. We assess that for only half of gender-related articles, i.e., articles with words such as women or male in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports-and social-related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns.\n",
      "['analysis']\n",
      "['data collection', 'data analysis', 'natural language processing']\n",
      "0.0\n",
      "Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues. The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.\n",
      "['semantic representation', 'contextual language model', 'linguistic feature']\n",
      "['semantic representation', 'contextual language model', 'textual coherence metrics']\n",
      "1.0\n",
      "Exploiting multiple resources for Japanese to English patent translation. This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR-10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR-10 Patent translation data collection showed an improvement of the BLEU score when using a 5-grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary.\n",
      "['mt system', 'data collection']\n",
      "['machine translation', 'data extraction', 'language modeling']\n",
      "0.0\n",
      "Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus. Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.\n",
      "['transfer learning']\n",
      "['transfer learning', 'parallel corpus', 'lexicosyntactic features', 'machine translation']\n",
      "0.5\n",
      "An Analysis of Verbs in Financial News Articles and their Impact on Stock Price. Article terms can move stock prices. By analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement, we can build a model of price movement based upon the verbs used, to not only identify those terms that can move a stock price the most, but also whether they move the predicted price up or down.\n",
      "['discrete machine learning algorithm']\n",
      "['part-of-speech tagging', 'dependency parsing', 'verb sense disambiguation']\n",
      "0.0\n",
      "The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus. In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled \"blended\" emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which \"emotive\" tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.\n",
      "['emotive corpus', 'kappa statistic', 'coding scheme']\n",
      "['emotion recognition', 'multimodal annotation', 'kappa statistic']\n",
      "0.0\n",
      "IDIAP\\_TIET@LT-EDI-ACL2022 : Hope Speech Detection in Social Media using Contextualized BERT with Attention Mechanism. With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022. Our system submission ranks fourth with an F1-score of 0.84. We release our code-base here https: //github.com/Deepanshu-beep/ hope-speech-attention.\n",
      "['bert embeddings', 'attention mechanism']\n",
      "['bert', 'attention mechanism', 'contextualized bert']\n",
      "0.5\n",
      "A Case Study of Sockpuppet Detection in Wikipedia. This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia's regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases.\n",
      "['authorship attribution', 'voting scheme']\n",
      "['authorship attribution', 'voting scheme', 'short length comment']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i,d in test_set.iterrows():\n",
    "    actual=d['task_annotation']\n",
    "    actual = [x for x in actual if str(x) != 'nan']\n",
    "    predicted=d['tasks']\n",
    "    p2=precision(actual, predicted, 2)\n",
    "    test_set.loc[i,'task_p_at_2']=p2\n",
    "    \n",
    "    actual_m=d['method_annotation']\n",
    "    actual_m = [x for x in actual_m if str(x) != 'nan']\n",
    "    predicted_m=d['methods']\n",
    "    p2_m=precision(actual_m, predicted_m, 2)\n",
    "    test_set.loc[i,'method_p_at_2']=p2_m\n",
    "    \n",
    "    predicted_sci=d['task_scirex']\n",
    "    p2_sci=precision(actual, predicted_sci, 2)\n",
    "    test_set.loc[i,'task_sci_p_at_2']=p2_sci\n",
    "    \n",
    "    predicted_sci_m=d['method_scirex']\n",
    "    p2_sci_m=precision(actual_m, predicted_sci_m, 2)\n",
    "    test_set.loc[i,'method_sci_p_at_2']=p2_sci_m\n",
    "    \n",
    "    #actual_org=d['org_annotation']\n",
    "    #actual_org = [x for x in actual_org if str(x) != 'nan']\n",
    "    #if actual_org[0]=='no organization':\n",
    "    #    test_set.loc[i,'org_p_at_2']=np.NAN\n",
    "    #else:\n",
    "    #    predicted_org=d['organization']\n",
    "    #    p2_org=precision(actual_org, predicted_org, 2)\n",
    "    #    test_set.loc[i,'org_p_at_2']=p2_org\n",
    "    \n",
    "    print(d['text'])\n",
    "    print(actual_m)\n",
    "    print(predicted_m)\n",
    "    print(p2_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "526d6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.assign(correct_task=np.where(test_set.task_p_at_2>0,1,0))\n",
    "test_set=test_set.assign(correct_method=np.where(test_set.method_p_at_2>0,1,0))\n",
    "test_set=test_set.assign(correct_sci_task=np.where(test_set.task_sci_p_at_2>0,1,0))\n",
    "test_set=test_set.assign(correct_sci_method=np.where(test_set.method_sci_p_at_2>0,1,0))\n",
    "#test_set=test_set.assign(correct_org=np.where(test_set.org_p_at_2>0,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5d0b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_sim(actual, predicted):\n",
    "    if len(predicted)==0:\n",
    "        return 0\n",
    "    predicted_max=[]\n",
    "    for p in predicted:\n",
    "        max_ratio=0\n",
    "        for a in actual:\n",
    "            ratio = fuzz.partial_ratio(a.lower(), p.lower())\n",
    "            if ratio>max_ratio:\n",
    "                max_ratio=ratio\n",
    "        predicted_max.append(max_ratio)\n",
    "    return max(predicted_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e9c53a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Automatic Distinction between Specialized and Non-Specialized Occurrences of Verbs in Medical Corpora. The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with.\n",
      "['semantic annotation']\n",
      "['semantic annotation']\n",
      "100\n",
      "Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments. This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain.\n",
      "['topic model', 'word embedding', 'language model']\n",
      "['text classifiers', 'machine learning (\"ml\") approaches', 'statistical models', 'nlp methods', 'word embedding', 'topic model']\n",
      "100\n",
      "TEAM HUB@LT-EDI-EACL2021: Hope Speech Detection Based On Pre-trained Language Model. This article introduces the system description of TEAM HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.\n",
      "['language model', 'xlm-roberta', 'tf-idf']\n",
      "['xlm - roberta pre - trained language model']\n",
      "100\n",
      "Flytxt\\_NTNU at SemEval-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers. Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57% for subTask1 and 28% for subTask2.\n",
      "['supervised learning', 'conditional random fields']\n",
      "['supervised learning approach', 'na\\\\\"\\\\ive bayes classifiers']\n",
      "100\n",
      "Compiling an Interactive Literary Translation Web Site for Education Purposes. The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de MÃ¡laga are offered the possibility to take an English-Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation-aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.\n",
      "['web site']\n",
      "['course methodology']\n",
      "38\n",
      "Avoiding and Resolving Initiative Conflicts in Dialogue. In this paper, we report on an empirical study on initiative conflicts in human-human conversation. We examined these conflicts in two corpora of task-oriented dialogues. The results show that conversants try to avoid initiative conflicts, but when these conflicts occur, they are efficiently resolved by linguistic devices, such as volume.\n",
      "['empirical study']\n",
      "['linguistic devices']\n",
      "33\n",
      "Words are the Window to the Soul: Language-based User Representations for Fake News Detection. Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.\n",
      "['language-based user representations']\n",
      "['language - based user representations']\n",
      "94\n",
      "Constructing Multimodal Language Learner Texts Using LARA: Experiences with Nine Languages. LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by language learners. This involves semi-automatically tagging the text, adding other annotations and recording audio. The platform is suitable for creating texts in multiple languages via crowdsourcing techniques that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in Dutch, English, Farsi, French, German, Icelandic, Irish, Swedish and Turkish. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form.\n",
      "['learning and reading assistant']\n",
      "['crowdsourcing techniques', 'lara']\n",
      "75\n",
      "Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers. Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).\n",
      "['lexical properties', ' parallel gaze data']\n",
      "['online processing techniques']\n",
      "44\n",
      "Extracting Symptoms and their Status from Clinical Conversations. This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.\n",
      "['curriculum learning', 'sequence-to-sequence', 'hierarchical span-attribute tagging (sa-t) model']\n",
      "['deep learning approaches', 'curriculum learning']\n",
      "100\n",
      "Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications. This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1-score than the median in Task 1b.\n",
      "['pre-trained transformer', 'classifier ensembling']\n",
      "['oversampling', 'span detection models', 'classifier ensembling']\n",
      "100\n",
      "Question Answering in the Biomedical Domain. Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low.\n",
      "['question answering system']\n",
      "['question answering system', 'question answering techniques', 'open - domain techniques']\n",
      "100\n",
      "Unsupervised Term Discovery for Continuous Sign Language. Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before.\n",
      "['unsupervised term discovery']\n",
      "['sign language recognition (slr) systems']\n",
      "37\n",
      "Improving Relevance Quality in Product Search using High-Precision Query-Product Semantic Similarity. Ensuring relevance quality in product search is a critical task as it impacts the customer's ability to find intended products in the short-term as well as the general perception and trust of the e-commerce system in the long term. In this work we leverage a high-precision crossencoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline-generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re-ranking feature covering head/torso queries, and (3) as a training objective for optimization. We present results on effectiveness of this strategy for the large e-commerce setting, which has general applicability for choice of other high-precision models and tasks in ranking.\n",
      "['bert']\n",
      "['high - precision crossencoder bert model', 'high - precision models']\n",
      "100\n",
      "Initial Draft Guidelines for the Development of the Next-Generation Spoken Language Systems Speech Research Database. To best serve the strategic needs of the DARPA SLS research program by creating the next-generation speech database(s).\n",
      "['guidelines']\n",
      "[]\n",
      "0\n",
      "MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic. After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.\n",
      "['surveys', 'questionnaires']\n",
      "['human language technologies', 'language technology']\n",
      "43\n",
      "Exploring Stylistic Variation with Age and Income on Twitter. Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.\n",
      "['regression', 'stylistic features']\n",
      "[]\n",
      "0\n",
      "ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings. Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https://gitlab.com/thuan.\n",
      "['phobert embeddings', 'ensemble method']\n",
      "['ensemble method']\n",
      "100\n",
      "Prta: A System to Support the Analysis of Propaganda Techniques in the News. Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https://www.tanbih.org/prta.\n",
      "['propaganda persuasion techniques analyzer']\n",
      "['prta', 'rhetorical and psychological techniques', 'propaganda techniques']\n",
      "71\n",
      "Conversation Initiation by Diverse News Contents Introduction. In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open-domain nontask-oriented conversations and chitchat .\n",
      "['information retrieval', 'generation models']\n",
      "['conversation systems']\n",
      "59\n",
      "Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool. Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.\n",
      "['hierarchical attention capsule network', 'integrated gradients']\n",
      "['explanation tool', 'capsule system', 'hierarchical attention capsule network', 'integrated gradients', 'cola']\n",
      "100\n",
      "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies. Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.\n",
      "['masked language models', 'language models', 'entity-aware masking']\n",
      "['masked lms', 'masked language models', 'neural architectures']\n",
      "100\n",
      "A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for L2 Learners. The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.\n",
      "['grammatical analysis', 'working-memory-based processing']\n",
      "['l2 learners', 'wm - based processing analysis']\n",
      "71\n",
      "A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection. In this paper, we introduce a new English Twitter-based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.\n",
      "['transformers', 'dataset']\n",
      "[]\n",
      "0\n",
      "The N2 corpus: A semantically annotated collection of Islamist extremist stories. We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually doubleannotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure.\n",
      "['multi-layed annotation', 'annotation procedure']\n",
      "[]\n",
      "0\n",
      "Detecting Cognitive Distortions from Patient-Therapist Interactions. An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.\n",
      "['pretrained sentence-bert embeddings', 'svm classifier']\n",
      "['classification algorithms', 'natural language processing', 'svm classifier', 'cognitive behavioral therapy']\n",
      "100\n",
      "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text-and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\n",
      "['ocr', 'multimodal models']\n",
      "['multimodal models', 'machine learning systems', 'ocr']\n",
      "100\n",
      "Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria. This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy/hyponymy relation.\n",
      "['encoding terms']\n",
      "['iwn model', 'eurowordnet/italwordnet model']\n",
      "44\n",
      "Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections. In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.\n",
      "['automated clustering', 'automatic speech recognition techniques']\n",
      "['automatic speech recognition techniques', 'machine learning techniques']\n",
      "100\n",
      "Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL. This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.\n",
      "['bootstrapping']\n",
      "['contrastive analysis', 'bootstrapping approach', 'typology driven model', 'computational framework']\n",
      "100\n",
      "HUB@DravidianLangTech-EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media. This paper introduces the system description of the HUB team participating in Dravidian-LangTech-EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results.\n",
      "['multilingual bert']\n",
      "['multilingual bert model', 'fine - tuning methods']\n",
      "100\n",
      "On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA. Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction.\n",
      "['dependency schemes', 'bioinfer', 'genia treebank']\n",
      "['unifying syntax formalism', 'syntactic annotation schemes', 'unification', 'stanford scheme', 'parsers', 'parser', 'link grammar', 'corpora']\n",
      "60\n",
      "A Participant-based Approach for Event Summarization Using Twitter Streams. Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant-based event summarization approach that \"zooms-in\" the Twitter event streams to the participant level, detects the important sub-events associated with each participant using a novel mixture model that combines the \"burstiness\" and \"cohesiveness\" properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art.\n",
      "['participant-based approach']\n",
      "['participantbased approach', 'mixture model', 'participant - based event summarization approach']\n",
      "96\n",
      "ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents. We present ADVISER 1-an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.\n",
      "['python']\n",
      "['adviser 1']\n",
      "0\n",
      "Legal NERC with ontologies, Wikipedia and curriculum learning. In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.\n",
      "['named entity recognizer', 'classifier']\n",
      "['wikipediabased approach', 'lkif', 'curriculum learning']\n",
      "50\n",
      "Safety Information Mining --- What can NLP do in a disaster---. This paper describes efforts of NLP researchers to create a system to aid the relief efforts during the 2011 East Japan Earthquake. Specifically, we created a system to mine information regarding the safety of people in the disaster-stricken area from Twitter, a massive yet highly unorganized information source. We describe the large scale collaborative effort to rapidly create robust and effective systems for word segmentation, named entity recognition, and tweet classification. As a result of our efforts, we were able to effectively deliver new information about the safety of over 100 people in the disasterstricken area to a central repository for safety information.\n",
      "['robust and effective systems']\n",
      "['nlp']\n",
      "33\n",
      "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages. Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.\n",
      "['large language models']\n",
      "['text prediction algorithms', 'large language models', 'contextual text prediction', 'email and chat communication tools']\n",
      "100\n",
      "An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols. We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community. 1 1 The dataset is available on the authors' websites.\n",
      "['annotated corpus']\n",
      "['machine learning approaches']\n",
      "38\n",
      "Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision-Making. In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.\n",
      "['corpus', 'bert']\n",
      "['bert contextualized word embeddings', 'language representations']\n",
      "100\n",
      "Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning. How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.\n",
      "['propogation trees', 'kernel-based method']\n",
      "['rumor detection models', 'propagation tree kernel', 'kernel learning']\n",
      "88\n",
      "Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers. We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions-the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system (called EU-CLID) propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results (43% recall and 91% precision) on SAT algebra word problems. We also apply EUCLID to the public Dolphin algebra question set, and improve the state-of-the-art F 1-score from 73.9% to 77.0%.\n",
      "['tree transducer cascade']\n",
      "['tree transducer cascade', 'cascade of tree transducers', 'euclid']\n",
      "100\n",
      "Topic-Based Measures of Conversation for Detecting Mild CognitiveImpairment. Conversation is a complex cognitive task that engages multiple aspects of cognitive functions to remember the discussed topics, monitor the semantic and linguistic elements, and recognize others' emotions. In this paper, we propose a computational method based on the lexical coherence of consecutive utterances to quantify topical variations in semistructured conversations of older adults with cognitive impairments. Extracting the lexical knowledge of conversational utterances, our method generates a set of novel conversational measures that indicate underlying cognitive deficits among subjects with mild cognitive impairment (MCI). Our preliminary results verify the utility of the proposed conversation-based measures in distinguishing MCI from healthy controls.\n",
      "['lexical coherence of consecutive utterances']\n",
      "[]\n",
      "0\n",
      "MathAlign: Linking Formula Identifiers to their Contextual Natural Language Descriptions. Extending machine reading approaches to extract mathematical concepts and their descriptions is useful for a variety of tasks, ranging from mathematical information retrieval to increasing accessibility of scientific documents for the visually impaired. This entails segmenting mathematical formulae into identifiers and linking them to their natural language descriptions. We propose a rule-based approach for this task, which extracts L A T E X representations of formula identifiers and links them to their in-text descriptions, given only the original PDF and the location of the formula of interest. We also present a novel evaluation dataset for this task, as well as the tool used to create it.\n",
      "['rule-based approach']\n",
      "['machine reading approaches']\n",
      "68\n",
      "ReEscreve: a Translator-friendly Multi-purpose Paraphrasing Software Tool. \n",
      "['multi-purpose paraphrasing software tool']\n",
      "['reescreve']\n",
      "33\n",
      "CommandTalk: A Spoken-Language Interface for Battlefield Simulations. CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual-interpretation modhle, a \"push-to-talk\" agent, the ModSAF battlefield simulator, and \"Start-It\" (a graphical processing-spawning agent). Com-mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod-SAF system functions include the following: Show contour lines. Center on M1 platoon.\n",
      "['nuance speech recognition system', 'gemini natural language parsing', 'contextual-interpretation module', 'spoken-language interface']\n",
      "['open agent architecture', 'modsaf battlefield simulator', 'commandtalk', 'gemini naturallanguage parsing and interpretation system', 'contextual - interpretation modhle', 'nuance speech recognition system']\n",
      "100\n",
      "Parallels between Linguistics and Biology. In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues.\n",
      "['parallel construction', 'analogies']\n",
      "[]\n",
      "0\n",
      "Sentiment Analysis on the People's Daily. We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.\n",
      "['semi-supervised bootstrapping algorithm']\n",
      "['semi - supervised bootstrapping algorithm', 'bootstrapping approach', 'hierarchical bayesian model']\n",
      "95\n",
      "BIGODM System in the Social Media Mining for Health Applications Shared Task 2019. In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.\n",
      "['support vector machines', 'word embedding', 'linear kernel', 'bag-of-word', 'domain-knowledge', 'negation']\n",
      "['ensemble', 'linear support vector machine', 'vote - based undersampling ensemble approach', 'classifiers', 'vue', 'bigodm system', 'linear kernel']\n",
      "100\n",
      "Analyzing Stereotypes in Generative Text Inference Tasks. Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non-binary person, trans man, trans woman, cis man, cis woman\n",
      "['annotation', 'human judgement']\n",
      "['nlp systems']\n",
      "36\n",
      "A Checkpoint on Multilingual Misogyny Identification. We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.\n",
      "['transformers', 'bert']\n",
      "['single - language bert models', 'error analysis', 'multilingual and monolingual models', 'multilingual transformers', 'multilingual bert models', 'monolingual transformers', 'transfer learning approach']\n",
      "100\n",
      "Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants. Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A/B testing results show significant gain of customer experience metrics.\n",
      "['collaborative data relabeling']\n",
      "['collaborative data relabeling', 'cdr', 'skill recommender system', 'fallback skill recommendation system', 'collaborative data relabeling (cdr) method']\n",
      "100\n",
      "Statistical Machine Translation Models for Personalized Search. Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.\n",
      "['statistical machine translation models', 'relevance feedback']\n",
      "['search engine', 'noisy channel model', 'statistical machine translation models']\n",
      "100\n",
      "LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness. This paper describes LTL-UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).\n",
      "['embedding representation', 'multi-layer perceptron', 'bert']\n",
      "['bert', \"ltl - ude's systems\", 'embedding representation of postings']\n",
      "100\n",
      "Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services. We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.\n",
      "['content and document curation workflow manager']\n",
      "[]\n",
      "0\n",
      "Neural Networks for Joint Sentence Classification in Medical Paper Abstracts. Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-ofthe-art results on two different datasets for sequential sentence classification in medical abstracts.\n",
      "['artificial neural networks']\n",
      "['neural networks', 'ann models', 'sentence classification approaches', 'artificial neural networks', 'ann architecture', 'structured prediction']\n",
      "100\n",
      "NoPropaganda at SemEval-2020 Task 11: A Borrowed Approach to Sequence Tagging and Text Classification. This paper describes our contribution to SemEval-2020 Task 11: Detection Of Propaganda Techniques In News Articles. We start with simple LSTM baselines and move to an autoregressive transformer decoder to predict long continuous propaganda spans for the first subtask. We also adopt an approach from relation extraction by enveloping spans mentioned above with special tokens for the second subtask of propaganda technique classification. Our models report an F-score of 44.6% and a micro-averaged F-score of 58.2% for those tasks accordingly.\n",
      "['lstm', 'autoregressive transformer decoder']\n",
      "['lstm baselines', 'propaganda technique classification', 'autoregressive transformer decoder']\n",
      "100\n",
      "Interpretable Propaganda Detection in News Articles. Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.\n",
      "['qualitatively descriptive features', 'pre-trained language models']\n",
      "['automatic systems', 'deception techniques']\n",
      "55\n",
      "On Unifying Misinformation Detection. In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. â€  Work partially done while working at Facebook AI.\n",
      "['few-shot learning', 'unifiedm2']\n",
      "['unifiedm2', 'general - purpose misinformation model', \"unifiedm2's learned representation\"]\n",
      "100\n",
      "Lexically-Triggered Hidden Markov Models for Clinical Document Coding. The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.\n",
      "[' lexically-triggered hidden markov model']\n",
      "['discriminative hmm']\n",
      "39\n",
      "Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method. On a scientific concept hierarchy, a parent concept may have a few attributes, each of which has multiple values being a group of child concepts. We call these attributes facets: classification has a few facets such as application (e.g., face recognition), model (e.g., svm, knn), and metric (e.g., precision). In this work, we aim at building faceted concept hierarchies from scientific literature. Hierarchy construction methods heavily rely on hypernym detection, however, the faceted relations are parent-to-child links but the hypernym relation is a multi-hop, i.e., ancestor-todescendent link with a specific facet \"type-of\". We use information extraction techniques to find synonyms, sibling concepts, and ancestordescendent relations from a data science corpus. And we propose a hierarchy growth algorithm to infer the parent-child links from the three types of relationships. It resolves conflicts by maintaining the acyclic structure of a hierarchy.\n",
      "['information extraction techniques']\n",
      "['construction method', 'hypernym detection', 'hierarchy construction methods', 'information extraction techniques', 'svm', 'hierarchy growth algorithm']\n",
      "100\n",
      "Multilingual Generation and Summarization of Job Adverts: the TREE Project. A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.\n",
      "['query engine']\n",
      "['query engine', 'generation module', 'example - based pattern matcher', 'multilingual internet - based employment advertisement system', 'symbolic case - based reasoning techniques']\n",
      "100\n",
      "Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian-English Case Study. Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally.\n",
      "['speech recognition', 'corpus']\n",
      "['natural language processing', 'automated speech recognition']\n",
      "100\n",
      "MedAI at SemEval-2021 Task 10: Negation-aware Pre-training for Source-free Negation Detection Domain Adaptation. Due to the increasing concerns for data privacy, source-free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remains private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source-Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negationaware Pre-training (NAP) to incorporate negation knowledge into model. Our method wins the 1st place with F1-score of 0.822 on the official blind test set of Negation Detection Track.\n",
      "['negationaware pre-training']\n",
      "['negationaware pre - training']\n",
      "92\n",
      "An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline. This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.\n",
      "['contextual information']\n",
      "['senseclusters', 'unsupervised vector approach']\n",
      "27\n",
      "Extracting Fine-Grained Economic Events from Business News. Based on a recently developed fine-grained event extraction dataset for the economic domain, we present in a pilot study for supervised economic event extraction. We investigate how a stateof-the-art model for event extraction performs on the trigger and argument identification and classification. While F 1-scores of above 50% are obtained on the task of trigger identification, we observe a large gap in performance compared to results on the benchmark ACE05 dataset. We show that single-token triggers do not provide sufficient discriminative information for a finegrained event detection setup in a closed domain such as economics, since many classes have a large degree of lexico-semantic and contextual overlap.\n",
      "['pilot study']\n",
      "[]\n",
      "0\n",
      "Architectures of ``toy'' systems for teaching machine translation. This paper addresses the advantages of practical academic teaching of machine translation by implementations of \"toy\" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success.\n",
      "['describing two possible architectures']\n",
      "['educational toy systems', \"architectures of ``toy'' systems\"]\n",
      "48\n",
      "Dependency-Based Relation Mining for Biomedical Literature. We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.\n",
      "['dependency parser']\n",
      "['dependency parser', 'demonstrator']\n",
      "100\n",
      "Automatic Labeling of Problem-Solving Dialogues for Computational Microgenetic Learning Analytics. This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem-solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM-CRF). Both the linear chain CRF model and the LSTM-CRF model outperform the naÃ¯ve baselines by a large margin, and LSTM-CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping-stone to automating the microgenetic analysis of cognitive interactions between students.\n",
      "['sentence embeddings', 'linear chain crf model', 'rnns', 'lstm-crf', 'microgenetic learning analytics']\n",
      "['linear chain crf model', 'linear chain crfs', 'recurrent neural network model', 'neural network models', 'rnns']\n",
      "100\n",
      "Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children's Narrative. Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non-verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues.\n",
      "['linguistic and acoustic features']\n",
      "[]\n",
      "0\n",
      "WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference. Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.\n",
      "['pre-trained text encoder', 'syntax encoder', 'ensemble models']\n",
      "['feature encoder', 'ensemble models', 'syntax encoder', 'conflict resolution strategies', 'hybrid approach', 'base models']\n",
      "100\n",
      "Tintin at SemEval-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens. Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se-mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.\n",
      "['simple tokens']\n",
      "['tintin', 'supervised learning procedure', 'supervised learning']\n",
      "38\n",
      "Debiasing Embeddings for Reduced Gender Bias in Text Classification. Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.\n",
      "['classifier', 'embeddings']\n",
      "['debiasing embeddings', 'classifier']\n",
      "100\n",
      "A Multimodal Dataset for Deception Detection. This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states.\n",
      "['multimodal dataset', 'statistical analysis']\n",
      "['statistical analysis']\n",
      "100\n",
      "A Research Platform for Multi-Robot Dialogue with Humans. This paper presents a research platform that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates: a simulated ground robot and an aerial robot. This flexible language and robotic platform takes advantage of existing tools for speech recognition and dialogue management that are compatible with new domains, and implements an inter-agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.\n",
      "['research platform']\n",
      "[]\n",
      "0\n",
      "Evaluating productivity gains of hybrid ASR-MT systems for translation dictation.. This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces Nbest translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line offthe-shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7%, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4% or less would have resulted in statistically significant (p < 0.6) productivity gains in the order of 25.1% to 44.9% Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.\n",
      "['evaluation']\n",
      "['asr', 'human translator', 'smt', 'asr language model', 'asr system', 'vocabulary', 'automatic speech recognition', 'statistical machine translation']\n",
      "70\n",
      "Social Bias in Elicited Natural Language Inferences. We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human-elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.\n",
      "['statistical analysis']\n",
      "['human - elicitation protocol']\n",
      "35\n",
      "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses. Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health-much existing work and therapy have strong linguistic components, so the creation of a large, varied, language-centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self-reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful.\n",
      "['language analysis']\n",
      "['linguistic components']\n",
      "35\n",
      "Extracting Patient Clinical Profiles from Case Reports. This research aims to extract detailed clinical profiles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel markup tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classification system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports.\n",
      "['sentence classification system']\n",
      "['sentence classification system']\n",
      "100\n",
      "HumorHunter at SemEval-2021 Task 7: Humor and Offense Recognition with Disentangled Attention. In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).\n",
      "['deberta', 'disentangled attention']\n",
      "['deberta model', 'language models', 'hahackathon']\n",
      "100\n",
      "LEXIPLOIGISSI: An Educational Platform for the Teaching of Terminology in Greece. This paper introduces a project, LEXIPLOIGISSI * , which involves use of language resources for educational purposes. More particularly, the aim of the project is to develop written corpora, electronic dictionaries and exercises to enhance students' reading and writing abilities in six different school subjects. It is the product of a small-scale pilot program that will be part of the school curriculum in the three grades of Upper Secondary Education in Greece. The application seeks to create exploratory learning environments in which digital sound, image, text and video are fully integrated through the educational platform and placed under the direct control of users who are able to follow individual pathways through data stores. * The Institute for Language and Speech Processing has undertaken this project as the leading contractor and Kastaniotis Publications as a subcontractor. The first partner was responsible for the design, development and implementation of the educational platform, as well as for the provision of pedagogic scenarios of use; the second partner provided the resources (texts and multimedia material). The starting date of the project was June 1999, the development of the software and the collection of material lasted nine months.\n",
      "['educational platform']\n",
      "['educational platform', 'lexiploigissi']\n",
      "100\n",
      "Enriching An Academic knowledge base using Linked Open Data. In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter-operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm.\n",
      "['querying', 'extracting']\n",
      "['bootstrapping paradigm']\n",
      "60\n",
      "Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference. User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.\n",
      "['deep reinforcement learning']\n",
      "['rlta', 'deep reinforcement learning', 'text representations']\n",
      "100\n",
      "Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format. With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects.\n",
      "['data integration']\n",
      "[]\n",
      "0\n",
      "A Support System for Revising Titles to Stimulate the Lay Reader's Interest in Technical Achievements. When we write a report or an explanation on a newly-developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it \"title revision wizard\". The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves.\n",
      "['support system', 'questionnaire survey']\n",
      "['title revision wizard', 'support system']\n",
      "100\n",
      "Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity. Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.\n",
      "['corpus', 'neural model']\n",
      "['neural model']\n",
      "100\n",
      "Structured prediction models for RNN based sequence labeling in clinical text. Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side-effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF-LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities.\n",
      "['conditional random field', 'recurrent neural networks', 'crf-lstm model']\n",
      "['structured prediction models', 'crf - lstm model', 'recurrent neural networks', 'explicit modeling of pairwise potentials', 'conditional random field based structured learning models']\n",
      "100\n",
      "Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph. Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset.\n",
      "['global attention mechanism', 'symptom graph']\n",
      "['global attention mechanism']\n",
      "100\n",
      "Towards Automated Related Work Summarization. We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference/journal papers) as input, a related work summarization system creates a topic-biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.\n",
      "['summarization system']\n",
      "['rewos', 'prototype related work summarization system', 'related work summarization system']\n",
      "100\n",
      "Automatic Extraction of Reasoning Chains from Textual Reports. Many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. medical patient records, industrial accident reports, lawsuit records and investigation reports. Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. In this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. For this purpose, we developed a graph-based text representation that makes the relations between textual units explicit. This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. When applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes.\n",
      "['graph-based text representation', 'natural language processing tools', 'syntactic and discourse parsers']\n",
      "['syntactic and discourse parsers', 'graph - based text representation', 'natural language processing tools']\n",
      "100\n",
      "Automatic Term Extraction from Knowledge Bank of Economics. KB-N is a web-accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed.\n",
      "['statistical filtering']\n",
      "['linguistic , lexical and statistical filtering']\n",
      "100\n",
      "Multi-task Peer-Review Score Prediction. Automatic prediction of the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target aspect. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures, we thus propose a multi-task shared structure encoding approach that automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naÃ¯ve multi-task methods.\n",
      "['multi-task shared structure encoding approach', 'peer-review datasets']\n",
      "['naÃ¯ve multi - task methods']\n",
      "40\n",
      "Development and Use of an Evaluation Collection for Personalisation of Digital Newspapers. This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross-related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross-indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations-adapting to different users and adapting to particular users over time-providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.\n",
      "['evaluation collection']\n",
      "['user model', 'personalisation system', 'personalization system']\n",
      "48\n",
      "Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension. In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multimodal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called 'out-of-domain' issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.\n",
      "['multi-modal context graph understanding', 'self-supervised open-set comprehension', 'graph convolutional networks (gcn)']\n",
      "['graph convolutional networks']\n",
      "100\n",
      "Modeling Intensification for Sign Language Generation: A Computational Approach. End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.\n",
      "['supervised intensity tagger', 'transformer']\n",
      "['modeling intensification', 'transformer models', 'intensification modeling', 'sign language generation models', 'supervised intensity tagger']\n",
      "100\n",
      "Extraction and Exploration of Correlations in Patient Status Data. The paper discusses an Information Extraction approach, which is applied for the automatic processing of hospital Patient Records (PRs) in Bulgarian language. The main task reported here is retrieval of status descriptions related to anatomical organs. Due to the specific telegraphic PR style, the approach is focused on shallow analysis. Missing text descriptions and default values are another obstacle. To overcome it, we propose an algorithm for exploring the correlations between patient status data and the corresponding diagnosis. Rules for interdependencies of the patient status data are generated by clustering according to chosen metrics. In this way it becomes possible to fill in status templates for each patient when explicit descriptions are unavailable in the text. The article summarises evaluation results which concern the performance of the current IE prototype.\n",
      "['algorithm for exploring the correlations']\n",
      "['ie', 'information extraction approach', 'ie prototype']\n",
      "50\n",
      "Assessing Gender Bias in Wikipedia: Inequalities in Article Titles. Potential gender biases existing in Wikipedia's content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on biographies, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender-related asymmetries in Wikipedia titles from all domains. We assess that for only half of gender-related articles, i.e., articles with words such as women or male in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports-and social-related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns.\n",
      "['analysis']\n",
      "['wikipedia components']\n",
      "25\n",
      "Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues. The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.\n",
      "['semantic representation', 'contextual language model', 'linguistic feature']\n",
      "['semantic representation', 'machine learning method', 'contextual language model']\n",
      "100\n",
      "Exploiting multiple resources for Japanese to English patent translation. This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR-10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR-10 Patent translation data collection showed an improvement of the BLEU score when using a 5-grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary.\n",
      "['mt system', 'data collection']\n",
      "['japanese to english translation system', 'mt system']\n",
      "100\n",
      "Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus. Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.\n",
      "['transfer learning']\n",
      "['machine learning', 'transfer learning']\n",
      "100\n",
      "An Analysis of Verbs in Financial News Articles and their Impact on Stock Price. Article terms can move stock prices. By analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement, we can build a model of price movement based upon the verbs used, to not only identify those terms that can move a stock price the most, but also whether they move the predicted price up or down.\n",
      "['discrete machine learning algorithm']\n",
      "['discrete machine learning algorithm']\n",
      "100\n",
      "The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus. In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled \"blended\" emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which \"emotive\" tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.\n",
      "['emotive corpus', 'kappa statistic', 'coding scheme']\n",
      "['multimodal annotation scheme', 'logistic regression']\n",
      "69\n",
      "IDIAP\\_TIET@LT-EDI-ACL2022 : Hope Speech Detection in Social Media using Contextualized BERT with Attention Mechanism. With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022. Our system submission ranks fourth with an F1-score of 0.84. We release our code-base here https: //github.com/Deepanshu-beep/ hope-speech-attention.\n",
      "['bert embeddings', 'attention mechanism']\n",
      "['modular pipeline classifier', 'attention mechanism']\n",
      "100\n",
      "A Case Study of Sockpuppet Detection in Wikipedia. This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia's regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases.\n",
      "['authorship attribution', 'voting scheme']\n",
      "['authorship attribution methods', 'human process', 'voting scheme']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for i,d in test_set.iterrows():\n",
    "    actual=d['task_annotation']\n",
    "    actual = [x for x in actual if str(x) != 'nan']\n",
    "    predicted=d['tasks']\n",
    "    ratio=lev_sim(actual, predicted)\n",
    "    test_set.loc[i,'task_ratio']=ratio\n",
    "    \n",
    "    actual_m=d['method_annotation']\n",
    "    actual_m = [x for x in actual_m if str(x) != 'nan']\n",
    "    predicted_m=d['methods']\n",
    "    ratio_m=lev_sim(actual_m, predicted_m)\n",
    "    test_set.loc[i,'method_ratio']=ratio_m\n",
    "    \n",
    "    predicted_sci=d['task_scirex']\n",
    "    ratio_sci=lev_sim(actual, predicted_sci)\n",
    "    test_set.loc[i,'task_sci_ratio']=ratio_sci\n",
    "    \n",
    "    predicted_sci_m=d['method_scirex']\n",
    "    ratio_sci_m=lev_sim(actual_m, predicted_sci_m)\n",
    "    test_set.loc[i,'method_sci_ratio']=ratio_sci_m\n",
    "    \n",
    "    #actual_org=d['org_annotation']\n",
    "    #actual_org = [x for x in actual_org if str(x) != 'nan']\n",
    "    #if actual_org[0]=='no organization':\n",
    "    #    test_set.loc[i,'org_ratio']=np.NAN\n",
    "    #else:\n",
    "    #    predicted_org=d['organization']\n",
    "    #    ratio_org=lev_sim(actual_org, predicted_org)\n",
    "    #    test_set.loc[i,'org_ratio']=ratio_org\n",
    "    \n",
    "    print(d['text'])\n",
    "    #print(actual_m)\n",
    "    #print(predicted_m)\n",
    "    #print(ratio_m)\n",
    "    \n",
    "    print(actual_m)\n",
    "    print(predicted_sci_m)\n",
    "    print(ratio_sci_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce7e6913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b52a4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.assign(correct_ratio_task=np.where(test_set.task_ratio>=75,1,0))\n",
    "test_set=test_set.assign(correct_ratio_method=np.where(test_set.method_ratio>=75,1,0))\n",
    "test_set=test_set.assign(correct_ratio_sci_task=np.where(test_set.task_sci_ratio>=75,1,0))\n",
    "test_set=test_set.assign(correct_ratio_sci_method=np.where(test_set.method_sci_ratio>=75,1,0))\n",
    "#test_set=test_set.assign(correct_ratio_org=np.where(test_set.org_ratio>75,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88c9ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(actual,predicted):\n",
    "    if len(predicted)==0:\n",
    "        return 0,0,0\n",
    "    pred_set=set(predicted)\n",
    "    act_set=set(actual)\n",
    "    len_pred=len(pred_set)\n",
    "    len_act=len(act_set)\n",
    "\n",
    "    act_matches=[]\n",
    "    for a in act_set:\n",
    "        scores=[]\n",
    "        match=0\n",
    "        score_max=0\n",
    "        value_max=''\n",
    "        for p in pred_set:\n",
    "            ratio = fuzz.partial_ratio(a.lower(), p.lower())\n",
    "            if (ratio>=75) & (ratio>score_max):\n",
    "                value_max=p\n",
    "                match=1\n",
    "        if value_max!='':\n",
    "            pred_set.remove(value_max)\n",
    "\n",
    "        act_matches.append(match)\n",
    "\n",
    "    correct_complete=sum(act_matches)\n",
    "    precision = correct_complete / len_pred\n",
    "    recall = correct_complete / len_act\n",
    "    if (precision+recall>0):\n",
    "        f1=2*(precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1=0\n",
    "    return (f1,precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3042d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Automatic Distinction between Specialized and Non-Specialized Occurrences of Verbs in Medical Corpora. The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with.\n",
      "predicted task:  ['semantic annotation', 'part-of-speech tagging', 'verb sense disambiguation']\n",
      "predicted method:  ['semantic annotation', 'contrastive analysis', 'medical terminology']\n",
      "actual task:  ['contrastive automatic analysis of verbs']\n",
      "actual method:  ['semantic annotation']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments. This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain.\n",
      "predicted task:  ['text classification', 'topic modeling', 'word embedding', 'language modeling']\n",
      "predicted method:  ['topic modeling', 'word embedding', 'language modeling']\n",
      "actual task:  ['legal area classification']\n",
      "actual method:  ['topic model', 'word embedding', 'language model']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "TEAM HUB@LT-EDI-EACL2021: Hope Speech Detection Based On Pre-trained Language Model. This article introduces the system description of TEAM HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.\n",
      "predicted task:  ['text classification', 'pre-trained language model']\n",
      "predicted method:  ['xlm-roberta', 'tf-idf']\n",
      "actual task:  ['hope speech detection', 'text classification']\n",
      "actual method:  ['language model', 'xlm-roberta', 'tf-idf']\n",
      "0.5\n",
      "0.6666666666666666\n",
      "\n",
      "Flytxt\\_NTNU at SemEval-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers. Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57% for subTask1 and 28% for subTask2.\n",
      "predicted task:  ['identifying malware sentences', 'classifying malware tokens']\n",
      "predicted method:  ['conditional random fields', 'naive bayes classifiers']\n",
      "actual task:  ['identifying and classifying malware text']\n",
      "actual method:  ['supervised learning', 'conditional random fields']\n",
      "1.0\n",
      "0.5\n",
      "\n",
      "Compiling an Interactive Literary Translation Web Site for Education Purposes. The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de MÃ¡laga are offered the possibility to take an English-Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation-aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.\n",
      "predicted task:  ['translation', 'interpretation', 'debate']\n",
      "predicted method:  ['web resources', 'english-spanish literary translation', 'humanities', 'mailing list', 'debate forum', 'chat rooms']\n",
      "actual task:  ['compiling an interactive literary translation site']\n",
      "actual method:  ['web site']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Avoiding and Resolving Initiative Conflicts in Dialogue. In this paper, we report on an empirical study on initiative conflicts in human-human conversation. We examined these conflicts in two corpora of task-oriented dialogues. The results show that conversants try to avoid initiative conflicts, but when these conflicts occur, they are efficiently resolved by linguistic devices, such as volume.\n",
      "predicted task:  ['dialogue act recognition', 'dialogue act classification', 'dialogue act segmentation']\n",
      "predicted method:  ['empirical study', 'corpus analysis', 'linguistic analysis']\n",
      "actual task:  ['avoiding and resolving initiative conflicts']\n",
      "actual method:  ['empirical study']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Words are the Window to the Soul: Language-based User Representations for Fake News Detection. Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.\n",
      "predicted task:  ['fake news detection', 'language-based user representation', 'text classification']\n",
      "predicted method:  ['text representation', 'text classification', 'social network analysis']\n",
      "actual task:  ['fake news detection']\n",
      "actual method:  ['language-based user representations']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Constructing Multimodal Language Learner Texts Using LARA: Experiences with Nine Languages. LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by language learners. This involves semi-automatically tagging the text, adding other annotations and recording audio. The platform is suitable for creating texts in multiple languages via crowdsourcing techniques that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in Dutch, English, Farsi, French, German, Icelandic, Irish, Swedish and Turkish. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form.\n",
      "predicted task:  ['text annotation', 'audio recording', 'language learning']\n",
      "predicted method:  ['text tagging', 'audio recording', 'annotation']\n",
      "actual task:  ['conversion of plain texts']\n",
      "actual method:  ['learning and reading assistant']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers. Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).\n",
      "predicted task:  ['text classification', 'text processing', 'text understanding']\n",
      "predicted method:  ['lexical analysis', 'online processing', 'gaze-based measures']\n",
      "actual task:  ['eye-tracking']\n",
      "actual method:  ['lexical properties', ' parallel gaze data']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Extracting Symptoms and their Status from Clinical Conversations. This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.\n",
      "predicted task:  ['text classification', 'sequence labeling', 'sequence-to-sequence learning', 'span-attribute tagging']\n",
      "predicted method:  ['hierarchical span-attribute tagging', 'sequence-to-sequence model', 'deep learning']\n",
      "actual task:  ['extracting symptoms']\n",
      "actual method:  ['curriculum learning', 'sequence-to-sequence', 'hierarchical span-attribute tagging (sa-t) model']\n",
      "0.0\n",
      "0.6666666666666666\n",
      "\n",
      "Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications. This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1-score than the median in Task 1b.\n",
      "predicted task:  ['classification', 'span detection']\n",
      "predicted method:  ['pre-trained transformer-based models', 'oversampling', 'classifier ensembling']\n",
      "actual task:  ['span detection', 'classification']\n",
      "actual method:  ['pre-trained transformer', 'classifier ensembling']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Question Answering in the Biomedical Domain. Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low.\n",
      "predicted task:  ['question answering', 'information retrieval', 'text classification']\n",
      "predicted method:  ['lexical gap', 'quality of answer spans', 'limited annotated data']\n",
      "actual task:  ['question answering']\n",
      "actual method:  ['question answering system']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Unsupervised Term Discovery for Continuous Sign Language. Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before.\n",
      "predicted task:  ['unsupervised term discovery', 'sign language recognition', 'lexical unit discovery']\n",
      "predicted method:  ['unsupervised learning', 'spoken term discovery', 'hand shape and pose features']\n",
      "actual task:  ['sign language recognition']\n",
      "actual method:  ['unsupervised term discovery']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Improving Relevance Quality in Product Search using High-Precision Query-Product Semantic Similarity. Ensuring relevance quality in product search is a critical task as it impacts the customer's ability to find intended products in the short-term as well as the general perception and trust of the e-commerce system in the long term. In this work we leverage a high-precision crossencoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline-generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re-ranking feature covering head/torso queries, and (3) as a training objective for optimization. We present results on effectiveness of this strategy for the large e-commerce setting, which has general applicability for choice of other high-precision models and tasks in ranking.\n",
      "predicted task:  ['semantic similarity', 'ranking']\n",
      "predicted method:  ['cross-encoding', 'bert', 'semantic similarity']\n",
      "actual task:  ['relevance quality in product search']\n",
      "actual method:  ['bert']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Initial Draft Guidelines for the Development of the Next-Generation Spoken Language Systems Speech Research Database. To best serve the strategic needs of the DARPA SLS research program by creating the next-generation speech database(s).\n",
      "predicted task:  ['database creation', 'speech research']\n",
      "predicted method:  ['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "actual task:  ['spoken language systems']\n",
      "actual method:  ['guidelines']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic. After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.\n",
      "predicted task:  ['machine translation', 'information retrieval', 'language resources', 'tools and evaluation']\n",
      "predicted method:  ['machine translation', 'information retrieval', 'blark']\n",
      "actual task:  ['machine translation', 'information retrieval']\n",
      "actual method:  ['surveys', 'questionnaires']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Exploring Stylistic Variation with Age and Income on Twitter. Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.\n",
      "predicted task:  ['stylistic variation', 'syntactic features', 'predictive power', 'regression', 'temporal stylistic patterns']\n",
      "predicted method:  ['stylistic variation', 'syntactic features', 'writing style']\n",
      "actual task:  ['exploring stylistic variation']\n",
      "actual method:  ['regression', 'stylistic features']\n",
      "1.0\n",
      "0.5\n",
      "\n",
      "ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings. Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https://gitlab.com/thuan.\n",
      "predicted task:  ['fake news detection', 'linguistic feature extraction']\n",
      "predicted method:  ['phobert', 'ensemble model', 'linguistic features']\n",
      "actual task:  ['fake news detection']\n",
      "actual method:  ['phobert embeddings', 'ensemble method']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Prta: A System to Support the Analysis of Propaganda Techniques in the News. Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https://www.tanbih.org/prta.\n",
      "predicted task:  ['information retrieval', 'text classification', 'text mining']\n",
      "predicted method:  ['tokenization', 'part-of-speech tagging', 'dependency parsing']\n",
      "actual task:  ['analysis of propaganda techniques']\n",
      "actual method:  ['propaganda persuasion techniques analyzer']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Conversation Initiation by Diverse News Contents Introduction. In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open-domain nontask-oriented conversations and chitchat .\n",
      "predicted task:  ['conversation initiation', 'information retrieval', 'text generation']\n",
      "predicted method:  ['information retrieval', 'generation', 'crowd-sourcing', 'automatic evaluation', 'manual evaluation']\n",
      "actual task:  ['conversation systems']\n",
      "actual method:  ['information retrieval', 'generation models']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool. Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.\n",
      "predicted task:  ['text classification', 'offensive language detection']\n",
      "predicted method:  ['text classification', 'hierarchical attention', 'capsule networks', 'explanation tool']\n",
      "actual task:  ['categorizing offensive language']\n",
      "actual method:  ['hierarchical attention capsule network', 'integrated gradients']\n",
      "1.0\n",
      "0.5\n",
      "\n",
      "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies. Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.\n",
      "predicted task:  ['question answering', 'information retrieval', 'text classification']\n",
      "predicted method:  ['masked language models', 'entity-aware masking', 'biomedical entity-aware masking']\n",
      "actual task:  ['question answering', 'domain adaptaion', 'transfer learning']\n",
      "actual method:  ['masked language models', 'language models', 'entity-aware masking']\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "\n",
      "A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for L2 Learners. The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.\n",
      "predicted task:  ['syntax', 'working-memory']\n",
      "predicted method:  ['statistical modeling', 'working-memory capacity', 'island effects']\n",
      "actual task:  ['statistical modeling']\n",
      "actual method:  ['grammatical analysis', 'working-memory-based processing']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection. In this paper, we introduce a new English Twitter-based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.\n",
      "predicted task:  ['cyberbullying detection', 'online abuse detection', 'text classification']\n",
      "predicted method:  ['data collection', 'data analysis', 'deep learning']\n",
      "actual task:  ['online abuse detection']\n",
      "actual method:  ['transformers', 'dataset']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "The N2 corpus: A semantically annotated collection of Islamist extremist stories. We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually doubleannotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure.\n",
      "predicted task:  ['text classification', 'information extraction', 'named entity recognition', 'part-of-speech tagging', 'syntactic parsing', 'semantic role labeling', 'word sense disambiguation']\n",
      "predicted method:  ['syntax', 'semantics', 'co-reference', 'events', 'time expressions', 'temporal relationships', 'semantic roles', 'word senses']\n",
      "actual task:  ['corpus']\n",
      "actual method:  ['multi-layed annotation', 'annotation procedure']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Detecting Cognitive Distortions from Patient-Therapist Interactions. An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.\n",
      "predicted task:  ['text classification', 'feature engineering', 'data preprocessing']\n",
      "predicted method:  ['text classification', 'feature engineering', 'natural language processing']\n",
      "actual task:  ['cognitive distortion detection']\n",
      "actual method:  ['pretrained sentence-bert embeddings', 'svm classifier']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text-and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\n",
      "predicted task:  ['text classification', 'multimodal classification']\n",
      "predicted method:  ['ocr', 'multimodal models']\n",
      "actual task:  ['assessing dataset generalizability', 'hateful memes challenge', 'hateful meme detection']\n",
      "actual method:  ['ocr', 'multimodal models']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria. This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy/hyponymy relation.\n",
      "predicted task:  ['terminology extraction', 'term selection', 'synonym comparison', 'hyperonymy/hyponymy relation identification', 'mental association identification']\n",
      "predicted method:  ['term selection and extraction', 'synset comparison and evaluation', 'coding of new terms']\n",
      "actual task:  ['terminological database']\n",
      "actual method:  ['encoding terms']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections. In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.\n",
      "predicted task:  ['automatic speech recognition', 'boundary detection', 'topic classification']\n",
      "predicted method:  ['automatic speech recognition', 'machine learning', 'word error rates']\n",
      "actual task:  ['intellectual access to large oral history collections', 'topic classification', 'boundary detection']\n",
      "actual method:  ['automated clustering', 'automatic speech recognition techniques']\n",
      "0.6666666666666666\n",
      "0.5\n",
      "\n",
      "Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL. This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.\n",
      "predicted task:  ['computational linguistics', 'contrastive analysis', 'language modeling', 'machine translation', 'natural language processing', 'second language acquisition']\n",
      "predicted method:  ['contrastive analysis', 'predictive power', 'typology']\n",
      "actual task:  ['contrastive analysis', 'typology driven estimation']\n",
      "actual method:  ['bootstrapping']\n",
      "0.5\n",
      "0.0\n",
      "\n",
      "HUB@DravidianLangTech-EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media. This paper introduces the system description of the HUB team participating in Dravidian-LangTech-EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results.\n",
      "predicted task:  ['offensive language identification', 'code-mixing', 'multilingual bert']\n",
      "predicted method:  ['bert', 'multilingualism', 'code-mixing']\n",
      "actual task:  ['offensive language identification']\n",
      "actual method:  ['multilingual bert']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA. Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction.\n",
      "predicted task:  ['parsing', 'syntactic annotation', 'information extraction']\n",
      "predicted method:  ['parsing', 'dependency grammar', 'treebank']\n",
      "actual task:  ['biomedical information extraction']\n",
      "actual method:  ['dependency schemes', 'bioinfer', 'genia treebank']\n",
      "1.0\n",
      "0.3333333333333333\n",
      "\n",
      "A Participant-based Approach for Event Summarization Using Twitter Streams. Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant-based event summarization approach that \"zooms-in\" the Twitter event streams to the participant level, detects the important sub-events associated with each participant using a novel mixture model that combines the \"burstiness\" and \"cohesiveness\" properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art.\n",
      "predicted task:  ['event summarization', 'tweet classification']\n",
      "predicted method:  ['twitter event summarization', 'mixture model', 'burstiness', 'cohesiveness']\n",
      "actual task:  ['event summarization']\n",
      "actual method:  ['participant-based approach']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents. We present ADVISER 1-an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.\n",
      "predicted task:  ['dialog system', 'speech recognition', 'text recognition', 'vision recognition', 'emotion recognition', 'engagement level prediction', 'backchanneling']\n",
      "predicted method:  ['speech recognition', 'text processing', 'vision processing']\n",
      "actual task:  ['multi-domain dialog system']\n",
      "actual method:  ['python']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Legal NERC with ontologies, Wikipedia and curriculum learning. In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.\n",
      "predicted task:  ['named entity recognition', 'text classification', 'ontology mapping', 'curriculum learning']\n",
      "predicted method:  ['named entity recognition', 'classifier', 'curriculum learning']\n",
      "actual task:  ['legal domain ontology']\n",
      "actual method:  ['named entity recognizer', 'classifier']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Safety Information Mining --- What can NLP do in a disaster---. This paper describes efforts of NLP researchers to create a system to aid the relief efforts during the 2011 East Japan Earthquake. Specifically, we created a system to mine information regarding the safety of people in the disaster-stricken area from Twitter, a massive yet highly unorganized information source. We describe the large scale collaborative effort to rapidly create robust and effective systems for word segmentation, named entity recognition, and tweet classification. As a result of our efforts, we were able to effectively deliver new information about the safety of over 100 people in the disasterstricken area to a central repository for safety information.\n",
      "predicted task:  ['word segmentation', 'named entity recognition', 'tweet classification']\n",
      "predicted method:  ['word segmentation', 'named entity recognition', 'tweet classification']\n",
      "actual task:  ['information mining']\n",
      "actual method:  ['robust and effective systems']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages. Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.\n",
      "predicted task:  ['text prediction', 'phrase completion', 'language modeling']\n",
      "predicted method:  ['text prediction', 'language modeling', 'contextual signal']\n",
      "actual task:  ['text prediction']\n",
      "actual method:  ['large language models']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols. We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community. 1 1 The dataset is available on the authors' websites.\n",
      "predicted task:  ['shallow semantic parsing', 'natural language processing', 'text annotation', 'text classification']\n",
      "predicted method:  ['shallow semantic parsing', 'machine learning', 'natural language processing']\n",
      "actual task:  ['conversion of protocols into a machine-readable format']\n",
      "actual method:  ['annotated corpus']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision-Making. In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.\n",
      "predicted task:  ['stance classification', 'outcome prediction', 'impact assessment']\n",
      "predicted method:  ['stance classification', 'outcome prediction', 'impact assessment']\n",
      "actual task:  ['stance classification', 'outcome prediction', 'and impact assessment']\n",
      "actual method:  ['corpus', 'bert']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning. How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.\n",
      "predicted task:  ['identifying rumors', 'modeling microblog posts diffusion', 'detecting rumors']\n",
      "predicted method:  ['kernel learning', 'tree-based models', 'propagation tree kernel']\n",
      "actual task:  ['identifying rumors']\n",
      "actual method:  ['propogation trees', 'kernel-based method']\n",
      "1.0\n",
      "0.5\n",
      "\n",
      "Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers. We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions-the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system (called EU-CLID) propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results (43% recall and 91% precision) on SAT algebra word problems. We also apply EUCLID to the public Dolphin algebra question set, and improve the state-of-the-art F 1-score from 73.9% to 77.0%.\n",
      "predicted task:  ['semantic parsing', 'coreference resolution', 'verb interpretation']\n",
      "predicted method:  ['tree transducers', 'coreference resolution', 'verb interpretation']\n",
      "actual task:  ['semantic parsing']\n",
      "actual method:  ['tree transducer cascade']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Topic-Based Measures of Conversation for Detecting Mild CognitiveImpairment. Conversation is a complex cognitive task that engages multiple aspects of cognitive functions to remember the discussed topics, monitor the semantic and linguistic elements, and recognize others' emotions. In this paper, we propose a computational method based on the lexical coherence of consecutive utterances to quantify topical variations in semistructured conversations of older adults with cognitive impairments. Extracting the lexical knowledge of conversational utterances, our method generates a set of novel conversational measures that indicate underlying cognitive deficits among subjects with mild cognitive impairment (MCI). Our preliminary results verify the utility of the proposed conversation-based measures in distinguishing MCI from healthy controls.\n",
      "predicted task:  ['topic detection', 'lexical coherence', 'conversation analysis']\n",
      "predicted method:  ['lexical coherence', 'topic modeling', 'conversation analysis']\n",
      "actual task:  ['detecting mild cognitiveimpairment']\n",
      "actual method:  ['lexical coherence of consecutive utterances']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "MathAlign: Linking Formula Identifiers to their Contextual Natural Language Descriptions. Extending machine reading approaches to extract mathematical concepts and their descriptions is useful for a variety of tasks, ranging from mathematical information retrieval to increasing accessibility of scientific documents for the visually impaired. This entails segmenting mathematical formulae into identifiers and linking them to their natural language descriptions. We propose a rule-based approach for this task, which extracts L A T E X representations of formula identifiers and links them to their in-text descriptions, given only the original PDF and the location of the formula of interest. We also present a novel evaluation dataset for this task, as well as the tool used to create it.\n",
      "predicted task:  ['information retrieval', 'accessibility']\n",
      "predicted method:  ['rule-based approach', 'natural language processing', 'machine reading']\n",
      "actual task:  ['machine reading']\n",
      "actual method:  ['rule-based approach']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "ReEscreve: a Translator-friendly Multi-purpose Paraphrasing Software Tool. \n",
      "predicted task:  ['text generation', 'text summarization', 'text simplification', 'text translation']\n",
      "predicted method:  ['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "actual task:  ['paraphrasing']\n",
      "actual method:  ['multi-purpose paraphrasing software tool']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "CommandTalk: A Spoken-Language Interface for Battlefield Simulations. CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual-interpretation modhle, a \"push-to-talk\" agent, the ModSAF battlefield simulator, and \"Start-It\" (a graphical processing-spawning agent). Com-mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod-SAF system functions include the following: Show contour lines. Center on M1 platoon.\n",
      "predicted task:  ['speech recognition', 'natural language parsing', 'contextual interpretation']\n",
      "predicted method:  ['speech recognition', 'natural language parsing', 'contextual interpretation']\n",
      "actual task:  ['battlefield simulations']\n",
      "actual method:  ['nuance speech recognition system', 'gemini natural language parsing', 'contextual-interpretation module', 'spoken-language interface']\n",
      "0.0\n",
      "0.75\n",
      "\n",
      "Parallels between Linguistics and Biology. In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues.\n",
      "predicted task:  ['cross-disciplinary research', 'new research avenues']\n",
      "predicted method:  ['method1', 'method2', 'method3']\n",
      "actual task:  ['parallels between linguistics and biology']\n",
      "actual method:  ['parallel construction', 'analogies']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Sentiment Analysis on the People's Daily. We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.\n",
      "predicted task:  ['sentiment target clustering', 'subjective lexicons extraction', 'sentiment prediction']\n",
      "predicted method:  ['semi-supervised bootstrapping', 'hierarchical bayesian model']\n",
      "actual task:  ['sentiment analysis']\n",
      "actual method:  ['semi-supervised bootstrapping algorithm']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "BIGODM System in the Social Media Mining for Health Applications Shared Task 2019. In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.\n",
      "predicted task:  ['classification', 'information extraction', 'social media mining']\n",
      "predicted method:  ['vote-based undersampling', 'linear support vector machine', 'bag-of-word', 'domain knowledge', 'negation', 'word embedding']\n",
      "actual task:  ['social media mining']\n",
      "actual method:  ['support vector machines', 'word embedding', 'linear kernel', 'bag-of-word', 'domain-knowledge', 'negation']\n",
      "1.0\n",
      "0.8333333333333334\n",
      "\n",
      "Analyzing Stereotypes in Generative Text Inference Tasks. Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non-binary person, trans man, trans woman, cis man, cis woman\n",
      "predicted task:  ['stereotype identification', 'natural language inference', 'commonsense inference']\n",
      "predicted method:  ['stereotype analysis', 'natural language inference', 'commonsense inference']\n",
      "actual task:  ['analyzing stereotypes']\n",
      "actual method:  ['annotation', 'human judgement']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "A Checkpoint on Multilingual Misogyny Identification. We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.\n",
      "predicted task:  ['identifying misogyny in tweets', 'zero-shot classification across languages']\n",
      "predicted method:  ['pre-training', 'transfer learning', 'zero-shot classification']\n",
      "actual task:  ['misogyny identification']\n",
      "actual method:  ['transformers', 'bert']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants. Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A/B testing results show significant gain of customer experience metrics.\n",
      "predicted task:  ['automatic speech recognition', 'natural language understanding', 'skill recommendation']\n",
      "predicted method:  ['collaborative data relabeling', 'automatic speech recognition', 'natural language understanding']\n",
      "actual task:  ['recommendation system']\n",
      "actual method:  ['collaborative data relabeling']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Statistical Machine Translation Models for Personalized Search. Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.\n",
      "predicted task:  ['statistical machine translation', 'relevance feedback', 'query formulation', 'document retrieval', 'user profiling']\n",
      "predicted method:  ['statistical machine translation', 'relevance feedback', 'noisy channel model']\n",
      "actual task:  ['personalized search web']\n",
      "actual method:  ['statistical machine translation models', 'relevance feedback']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness. This paper describes LTL-UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).\n",
      "predicted task:  ['text classification', 'offensive language detection']\n",
      "predicted method:  ['bert', 'multi-layer perceptron', 'two-vote classification']\n",
      "actual task:  ['categorizing offensiveness']\n",
      "actual method:  ['embedding representation', 'multi-layer perceptron', 'bert']\n",
      "0.0\n",
      "0.6666666666666666\n",
      "\n",
      "Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services. We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.\n",
      "predicted task:  ['text processing', 'document curation', 'microservices architecture', 'content workflow management']\n",
      "predicted method:  ['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "actual task:  ['natural legal language processing and document curation']\n",
      "actual method:  ['content and document curation workflow manager']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Neural Networks for Joint Sentence Classification in Medical Paper Abstracts. Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-ofthe-art results on two different datasets for sequential sentence classification in medical abstracts.\n",
      "predicted task:  ['sentence classification', 'joint sentence classification']\n",
      "predicted method:  ['neural networks', 'artificial neural networks', 'sentence classification']\n",
      "actual task:  ['sentence classification']\n",
      "actual method:  ['artificial neural networks']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "NoPropaganda at SemEval-2020 Task 11: A Borrowed Approach to Sequence Tagging and Text Classification. This paper describes our contribution to SemEval-2020 Task 11: Detection Of Propaganda Techniques In News Articles. We start with simple LSTM baselines and move to an autoregressive transformer decoder to predict long continuous propaganda spans for the first subtask. We also adopt an approach from relation extraction by enveloping spans mentioned above with special tokens for the second subtask of propaganda technique classification. Our models report an F-score of 44.6% and a micro-averaged F-score of 58.2% for those tasks accordingly.\n",
      "predicted task:  ['sequence tagging', 'text classification']\n",
      "predicted method:  ['lstm', 'transformer', 'relation extraction']\n",
      "actual task:  ['detection of propaganda techniques', 'propaganda technique classification']\n",
      "actual method:  ['lstm', 'autoregressive transformer decoder']\n",
      "0.5\n",
      "1.0\n",
      "\n",
      "Interpretable Propaganda Detection in News Articles. Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.\n",
      "predicted task:  ['text classification', 'information extraction', 'propaganda detection']\n",
      "predicted method:  ['text classification', 'feature engineering', 'pre-trained language models']\n",
      "actual task:  ['propaganda detection']\n",
      "actual method:  ['qualitatively descriptive features', 'pre-trained language models']\n",
      "1.0\n",
      "0.5\n",
      "\n",
      "On Unifying Misinformation Detection. In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. â€  Work partially done while working at Facebook AI.\n",
      "predicted task:  ['news bias detection', 'clickbait detection', 'fake news detection', 'rumor verification']\n",
      "predicted method:  ['unification', 'joint modeling', 'representation learning']\n",
      "actual task:  ['misinformation detection', 'detecting news bias', 'verifying rumors']\n",
      "actual method:  ['few-shot learning', 'unifiedm2']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Lexically-Triggered Hidden Markov Models for Clinical Document Coding. The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.\n",
      "predicted task:  ['document classification', 'phrase detection', 'code assignment']\n",
      "predicted method:  ['lexical matching', 'hidden markov models', 'term dictionary']\n",
      "actual task:  ['clinical document coding', 'document classification']\n",
      "actual method:  [' lexically-triggered hidden markov model']\n",
      "0.5\n",
      "1.0\n",
      "\n",
      "Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method. On a scientific concept hierarchy, a parent concept may have a few attributes, each of which has multiple values being a group of child concepts. We call these attributes facets: classification has a few facets such as application (e.g., face recognition), model (e.g., svm, knn), and metric (e.g., precision). In this work, we aim at building faceted concept hierarchies from scientific literature. Hierarchy construction methods heavily rely on hypernym detection, however, the faceted relations are parent-to-child links but the hypernym relation is a multi-hop, i.e., ancestor-todescendent link with a specific facet \"type-of\". We use information extraction techniques to find synonyms, sibling concepts, and ancestordescendent relations from a data science corpus. And we propose a hierarchy growth algorithm to infer the parent-child links from the three types of relationships. It resolves conflicts by maintaining the acyclic structure of a hierarchy.\n",
      "predicted task:  ['information extraction', 'hypernym detection', 'text classification']\n",
      "predicted method:  ['hypernym detection', 'information extraction', 'hierarchy growth algorithm']\n",
      "actual task:  ['organize scientific concepts']\n",
      "actual method:  ['information extraction techniques']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Multilingual Generation and Summarization of Job Adverts: the TREE Project. A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.\n",
      "predicted task:  ['text analysis', 'text generation', 'text summarization', 'text search']\n",
      "predicted method:  ['example-based learning', 'pattern matching', 'case-based reasoning', 'text generation', 'text summarization']\n",
      "actual task:  ['multilingual generation', 'summarization']\n",
      "actual method:  ['query engine']\n",
      "0.5\n",
      "0.0\n",
      "\n",
      "Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian-English Case Study. Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally.\n",
      "predicted task:  ['automated speech recognition', 'corpus building', 'error analysis']\n",
      "predicted method:  ['automated speech recognition', 'computer-assisted process', 'error analysis']\n",
      "actual task:  ['bilingual language teaching']\n",
      "actual method:  ['speech recognition', 'corpus']\n",
      "0.0\n",
      "0.5\n",
      "\n",
      "MedAI at SemEval-2021 Task 10: Negation-aware Pre-training for Source-free Negation Detection Domain Adaptation. Due to the increasing concerns for data privacy, source-free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remains private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source-Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negationaware Pre-training (NAP) to incorporate negation knowledge into model. Our method wins the 1st place with F1-score of 0.822 on the official blind test set of Negation Detection Track.\n",
      "predicted task:  ['negation detection', 'unsupervised domain adaptation']\n",
      "predicted method:  ['unsupervised domain adaptation', 'pseudo labeling', 'negation-aware pre-training']\n",
      "actual task:  ['source-free negation detection']\n",
      "actual method:  ['negationaware pre-training']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline. This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.\n",
      "predicted task:  ['term disambiguation', 'all-word disambiguation', 'sense clustering']\n",
      "predicted method:  ['vector space model', 'unsupervised learning', 'stoplist']\n",
      "actual task:  ['biomedical term disambiguation']\n",
      "actual method:  ['contextual information']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Extracting Fine-Grained Economic Events from Business News. Based on a recently developed fine-grained event extraction dataset for the economic domain, we present in a pilot study for supervised economic event extraction. We investigate how a stateof-the-art model for event extraction performs on the trigger and argument identification and classification. While F 1-scores of above 50% are obtained on the task of trigger identification, we observe a large gap in performance compared to results on the benchmark ACE05 dataset. We show that single-token triggers do not provide sufficient discriminative information for a finegrained event detection setup in a closed domain such as economics, since many classes have a large degree of lexico-semantic and contextual overlap.\n",
      "predicted task:  ['event extraction', 'trigger identification', 'argument identification', 'argument classification']\n",
      "predicted method:  ['supervised learning', 'event extraction', 'trigger identification', 'argument identification', 'classification']\n",
      "actual task:  ['supervised economic event extraction']\n",
      "actual method:  ['pilot study']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Architectures of ``toy'' systems for teaching machine translation. This paper addresses the advantages of practical academic teaching of machine translation by implementations of \"toy\" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success.\n",
      "predicted task:  ['machine translation', 'teaching', 'evaluation']\n",
      "predicted method:  ['method1', 'method2', 'method3']\n",
      "actual task:  ['academic teaching']\n",
      "actual method:  ['describing two possible architectures']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Dependency-Based Relation Mining for Biomedical Literature. We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.\n",
      "predicted task:  ['relationship detection', 'parsing', 'sentence selection']\n",
      "predicted method:  ['syntax-based filtering', 'dependency parsing']\n",
      "actual task:  ['dependency-based relation mining', 'text mining']\n",
      "actual method:  ['dependency parser']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Automatic Labeling of Problem-Solving Dialogues for Computational Microgenetic Learning Analytics. This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem-solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM-CRF). Both the linear chain CRF model and the LSTM-CRF model outperform the naÃ¯ve baselines by a large margin, and LSTM-CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping-stone to automating the microgenetic analysis of cognitive interactions between students.\n",
      "predicted task:  ['dialogue segmentation', 'computational thinking', 'microgenetic analysis']\n",
      "predicted method:  ['recurrent neural networks', 'long short-term memory', 'crfs']\n",
      "actual task:  ['automatic labeling']\n",
      "actual method:  ['sentence embeddings', 'linear chain crf model', 'rnns', 'lstm-crf', 'microgenetic learning analytics']\n",
      "0.0\n",
      "0.2\n",
      "\n",
      "Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children's Narrative. Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non-verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues.\n",
      "predicted task:  ['classification', 'feature engineering', 'exploratory data analysis']\n",
      "predicted method:  ['automatic classification', 'word categories', 'prosody', 'voice quality']\n",
      "actual task:  ['automatic identification of autism spectrum disorders']\n",
      "actual method:  ['linguistic and acoustic features']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference. Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.\n",
      "predicted task:  ['natural language inference', 'text encoding', 'syntax encoding', 'feature encoding', 'ensemble modeling', 'conflict resolution']\n",
      "predicted method:  ['pre-trained text encoder', 'syntax encoder', 'feature encoder']\n",
      "actual task:  ['natural language inference']\n",
      "actual method:  ['pre-trained text encoder', 'syntax encoder', 'ensemble models']\n",
      "1.0\n",
      "0.6666666666666666\n",
      "\n",
      "Tintin at SemEval-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens. Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se-mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.\n",
      "predicted task:  ['hyperpartisan news detection', 'text classification']\n",
      "predicted method:  ['supervised learning', 'tokenization']\n",
      "actual task:  ['hyperpartisan news detection']\n",
      "actual method:  ['simple tokens']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Debiasing Embeddings for Reduced Gender Bias in Text Classification. Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.\n",
      "predicted task:  ['text classification', 'debiasing word embeddings']\n",
      "predicted method:  ['word embeddings', 'debiasing', 'classification']\n",
      "actual task:  ['text classification']\n",
      "actual method:  ['classifier', 'embeddings']\n",
      "1.0\n",
      "1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Multimodal Dataset for Deception Detection. This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states.\n",
      "predicted task:  ['deception detection', 'multimodal dataset construction', 'statistical analysis']\n",
      "predicted method:  ['tokenization', 'part-of-speech tagging', 'parsing']\n",
      "actual task:  ['deception detection']\n",
      "actual method:  ['multimodal dataset', 'statistical analysis']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "A Research Platform for Multi-Robot Dialogue with Humans. This paper presents a research platform that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates: a simulated ground robot and an aerial robot. This flexible language and robotic platform takes advantage of existing tools for speech recognition and dialogue management that are compatible with new domains, and implements an inter-agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.\n",
      "predicted task:  ['spoken dialogue interaction', 'speech recognition', 'dialogue management']\n",
      "predicted method:  ['speech recognition', 'dialogue management', 'inter-agent communication']\n",
      "actual task:  ['multi-robot dialogue with humans']\n",
      "actual method:  ['research platform']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Evaluating productivity gains of hybrid ASR-MT systems for translation dictation.. This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces Nbest translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line offthe-shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7%, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4% or less would have resulted in statistically significant (p < 0.6) productivity gains in the order of 25.1% to 44.9% Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.\n",
      "predicted task:  ['translation dictation', 'automatic speech recognition', 'statistical machine translation']\n",
      "predicted method:  ['automatic speech recognition', 'statistical machine translation']\n",
      "actual task:  ['translation dictation', 'automatic speech recognition']\n",
      "actual method:  ['evaluation']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Social Bias in Elicited Natural Language Inferences. We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human-elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.\n",
      "predicted task:  ['bias and stereotyping in nlp data']\n",
      "predicted method:  ['stereotyping', 'bias', 'human-elicitation']\n",
      "actual task:  ['natural language inference']\n",
      "actual method:  ['statistical analysis']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses. Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health-much existing work and therapy have strong linguistic components, so the creation of a large, varied, language-centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self-reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful.\n",
      "predicted task:  ['text classification', 'topic modeling', 'lexical analysis']\n",
      "predicted method:  ['identifying self-reported statements of diagnosis', 'exploring language differences']\n",
      "actual task:  ['mental health']\n",
      "actual method:  ['language analysis']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Extracting Patient Clinical Profiles from Case Reports. This research aims to extract detailed clinical profiles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel markup tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classification system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports.\n",
      "predicted task:  ['text classification', 'information extraction', 'data mining']\n",
      "predicted method:  ['text classification', 'part-of-speech tagging', 'named entity recognition']\n",
      "actual task:  ['extracting patient clinical profiles']\n",
      "actual method:  ['sentence classification system']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "HumorHunter at SemEval-2021 Task 7: Humor and Offense Recognition with Disentangled Attention. In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).\n",
      "predicted task:  ['humor recognition', 'offense recognition', 'text classification', 'natural language understanding']\n",
      "predicted method:  ['deberta', 'attention', 'bert']\n",
      "actual task:  ['humor and offense recognition']\n",
      "actual method:  ['deberta', 'disentangled attention']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "LEXIPLOIGISSI: An Educational Platform for the Teaching of Terminology in Greece. This paper introduces a project, LEXIPLOIGISSI * , which involves use of language resources for educational purposes. More particularly, the aim of the project is to develop written corpora, electronic dictionaries and exercises to enhance students' reading and writing abilities in six different school subjects. It is the product of a small-scale pilot program that will be part of the school curriculum in the three grades of Upper Secondary Education in Greece. The application seeks to create exploratory learning environments in which digital sound, image, text and video are fully integrated through the educational platform and placed under the direct control of users who are able to follow individual pathways through data stores. * The Institute for Language and Speech Processing has undertaken this project as the leading contractor and Kastaniotis Publications as a subcontractor. The first partner was responsible for the design, development and implementation of the educational platform, as well as for the provision of pedagogic scenarios of use; the second partner provided the resources (texts and multimedia material). The starting date of the project was June 1999, the development of the software and the collection of material lasted nine months.\n",
      "predicted task:  ['text processing', 'information retrieval', 'machine translation']\n",
      "predicted method:  ['lexical analysis', 'part-of-speech tagging', 'parsing']\n",
      "actual task:  ['teaching of terminology']\n",
      "actual method:  ['educational platform']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Enriching An Academic knowledge base using Linked Open Data. In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter-operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm.\n",
      "predicted task:  ['information extraction', 'ontology mapping', 'linked open data']\n",
      "predicted method:  ['ontology mapping', 'linked open data', 'semantic web standards']\n",
      "actual task:  ['domain ontology']\n",
      "actual method:  ['querying', 'extracting']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference. User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.\n",
      "predicted task:  ['textual data anonymization', 'deep reinforcement learning']\n",
      "predicted method:  ['reinforcement learning', 'deep learning', 'text representation']\n",
      "actual task:  ['text anonymization']\n",
      "actual method:  ['deep reinforcement learning']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format. With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects.\n",
      "predicted task:  ['toxic comment classification', 'data integration', 'data format standardization']\n",
      "predicted method:  ['data integration', 'toxic comment classification', 'web scraping']\n",
      "actual task:  ['toxic comment classification']\n",
      "actual method:  ['data integration']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "A Support System for Revising Titles to Stimulate the Lay Reader's Interest in Technical Achievements. When we write a report or an explanation on a newly-developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it \"title revision wizard\". The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves.\n",
      "predicted task:  ['text generation', 'text revision', 'text simplification']\n",
      "predicted method:  ['text generation', 'text revision', 'question generation']\n",
      "actual task:  ['revising titles']\n",
      "actual method:  ['support system', 'questionnaire survey']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity. Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.\n",
      "predicted task:  ['text classification', 'bias detection', 'political event coverage analysis']\n",
      "predicted method:  ['neural networks', 'text classification', 'corpus linguistics']\n",
      "actual task:  ['analyzing political bias']\n",
      "actual method:  ['corpus', 'neural model']\n",
      "0.0\n",
      "0.5\n",
      "\n",
      "Structured prediction models for RNN based sequence labeling in clinical text. Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side-effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF-LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities.\n",
      "predicted task:  ['sequence labeling', 'named entity recognition', 'information extraction']\n",
      "predicted method:  ['sequence labeling', 'named entity recognition', 'information extraction']\n",
      "actual task:  ['sequence labeling', 'named entity recognition', 'information extraction']\n",
      "actual method:  ['conditional random field', 'recurrent neural networks', 'crf-lstm model']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph. Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset.\n",
      "predicted task:  ['dialogue symptom diagnosis', 'global attention', 'symptom graph']\n",
      "predicted method:  ['global attention', 'symptom graph']\n",
      "actual task:  ['symptom diagnosis']\n",
      "actual method:  ['global attention mechanism', 'symptom graph']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Towards Automated Related Work Summarization. We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference/journal papers) as input, a related work summarization system creates a topic-biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.\n",
      "predicted task:  ['automatic summarization', 'extractive summarization', 'information retrieval']\n",
      "predicted method:  ['textual entailment', 'information extraction', 'sentence selection']\n",
      "actual task:  ['automatic related work summarization']\n",
      "actual method:  ['summarization system']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Automatic Extraction of Reasoning Chains from Textual Reports. Many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. medical patient records, industrial accident reports, lawsuit records and investigation reports. Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. In this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. For this purpose, we developed a graph-based text representation that makes the relations between textual units explicit. This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. When applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes.\n",
      "predicted task:  ['textual entailment', 'information extraction', 'parsing', 'discourse parsing']\n",
      "predicted method:  ['syntactic parsing', 'discourse parsing']\n",
      "actual task:  ['automatic extraction']\n",
      "actual method:  ['graph-based text representation', 'natural language processing tools', 'syntactic and discourse parsers']\n",
      "1.0\n",
      "0.3333333333333333\n",
      "\n",
      "Automatic Term Extraction from Knowledge Bank of Economics. KB-N is a web-accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed.\n",
      "predicted task:  ['term extraction', 'word sense disambiguation']\n",
      "predicted method:  ['automatic term extraction', 'linguistic filtering', 'lexical filtering', 'statistical filtering']\n",
      "actual task:  ['automatic term extraction']\n",
      "actual method:  ['statistical filtering']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Multi-task Peer-Review Score Prediction. Automatic prediction of the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target aspect. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures, we thus propose a multi-task shared structure encoding approach that automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naÃ¯ve multi-task methods.\n",
      "predicted task:  ['task1', 'task2', 'task3', 'task4', 'task5', 'task6', 'task7', 'task8', 'task9', 'task10', 'task11', 'task12', 'task13', 'task']\n",
      "predicted method:  ['multi-task learning', 'shared structure encoding']\n",
      "actual task:  ['peer-review score prediction']\n",
      "actual method:  ['multi-task shared structure encoding approach', 'peer-review datasets']\n",
      "0.0\n",
      "0.5\n",
      "\n",
      "Development and Use of an Evaluation Collection for Personalisation of Digital Newspapers. This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross-related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross-indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations-adapting to different users and adapting to particular users over time-providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.\n",
      "predicted task:  ['information retrieval', 'information extraction', 'text classification', 'text clustering']\n",
      "predicted method:  ['information retrieval', 'text classification', 'text clustering', 'text summarization']\n",
      "actual task:  ['personalisation system']\n",
      "actual method:  ['evaluation collection']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension. In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multimodal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called 'out-of-domain' issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.\n",
      "predicted task:  ['textbook question answering', 'multimodal context understanding', 'self-supervised open-set learning']\n",
      "predicted method:  ['graph convolutional networks', 'self-supervised learning']\n",
      "actual task:  ['textbook question answering']\n",
      "actual method:  ['multi-modal context graph understanding', 'self-supervised open-set comprehension', 'graph convolutional networks (gcn)']\n",
      "1.0\n",
      "0.6666666666666666\n",
      "\n",
      "Modeling Intensification for Sign Language Generation: A Computational Approach. End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.\n",
      "predicted task:  ['sign language generation', 'data annotation', 'supervised learning']\n",
      "predicted method:  ['supervised learning', 'data-driven modeling', 'transformer models']\n",
      "actual task:  ['sign language generation']\n",
      "actual method:  ['supervised intensity tagger', 'transformer']\n",
      "1.0\n",
      "0.5\n",
      "\n",
      "Extraction and Exploration of Correlations in Patient Status Data. The paper discusses an Information Extraction approach, which is applied for the automatic processing of hospital Patient Records (PRs) in Bulgarian language. The main task reported here is retrieval of status descriptions related to anatomical organs. Due to the specific telegraphic PR style, the approach is focused on shallow analysis. Missing text descriptions and default values are another obstacle. To overcome it, we propose an algorithm for exploring the correlations between patient status data and the corresponding diagnosis. Rules for interdependencies of the patient status data are generated by clustering according to chosen metrics. In this way it becomes possible to fill in status templates for each patient when explicit descriptions are unavailable in the text. The article summarises evaluation results which concern the performance of the current IE prototype.\n",
      "predicted task:  ['information extraction', 'text processing', 'shallow analysis', 'data retrieval', 'data processing']\n",
      "predicted method:  ['information extraction', 'clustering', 'metrics']\n",
      "actual task:  ['information extraction']\n",
      "actual method:  ['algorithm for exploring the correlations']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Assessing Gender Bias in Wikipedia: Inequalities in Article Titles. Potential gender biases existing in Wikipedia's content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on biographies, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender-related asymmetries in Wikipedia titles from all domains. We assess that for only half of gender-related articles, i.e., articles with words such as women or male in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports-and social-related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns.\n",
      "predicted task:  ['text classification', 'information extraction', 'entity recognition', 'part-of-speech tagging', 'parsing']\n",
      "predicted method:  ['data collection', 'data analysis', 'natural language processing']\n",
      "actual task:  ['gender bias detection']\n",
      "actual method:  ['analysis']\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues. The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.\n",
      "predicted task:  ['classification', 'semantic representation', 'language modeling']\n",
      "predicted method:  ['semantic representation', 'contextual language model', 'textual coherence metrics']\n",
      "actual task:  ['identifying nuances in fake news']\n",
      "actual method:  ['semantic representation', 'contextual language model', 'linguistic feature']\n",
      "0.0\n",
      "0.6666666666666666\n",
      "\n",
      "Exploiting multiple resources for Japanese to English patent translation. This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR-10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR-10 Patent translation data collection showed an improvement of the BLEU score when using a 5-grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary.\n",
      "predicted task:  ['machine translation', 'data collection', 'data extraction']\n",
      "predicted method:  ['machine translation', 'data extraction', 'language modeling']\n",
      "actual task:  ['patent translation']\n",
      "actual method:  ['mt system', 'data collection']\n",
      "1.0\n",
      "0.0\n",
      "\n",
      "Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus. Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.\n",
      "predicted task:  ['transfer learning', 'feature engineering', 'dementia detection', 'machine translation']\n",
      "predicted method:  ['transfer learning', 'parallel corpus', 'lexicosyntactic features', 'machine translation']\n",
      "actual task:  ['dementia detection']\n",
      "actual method:  ['transfer learning']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "An Analysis of Verbs in Financial News Articles and their Impact on Stock Price. Article terms can move stock prices. By analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement, we can build a model of price movement based upon the verbs used, to not only identify those terms that can move a stock price the most, but also whether they move the predicted price up or down.\n",
      "predicted task:  ['identifying the sentiment of a text', 'identifying the topic of a text', 'part-of-speech tagging', 'named entity recognition']\n",
      "predicted method:  ['part-of-speech tagging', 'dependency parsing', 'verb sense disambiguation']\n",
      "actual task:  ['analysis of verbs']\n",
      "actual method:  ['discrete machine learning algorithm']\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus. In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled \"blended\" emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which \"emotive\" tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.\n",
      "predicted task:  ['emotion recognition', 'annotation of blended emotions']\n",
      "predicted method:  ['emotion recognition', 'multimodal annotation', 'kappa statistic']\n",
      "actual task:  ['emotion in dialogue']\n",
      "actual method:  ['emotive corpus', 'kappa statistic', 'coding scheme']\n",
      "0.0\n",
      "0.3333333333333333\n",
      "\n",
      "IDIAP\\_TIET@LT-EDI-ACL2022 : Hope Speech Detection in Social Media using Contextualized BERT with Attention Mechanism. With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022. Our system submission ranks fourth with an F1-score of 0.84. We release our code-base here https: //github.com/Deepanshu-beep/ hope-speech-attention.\n",
      "predicted task:  ['text classification', 'hate speech detection']\n",
      "predicted method:  ['bert', 'attention mechanism', 'contextualized bert']\n",
      "actual task:  ['hope speech detection']\n",
      "actual method:  ['bert embeddings', 'attention mechanism']\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "A Case Study of Sockpuppet Detection in Wikipedia. This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia's regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases.\n",
      "predicted task:  ['authorship attribution', 'text classification']\n",
      "predicted method:  ['authorship attribution', 'voting scheme', 'short length comment']\n",
      "actual task:  ['detection of sockpuppeteering']\n",
      "actual method:  ['authorship attribution', 'voting scheme']\n",
      "0.0\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,d in test_set.iterrows():\n",
    "    actual=d['task_annotation']\n",
    "    actual = [x for x in actual if str(x) != 'nan']\n",
    "    predicted=d['tasks']\n",
    "    f1,precision,recall=get_metrics(actual, predicted)\n",
    "    test_set.loc[i,'task_f1']=f1\n",
    "    test_set.loc[i,'task_precision']=precision\n",
    "    test_set.loc[i,'task_recall']=recall\n",
    "    \n",
    "    actual_m=d['method_annotation']\n",
    "    actual_m = [x for x in actual_m if str(x) != 'nan']\n",
    "    predicted_m=d['methods']\n",
    "    f1,precision,recall=get_metrics(actual_m, predicted_m)\n",
    "    test_set.loc[i,'method_f1']=f1\n",
    "    test_set.loc[i,'method_precision']=precision\n",
    "    test_set.loc[i,'method_recall']=recall\n",
    "    \n",
    "    predicted_sci=d['task_scirex']\n",
    "    f1,precision,recall=get_metrics(actual, predicted_sci)\n",
    "    test_set.loc[i,'task_sci_f1']=f1\n",
    "    test_set.loc[i,'task_sci_precision']=precision\n",
    "    test_set.loc[i,'task_sci_recall']=recall\n",
    "    \n",
    "    predicted_sci_m=d['method_scirex']\n",
    "    f1,precision,recall=get_metrics(actual_m, predicted_sci_m)\n",
    "    test_set.loc[i,'method_sci_ratio']=ratio_sci_m\n",
    "    test_set.loc[i,'method_sci_f1']=f1\n",
    "    test_set.loc[i,'method_sci_precision']=precision\n",
    "    test_set.loc[i,'method_sci_recall']=recall\n",
    "\n",
    "    \n",
    "#    actual_org=d['org_annotation']\n",
    "#    actual_org = [x for x in actual_org if str(x) != 'nan']\n",
    "#    if actual_org[0]=='no organization':\n",
    "#        test_set.loc[i,'org_ratio']=np.NAN\n",
    "#        test_set.loc[i,'org_f1']=np.NAN\n",
    "#        test_set.loc[i,'org_precision']=np.NAN\n",
    "#        test_set.loc[i,'org_recall']=np.NAN\n",
    "#    else:\n",
    "#        predicted_org=d['organization']\n",
    "#        f1,precision,recall=get_metrics(actual_org, predicted_org)\n",
    "#        test_set.loc[i,'org_ratio']=ratio_org\n",
    "#        test_set.loc[i,'org_f1']=f1\n",
    "#        test_set.loc[i,'org_precision']=precision\n",
    "#        test_set.loc[i,'org_recall']=recall\n",
    "    \n",
    "    print(d['text'])\n",
    "    print(\"predicted task: \",predicted)\n",
    "    print(\"predicted method: \",predicted_m)\n",
    "    #print(ratio_m)\n",
    "    \n",
    "    print(\"actual task: \",actual)\n",
    "    print(\"actual method: \",actual_m)\n",
    "    #print(predicted_sci)\n",
    "    #print(test_set.loc[i,'task_sci_f1'])\n",
    "    #print(test_set.loc[i,'task_sci_precision'])\n",
    "    print(test_set.loc[i,'task_recall'])\n",
    "    print(test_set.loc[i,'method_recall'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ba089e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set.method_sci_recall.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e50204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_p_at_2</th>\n",
       "      <th>method_p_at_2</th>\n",
       "      <th>task_sci_p_at_2</th>\n",
       "      <th>method_sci_p_at_2</th>\n",
       "      <th>correct_task</th>\n",
       "      <th>correct_method</th>\n",
       "      <th>correct_sci_task</th>\n",
       "      <th>correct_sci_method</th>\n",
       "      <th>task_ratio</th>\n",
       "      <th>method_ratio</th>\n",
       "      <th>task_sci_ratio</th>\n",
       "      <th>method_sci_ratio</th>\n",
       "      <th>correct_ratio_task</th>\n",
       "      <th>correct_ratio_method</th>\n",
       "      <th>correct_ratio_sci_task</th>\n",
       "      <th>correct_ratio_sci_method</th>\n",
       "      <th>task_f1</th>\n",
       "      <th>task_precision</th>\n",
       "      <th>task_recall</th>\n",
       "      <th>method_f1</th>\n",
       "      <th>method_precision</th>\n",
       "      <th>method_recall</th>\n",
       "      <th>task_sci_f1</th>\n",
       "      <th>task_sci_precision</th>\n",
       "      <th>task_sci_recall</th>\n",
       "      <th>method_sci_f1</th>\n",
       "      <th>method_sci_precision</th>\n",
       "      <th>method_sci_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.50</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>14.56</td>\n",
       "      <td>28.16</td>\n",
       "      <td>23.30</td>\n",
       "      <td>31.07</td>\n",
       "      <td>29.13</td>\n",
       "      <td>7406.80</td>\n",
       "      <td>7708.74</td>\n",
       "      <td>6966.99</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>51.46</td>\n",
       "      <td>57.28</td>\n",
       "      <td>57.28</td>\n",
       "      <td>59.22</td>\n",
       "      <td>27.56</td>\n",
       "      <td>20.67</td>\n",
       "      <td>48.06</td>\n",
       "      <td>33.47</td>\n",
       "      <td>28.62</td>\n",
       "      <td>47.04</td>\n",
       "      <td>36.03</td>\n",
       "      <td>29.53</td>\n",
       "      <td>55.34</td>\n",
       "      <td>34.50</td>\n",
       "      <td>32.38</td>\n",
       "      <td>45.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.34</td>\n",
       "      <td>29.57</td>\n",
       "      <td>26.71</td>\n",
       "      <td>22.83</td>\n",
       "      <td>45.20</td>\n",
       "      <td>42.48</td>\n",
       "      <td>46.50</td>\n",
       "      <td>45.66</td>\n",
       "      <td>2566.88</td>\n",
       "      <td>2522.41</td>\n",
       "      <td>3785.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.22</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.38</td>\n",
       "      <td>29.48</td>\n",
       "      <td>24.30</td>\n",
       "      <td>48.52</td>\n",
       "      <td>32.25</td>\n",
       "      <td>30.19</td>\n",
       "      <td>44.78</td>\n",
       "      <td>35.98</td>\n",
       "      <td>33.55</td>\n",
       "      <td>49.02</td>\n",
       "      <td>32.92</td>\n",
       "      <td>34.38</td>\n",
       "      <td>43.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2900.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4750.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4250.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7800.00</td>\n",
       "      <td>8800.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>57.14</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_p_at_2  method_p_at_2  task_sci_p_at_2  method_sci_p_at_2  \\\n",
       "count     10300.00       10300.00         10300.00           10300.00   \n",
       "mean         16.50          15.05            16.99              14.56   \n",
       "std          28.34          29.57            26.71              22.83   \n",
       "min           0.00           0.00             0.00               0.00   \n",
       "25%           0.00           0.00             0.00               0.00   \n",
       "50%           0.00           0.00             0.00               0.00   \n",
       "75%          50.00           0.00            50.00              50.00   \n",
       "max         100.00         100.00           100.00              50.00   \n",
       "\n",
       "       correct_task  correct_method  correct_sci_task  correct_sci_method  \\\n",
       "count      10300.00        10300.00          10300.00            10300.00   \n",
       "mean          28.16           23.30             31.07               29.13   \n",
       "std           45.20           42.48             46.50               45.66   \n",
       "min            0.00            0.00              0.00                0.00   \n",
       "25%            0.00            0.00              0.00                0.00   \n",
       "50%            0.00            0.00              0.00                0.00   \n",
       "75%          100.00            0.00            100.00              100.00   \n",
       "max          100.00          100.00            100.00              100.00   \n",
       "\n",
       "       task_ratio  method_ratio  task_sci_ratio  method_sci_ratio  \\\n",
       "count    10300.00      10300.00        10300.00           10300.0   \n",
       "mean      7406.80       7708.74         6966.99           10000.0   \n",
       "std       2566.88       2522.41         3785.30               0.0   \n",
       "min       2500.00       2900.00            0.00           10000.0   \n",
       "25%       4750.00       5000.00         4250.00           10000.0   \n",
       "50%       7800.00       8800.00        10000.00           10000.0   \n",
       "75%      10000.00      10000.00        10000.00           10000.0   \n",
       "max      10000.00      10000.00        10000.00           10000.0   \n",
       "\n",
       "       correct_ratio_task  correct_ratio_method  correct_ratio_sci_task  \\\n",
       "count            10300.00              10300.00                10300.00   \n",
       "mean                51.46                 57.28                   57.28   \n",
       "std                 50.22                 49.71                   49.71   \n",
       "min                  0.00                  0.00                    0.00   \n",
       "25%                  0.00                  0.00                    0.00   \n",
       "50%                100.00                100.00                  100.00   \n",
       "75%                100.00                100.00                  100.00   \n",
       "max                100.00                100.00                  100.00   \n",
       "\n",
       "       correct_ratio_sci_method   task_f1  task_precision  task_recall  \\\n",
       "count                  10300.00  10300.00        10300.00     10300.00   \n",
       "mean                      59.22     27.56           20.67        48.06   \n",
       "std                       49.38     29.48           24.30        48.52   \n",
       "min                        0.00      0.00            0.00         0.00   \n",
       "25%                        0.00      0.00            0.00         0.00   \n",
       "50%                      100.00     25.00           16.67        50.00   \n",
       "75%                      100.00     50.00           33.33       100.00   \n",
       "max                      100.00    100.00          100.00       100.00   \n",
       "\n",
       "       method_f1  method_precision  method_recall  task_sci_f1  \\\n",
       "count   10300.00          10300.00       10300.00     10300.00   \n",
       "mean       33.47             28.62          47.04        36.03   \n",
       "std        32.25             30.19          44.78        35.98   \n",
       "min         0.00              0.00           0.00         0.00   \n",
       "25%         0.00              0.00           0.00         0.00   \n",
       "50%        40.00             33.33          50.00        40.00   \n",
       "75%        50.00             45.00         100.00        66.67   \n",
       "max       100.00            100.00         100.00       100.00   \n",
       "\n",
       "       task_sci_precision  task_sci_recall  method_sci_f1  \\\n",
       "count            10300.00         10300.00       10300.00   \n",
       "mean                29.53            55.34          34.50   \n",
       "std                 33.55            49.02          32.92   \n",
       "min                  0.00             0.00           0.00   \n",
       "25%                  0.00             0.00           0.00   \n",
       "50%                 25.00           100.00          40.00   \n",
       "75%                 50.00           100.00          57.14   \n",
       "max                100.00           100.00         100.00   \n",
       "\n",
       "       method_sci_precision  method_sci_recall  \n",
       "count              10300.00           10300.00  \n",
       "mean                  32.38              45.61  \n",
       "std                   34.38              43.02  \n",
       "min                    0.00               0.00  \n",
       "25%                    0.00               0.00  \n",
       "50%                   33.33              50.00  \n",
       "75%                   50.00             100.00  \n",
       "max                  100.00             100.00  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set.describe()*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f905b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_p_at_2</th>\n",
       "      <th>method_p_at_2</th>\n",
       "      <th>task_sci_p_at_2</th>\n",
       "      <th>method_sci_p_at_2</th>\n",
       "      <th>correct_task</th>\n",
       "      <th>correct_method</th>\n",
       "      <th>correct_sci_task</th>\n",
       "      <th>correct_sci_method</th>\n",
       "      <th>task_ratio</th>\n",
       "      <th>method_ratio</th>\n",
       "      <th>task_sci_ratio</th>\n",
       "      <th>method_sci_ratio</th>\n",
       "      <th>correct_ratio_task</th>\n",
       "      <th>correct_ratio_method</th>\n",
       "      <th>correct_ratio_sci_task</th>\n",
       "      <th>correct_ratio_sci_method</th>\n",
       "      <th>task_f1</th>\n",
       "      <th>task_precision</th>\n",
       "      <th>task_recall</th>\n",
       "      <th>method_f1</th>\n",
       "      <th>method_precision</th>\n",
       "      <th>method_recall</th>\n",
       "      <th>task_sci_f1</th>\n",
       "      <th>task_sci_precision</th>\n",
       "      <th>task_sci_recall</th>\n",
       "      <th>method_sci_f1</th>\n",
       "      <th>method_sci_precision</th>\n",
       "      <th>method_sci_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.50</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>14.56</td>\n",
       "      <td>28.16</td>\n",
       "      <td>23.30</td>\n",
       "      <td>31.07</td>\n",
       "      <td>29.13</td>\n",
       "      <td>7406.80</td>\n",
       "      <td>7708.74</td>\n",
       "      <td>6966.99</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>51.46</td>\n",
       "      <td>57.28</td>\n",
       "      <td>57.28</td>\n",
       "      <td>58.25</td>\n",
       "      <td>27.56</td>\n",
       "      <td>20.67</td>\n",
       "      <td>48.06</td>\n",
       "      <td>33.09</td>\n",
       "      <td>28.14</td>\n",
       "      <td>46.72</td>\n",
       "      <td>36.03</td>\n",
       "      <td>29.53</td>\n",
       "      <td>55.34</td>\n",
       "      <td>34.18</td>\n",
       "      <td>32.22</td>\n",
       "      <td>44.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.34</td>\n",
       "      <td>29.57</td>\n",
       "      <td>26.71</td>\n",
       "      <td>22.83</td>\n",
       "      <td>45.20</td>\n",
       "      <td>42.48</td>\n",
       "      <td>46.50</td>\n",
       "      <td>45.66</td>\n",
       "      <td>2566.88</td>\n",
       "      <td>2522.41</td>\n",
       "      <td>3785.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.22</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.56</td>\n",
       "      <td>29.48</td>\n",
       "      <td>24.30</td>\n",
       "      <td>48.52</td>\n",
       "      <td>31.93</td>\n",
       "      <td>29.42</td>\n",
       "      <td>44.75</td>\n",
       "      <td>35.98</td>\n",
       "      <td>33.55</td>\n",
       "      <td>49.02</td>\n",
       "      <td>33.09</td>\n",
       "      <td>34.65</td>\n",
       "      <td>42.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2900.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4750.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4250.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7800.00</td>\n",
       "      <td>8800.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>28.57</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>57.14</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_p_at_2  method_p_at_2  task_sci_p_at_2  method_sci_p_at_2  \\\n",
       "count     10300.00       10300.00         10300.00           10300.00   \n",
       "mean         16.50          15.05            16.99              14.56   \n",
       "std          28.34          29.57            26.71              22.83   \n",
       "min           0.00           0.00             0.00               0.00   \n",
       "25%           0.00           0.00             0.00               0.00   \n",
       "50%           0.00           0.00             0.00               0.00   \n",
       "75%          50.00           0.00            50.00              50.00   \n",
       "max         100.00         100.00           100.00              50.00   \n",
       "\n",
       "       correct_task  correct_method  correct_sci_task  correct_sci_method  \\\n",
       "count      10300.00        10300.00          10300.00            10300.00   \n",
       "mean          28.16           23.30             31.07               29.13   \n",
       "std           45.20           42.48             46.50               45.66   \n",
       "min            0.00            0.00              0.00                0.00   \n",
       "25%            0.00            0.00              0.00                0.00   \n",
       "50%            0.00            0.00              0.00                0.00   \n",
       "75%          100.00            0.00            100.00              100.00   \n",
       "max          100.00          100.00            100.00              100.00   \n",
       "\n",
       "       task_ratio  method_ratio  task_sci_ratio  method_sci_ratio  \\\n",
       "count    10300.00      10300.00        10300.00           10300.0   \n",
       "mean      7406.80       7708.74         6966.99           10000.0   \n",
       "std       2566.88       2522.41         3785.30               0.0   \n",
       "min       2500.00       2900.00            0.00           10000.0   \n",
       "25%       4750.00       5000.00         4250.00           10000.0   \n",
       "50%       7800.00       8800.00        10000.00           10000.0   \n",
       "75%      10000.00      10000.00        10000.00           10000.0   \n",
       "max      10000.00      10000.00        10000.00           10000.0   \n",
       "\n",
       "       correct_ratio_task  correct_ratio_method  correct_ratio_sci_task  \\\n",
       "count            10300.00              10300.00                10300.00   \n",
       "mean                51.46                 57.28                   57.28   \n",
       "std                 50.22                 49.71                   49.71   \n",
       "min                  0.00                  0.00                    0.00   \n",
       "25%                  0.00                  0.00                    0.00   \n",
       "50%                100.00                100.00                  100.00   \n",
       "75%                100.00                100.00                  100.00   \n",
       "max                100.00                100.00                  100.00   \n",
       "\n",
       "       correct_ratio_sci_method   task_f1  task_precision  task_recall  \\\n",
       "count                  10300.00  10300.00        10300.00     10300.00   \n",
       "mean                      58.25     27.56           20.67        48.06   \n",
       "std                       49.56     29.48           24.30        48.52   \n",
       "min                        0.00      0.00            0.00         0.00   \n",
       "25%                        0.00      0.00            0.00         0.00   \n",
       "50%                      100.00     25.00           16.67        50.00   \n",
       "75%                      100.00     50.00           33.33       100.00   \n",
       "max                      100.00    100.00          100.00       100.00   \n",
       "\n",
       "       method_f1  method_precision  method_recall  task_sci_f1  \\\n",
       "count   10300.00          10300.00       10300.00     10300.00   \n",
       "mean       33.09             28.14          46.72        36.03   \n",
       "std        31.93             29.42          44.75        35.98   \n",
       "min         0.00              0.00           0.00         0.00   \n",
       "25%         0.00              0.00           0.00         0.00   \n",
       "50%        40.00             33.33          50.00        40.00   \n",
       "75%        50.00             45.00         100.00        66.67   \n",
       "max       100.00            100.00         100.00       100.00   \n",
       "\n",
       "       task_sci_precision  task_sci_recall  method_sci_f1  \\\n",
       "count            10300.00         10300.00       10300.00   \n",
       "mean                29.53            55.34          34.18   \n",
       "std                 33.55            49.02          33.09   \n",
       "min                  0.00             0.00           0.00   \n",
       "25%                  0.00             0.00           0.00   \n",
       "50%                 25.00           100.00          40.00   \n",
       "75%                 50.00           100.00          57.14   \n",
       "max                100.00           100.00         100.00   \n",
       "\n",
       "       method_sci_precision  method_sci_recall  \n",
       "count              10300.00           10300.00  \n",
       "mean                  32.22              44.97  \n",
       "std                   34.65              42.95  \n",
       "min                    0.00               0.00  \n",
       "25%                    0.00               0.00  \n",
       "50%                   28.57              50.00  \n",
       "75%                   50.00             100.00  \n",
       "max                  100.00             100.00  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set.describe()*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f48e4a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_p_at_2</th>\n",
       "      <th>method_p_at_2</th>\n",
       "      <th>task_sci_p_at_2</th>\n",
       "      <th>method_sci_p_at_2</th>\n",
       "      <th>correct_task</th>\n",
       "      <th>correct_method</th>\n",
       "      <th>correct_sci_task</th>\n",
       "      <th>correct_sci_method</th>\n",
       "      <th>task_ratio</th>\n",
       "      <th>method_ratio</th>\n",
       "      <th>task_sci_ratio</th>\n",
       "      <th>method_sci_ratio</th>\n",
       "      <th>correct_ratio_task</th>\n",
       "      <th>correct_ratio_method</th>\n",
       "      <th>correct_ratio_sci_task</th>\n",
       "      <th>correct_ratio_sci_method</th>\n",
       "      <th>task_f1</th>\n",
       "      <th>task_precision</th>\n",
       "      <th>task_recall</th>\n",
       "      <th>method_f1</th>\n",
       "      <th>method_precision</th>\n",
       "      <th>method_recall</th>\n",
       "      <th>task_sci_f1</th>\n",
       "      <th>task_sci_precision</th>\n",
       "      <th>task_sci_recall</th>\n",
       "      <th>method_sci_f1</th>\n",
       "      <th>method_sci_precision</th>\n",
       "      <th>method_sci_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.50</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>14.56</td>\n",
       "      <td>28.16</td>\n",
       "      <td>23.30</td>\n",
       "      <td>31.07</td>\n",
       "      <td>29.13</td>\n",
       "      <td>7406.80</td>\n",
       "      <td>7708.74</td>\n",
       "      <td>6966.99</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>51.46</td>\n",
       "      <td>57.28</td>\n",
       "      <td>57.28</td>\n",
       "      <td>58.25</td>\n",
       "      <td>27.56</td>\n",
       "      <td>20.67</td>\n",
       "      <td>48.06</td>\n",
       "      <td>33.09</td>\n",
       "      <td>28.14</td>\n",
       "      <td>46.72</td>\n",
       "      <td>36.03</td>\n",
       "      <td>29.53</td>\n",
       "      <td>55.34</td>\n",
       "      <td>34.18</td>\n",
       "      <td>32.22</td>\n",
       "      <td>44.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.34</td>\n",
       "      <td>29.57</td>\n",
       "      <td>26.71</td>\n",
       "      <td>22.83</td>\n",
       "      <td>45.20</td>\n",
       "      <td>42.48</td>\n",
       "      <td>46.50</td>\n",
       "      <td>45.66</td>\n",
       "      <td>2566.88</td>\n",
       "      <td>2522.41</td>\n",
       "      <td>3785.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.22</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.56</td>\n",
       "      <td>29.48</td>\n",
       "      <td>24.30</td>\n",
       "      <td>48.52</td>\n",
       "      <td>31.93</td>\n",
       "      <td>29.42</td>\n",
       "      <td>44.75</td>\n",
       "      <td>35.98</td>\n",
       "      <td>33.55</td>\n",
       "      <td>49.02</td>\n",
       "      <td>33.09</td>\n",
       "      <td>34.65</td>\n",
       "      <td>42.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2900.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4750.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4250.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7800.00</td>\n",
       "      <td>8800.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>28.57</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>57.14</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_p_at_2  method_p_at_2  task_sci_p_at_2  method_sci_p_at_2  \\\n",
       "count     10300.00       10300.00         10300.00           10300.00   \n",
       "mean         16.50          15.05            16.99              14.56   \n",
       "std          28.34          29.57            26.71              22.83   \n",
       "min           0.00           0.00             0.00               0.00   \n",
       "25%           0.00           0.00             0.00               0.00   \n",
       "50%           0.00           0.00             0.00               0.00   \n",
       "75%          50.00           0.00            50.00              50.00   \n",
       "max         100.00         100.00           100.00              50.00   \n",
       "\n",
       "       correct_task  correct_method  correct_sci_task  correct_sci_method  \\\n",
       "count      10300.00        10300.00          10300.00            10300.00   \n",
       "mean          28.16           23.30             31.07               29.13   \n",
       "std           45.20           42.48             46.50               45.66   \n",
       "min            0.00            0.00              0.00                0.00   \n",
       "25%            0.00            0.00              0.00                0.00   \n",
       "50%            0.00            0.00              0.00                0.00   \n",
       "75%          100.00            0.00            100.00              100.00   \n",
       "max          100.00          100.00            100.00              100.00   \n",
       "\n",
       "       task_ratio  method_ratio  task_sci_ratio  method_sci_ratio  \\\n",
       "count    10300.00      10300.00        10300.00           10300.0   \n",
       "mean      7406.80       7708.74         6966.99           10000.0   \n",
       "std       2566.88       2522.41         3785.30               0.0   \n",
       "min       2500.00       2900.00            0.00           10000.0   \n",
       "25%       4750.00       5000.00         4250.00           10000.0   \n",
       "50%       7800.00       8800.00        10000.00           10000.0   \n",
       "75%      10000.00      10000.00        10000.00           10000.0   \n",
       "max      10000.00      10000.00        10000.00           10000.0   \n",
       "\n",
       "       correct_ratio_task  correct_ratio_method  correct_ratio_sci_task  \\\n",
       "count            10300.00              10300.00                10300.00   \n",
       "mean                51.46                 57.28                   57.28   \n",
       "std                 50.22                 49.71                   49.71   \n",
       "min                  0.00                  0.00                    0.00   \n",
       "25%                  0.00                  0.00                    0.00   \n",
       "50%                100.00                100.00                  100.00   \n",
       "75%                100.00                100.00                  100.00   \n",
       "max                100.00                100.00                  100.00   \n",
       "\n",
       "       correct_ratio_sci_method   task_f1  task_precision  task_recall  \\\n",
       "count                  10300.00  10300.00        10300.00     10300.00   \n",
       "mean                      58.25     27.56           20.67        48.06   \n",
       "std                       49.56     29.48           24.30        48.52   \n",
       "min                        0.00      0.00            0.00         0.00   \n",
       "25%                        0.00      0.00            0.00         0.00   \n",
       "50%                      100.00     25.00           16.67        50.00   \n",
       "75%                      100.00     50.00           33.33       100.00   \n",
       "max                      100.00    100.00          100.00       100.00   \n",
       "\n",
       "       method_f1  method_precision  method_recall  task_sci_f1  \\\n",
       "count   10300.00          10300.00       10300.00     10300.00   \n",
       "mean       33.09             28.14          46.72        36.03   \n",
       "std        31.93             29.42          44.75        35.98   \n",
       "min         0.00              0.00           0.00         0.00   \n",
       "25%         0.00              0.00           0.00         0.00   \n",
       "50%        40.00             33.33          50.00        40.00   \n",
       "75%        50.00             45.00         100.00        66.67   \n",
       "max       100.00            100.00         100.00       100.00   \n",
       "\n",
       "       task_sci_precision  task_sci_recall  method_sci_f1  \\\n",
       "count            10300.00         10300.00       10300.00   \n",
       "mean                29.53            55.34          34.18   \n",
       "std                 33.55            49.02          33.09   \n",
       "min                  0.00             0.00           0.00   \n",
       "25%                  0.00             0.00           0.00   \n",
       "50%                 25.00           100.00          40.00   \n",
       "75%                 50.00           100.00          57.14   \n",
       "max                100.00           100.00         100.00   \n",
       "\n",
       "       method_sci_precision  method_sci_recall  \n",
       "count              10300.00           10300.00  \n",
       "mean                  32.22              44.97  \n",
       "std                   34.65              42.95  \n",
       "min                    0.00               0.00  \n",
       "25%                    0.00               0.00  \n",
       "50%                   28.57              50.00  \n",
       "75%                   50.00             100.00  \n",
       "max                  100.00             100.00  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set.describe()*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73cd4f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_p_at_2</th>\n",
       "      <th>method_p_at_2</th>\n",
       "      <th>task_sci_p_at_2</th>\n",
       "      <th>method_sci_p_at_2</th>\n",
       "      <th>correct_task</th>\n",
       "      <th>correct_method</th>\n",
       "      <th>correct_sci_task</th>\n",
       "      <th>correct_sci_method</th>\n",
       "      <th>task_ratio</th>\n",
       "      <th>method_ratio</th>\n",
       "      <th>task_sci_ratio</th>\n",
       "      <th>method_sci_ratio</th>\n",
       "      <th>correct_ratio_task</th>\n",
       "      <th>correct_ratio_method</th>\n",
       "      <th>correct_ratio_sci_task</th>\n",
       "      <th>correct_ratio_sci_method</th>\n",
       "      <th>task_f1</th>\n",
       "      <th>task_precision</th>\n",
       "      <th>task_recall</th>\n",
       "      <th>method_f1</th>\n",
       "      <th>method_precision</th>\n",
       "      <th>method_recall</th>\n",
       "      <th>task_sci_f1</th>\n",
       "      <th>task_sci_precision</th>\n",
       "      <th>task_sci_recall</th>\n",
       "      <th>method_sci_f1</th>\n",
       "      <th>method_sci_precision</th>\n",
       "      <th>method_sci_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "      <td>10300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.53</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.99</td>\n",
       "      <td>14.56</td>\n",
       "      <td>26.21</td>\n",
       "      <td>23.30</td>\n",
       "      <td>31.07</td>\n",
       "      <td>29.13</td>\n",
       "      <td>7343.69</td>\n",
       "      <td>7708.74</td>\n",
       "      <td>6966.99</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>48.54</td>\n",
       "      <td>57.28</td>\n",
       "      <td>57.28</td>\n",
       "      <td>58.25</td>\n",
       "      <td>25.94</td>\n",
       "      <td>19.34</td>\n",
       "      <td>45.63</td>\n",
       "      <td>33.09</td>\n",
       "      <td>28.14</td>\n",
       "      <td>46.72</td>\n",
       "      <td>36.03</td>\n",
       "      <td>29.53</td>\n",
       "      <td>55.34</td>\n",
       "      <td>33.86</td>\n",
       "      <td>31.90</td>\n",
       "      <td>44.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.03</td>\n",
       "      <td>29.57</td>\n",
       "      <td>26.71</td>\n",
       "      <td>22.83</td>\n",
       "      <td>44.19</td>\n",
       "      <td>42.48</td>\n",
       "      <td>46.50</td>\n",
       "      <td>45.66</td>\n",
       "      <td>2520.57</td>\n",
       "      <td>2522.41</td>\n",
       "      <td>3785.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.22</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.71</td>\n",
       "      <td>49.56</td>\n",
       "      <td>29.11</td>\n",
       "      <td>23.89</td>\n",
       "      <td>48.62</td>\n",
       "      <td>31.93</td>\n",
       "      <td>29.42</td>\n",
       "      <td>44.75</td>\n",
       "      <td>35.98</td>\n",
       "      <td>33.55</td>\n",
       "      <td>49.02</td>\n",
       "      <td>32.93</td>\n",
       "      <td>34.48</td>\n",
       "      <td>42.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2500.00</td>\n",
       "      <td>2900.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4700.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4250.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7400.00</td>\n",
       "      <td>8800.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>28.57</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>53.57</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_p_at_2  method_p_at_2  task_sci_p_at_2  method_sci_p_at_2  \\\n",
       "count     10300.00       10300.00         10300.00           10300.00   \n",
       "mean         15.53          15.05            16.99              14.56   \n",
       "std          28.03          29.57            26.71              22.83   \n",
       "min           0.00           0.00             0.00               0.00   \n",
       "25%           0.00           0.00             0.00               0.00   \n",
       "50%           0.00           0.00             0.00               0.00   \n",
       "75%          50.00           0.00            50.00              50.00   \n",
       "max         100.00         100.00           100.00              50.00   \n",
       "\n",
       "       correct_task  correct_method  correct_sci_task  correct_sci_method  \\\n",
       "count      10300.00        10300.00          10300.00            10300.00   \n",
       "mean          26.21           23.30             31.07               29.13   \n",
       "std           44.19           42.48             46.50               45.66   \n",
       "min            0.00            0.00              0.00                0.00   \n",
       "25%            0.00            0.00              0.00                0.00   \n",
       "50%            0.00            0.00              0.00                0.00   \n",
       "75%          100.00            0.00            100.00              100.00   \n",
       "max          100.00          100.00            100.00              100.00   \n",
       "\n",
       "       task_ratio  method_ratio  task_sci_ratio  method_sci_ratio  \\\n",
       "count    10300.00      10300.00        10300.00           10300.0   \n",
       "mean      7343.69       7708.74         6966.99           10000.0   \n",
       "std       2520.57       2522.41         3785.30               0.0   \n",
       "min       2500.00       2900.00            0.00           10000.0   \n",
       "25%       4700.00       5000.00         4250.00           10000.0   \n",
       "50%       7400.00       8800.00        10000.00           10000.0   \n",
       "75%      10000.00      10000.00        10000.00           10000.0   \n",
       "max      10000.00      10000.00        10000.00           10000.0   \n",
       "\n",
       "       correct_ratio_task  correct_ratio_method  correct_ratio_sci_task  \\\n",
       "count            10300.00              10300.00                10300.00   \n",
       "mean                48.54                 57.28                   57.28   \n",
       "std                 50.22                 49.71                   49.71   \n",
       "min                  0.00                  0.00                    0.00   \n",
       "25%                  0.00                  0.00                    0.00   \n",
       "50%                  0.00                100.00                  100.00   \n",
       "75%                100.00                100.00                  100.00   \n",
       "max                100.00                100.00                  100.00   \n",
       "\n",
       "       correct_ratio_sci_method   task_f1  task_precision  task_recall  \\\n",
       "count                  10300.00  10300.00        10300.00     10300.00   \n",
       "mean                      58.25     25.94           19.34        45.63   \n",
       "std                       49.56     29.11           23.89        48.62   \n",
       "min                        0.00      0.00            0.00         0.00   \n",
       "25%                        0.00      0.00            0.00         0.00   \n",
       "50%                      100.00      0.00            0.00         0.00   \n",
       "75%                      100.00     50.00           33.33       100.00   \n",
       "max                      100.00    100.00          100.00       100.00   \n",
       "\n",
       "       method_f1  method_precision  method_recall  task_sci_f1  \\\n",
       "count   10300.00          10300.00       10300.00     10300.00   \n",
       "mean       33.09             28.14          46.72        36.03   \n",
       "std        31.93             29.42          44.75        35.98   \n",
       "min         0.00              0.00           0.00         0.00   \n",
       "25%         0.00              0.00           0.00         0.00   \n",
       "50%        40.00             33.33          50.00        40.00   \n",
       "75%        50.00             45.00         100.00        66.67   \n",
       "max       100.00            100.00         100.00       100.00   \n",
       "\n",
       "       task_sci_precision  task_sci_recall  method_sci_f1  \\\n",
       "count            10300.00         10300.00       10300.00   \n",
       "mean                29.53            55.34          33.86   \n",
       "std                 33.55            49.02          32.93   \n",
       "min                  0.00             0.00           0.00   \n",
       "25%                  0.00             0.00           0.00   \n",
       "50%                 25.00           100.00          40.00   \n",
       "75%                 50.00           100.00          53.57   \n",
       "max                100.00           100.00         100.00   \n",
       "\n",
       "       method_sci_precision  method_sci_recall  \n",
       "count              10300.00           10300.00  \n",
       "mean                  31.90              44.64  \n",
       "std                   34.48              42.91  \n",
       "min                    0.00               0.00  \n",
       "25%                    0.00               0.00  \n",
       "50%                   28.57              50.00  \n",
       "75%                   50.00             100.00  \n",
       "max                  100.00             100.00  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set.describe()*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d4c5402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_p_at_2</th>\n",
       "      <th>method_p_at_2</th>\n",
       "      <th>task_sci_p_at_2</th>\n",
       "      <th>method_sci_p_at_2</th>\n",
       "      <th>org_p_at_2</th>\n",
       "      <th>correct_task</th>\n",
       "      <th>correct_method</th>\n",
       "      <th>correct_sci_task</th>\n",
       "      <th>correct_sci_method</th>\n",
       "      <th>correct_org</th>\n",
       "      <th>task_ratio</th>\n",
       "      <th>method_ratio</th>\n",
       "      <th>task_sci_ratio</th>\n",
       "      <th>method_sci_ratio</th>\n",
       "      <th>org_ratio</th>\n",
       "      <th>correct_ratio_task</th>\n",
       "      <th>correct_ratio_method</th>\n",
       "      <th>correct_ratio_sci_task</th>\n",
       "      <th>correct_ratio_sci_method</th>\n",
       "      <th>correct_ratio_org</th>\n",
       "      <th>task_f1</th>\n",
       "      <th>task_precision</th>\n",
       "      <th>task_recall</th>\n",
       "      <th>method_f1</th>\n",
       "      <th>method_precision</th>\n",
       "      <th>method_recall</th>\n",
       "      <th>task_sci_f1</th>\n",
       "      <th>task_sci_precision</th>\n",
       "      <th>task_sci_recall</th>\n",
       "      <th>method_sci_f1</th>\n",
       "      <th>method_sci_precision</th>\n",
       "      <th>method_sci_recall</th>\n",
       "      <th>org_f1</th>\n",
       "      <th>org_precision</th>\n",
       "      <th>org_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>3900.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.06</td>\n",
       "      <td>17.36</td>\n",
       "      <td>19.44</td>\n",
       "      <td>11.81</td>\n",
       "      <td>30.77</td>\n",
       "      <td>33.33</td>\n",
       "      <td>26.39</td>\n",
       "      <td>33.33</td>\n",
       "      <td>23.61</td>\n",
       "      <td>26.39</td>\n",
       "      <td>8237.50</td>\n",
       "      <td>8344.44</td>\n",
       "      <td>6711.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>63.89</td>\n",
       "      <td>65.28</td>\n",
       "      <td>54.17</td>\n",
       "      <td>59.72</td>\n",
       "      <td>43.06</td>\n",
       "      <td>34.23</td>\n",
       "      <td>25.83</td>\n",
       "      <td>58.10</td>\n",
       "      <td>35.09</td>\n",
       "      <td>34.61</td>\n",
       "      <td>39.95</td>\n",
       "      <td>31.64</td>\n",
       "      <td>25.86</td>\n",
       "      <td>49.77</td>\n",
       "      <td>29.91</td>\n",
       "      <td>32.03</td>\n",
       "      <td>35.80</td>\n",
       "      <td>56.78</td>\n",
       "      <td>51.19</td>\n",
       "      <td>73.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.94</td>\n",
       "      <td>31.58</td>\n",
       "      <td>29.74</td>\n",
       "      <td>21.38</td>\n",
       "      <td>35.57</td>\n",
       "      <td>47.47</td>\n",
       "      <td>44.38</td>\n",
       "      <td>47.47</td>\n",
       "      <td>42.77</td>\n",
       "      <td>44.38</td>\n",
       "      <td>2329.63</td>\n",
       "      <td>2185.28</td>\n",
       "      <td>3916.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.37</td>\n",
       "      <td>47.94</td>\n",
       "      <td>50.18</td>\n",
       "      <td>49.39</td>\n",
       "      <td>49.86</td>\n",
       "      <td>28.39</td>\n",
       "      <td>23.63</td>\n",
       "      <td>46.81</td>\n",
       "      <td>30.65</td>\n",
       "      <td>31.76</td>\n",
       "      <td>36.66</td>\n",
       "      <td>32.61</td>\n",
       "      <td>29.73</td>\n",
       "      <td>48.25</td>\n",
       "      <td>27.85</td>\n",
       "      <td>33.17</td>\n",
       "      <td>36.21</td>\n",
       "      <td>35.72</td>\n",
       "      <td>36.72</td>\n",
       "      <td>40.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3100.00</td>\n",
       "      <td>2900.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6375.00</td>\n",
       "      <td>6700.00</td>\n",
       "      <td>4125.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>20.19</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>9700.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>33.33</td>\n",
       "      <td>33.33</td>\n",
       "      <td>20.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>30.95</td>\n",
       "      <td>33.33</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51.79</td>\n",
       "      <td>66.67</td>\n",
       "      <td>66.67</td>\n",
       "      <td>51.79</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>54.17</td>\n",
       "      <td>85.71</td>\n",
       "      <td>87.50</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_p_at_2  method_p_at_2  task_sci_p_at_2  method_sci_p_at_2  \\\n",
       "count      7200.00        7200.00          7200.00            7200.00   \n",
       "mean         18.06          17.36            19.44              11.81   \n",
       "std          26.94          31.58            29.74              21.38   \n",
       "min           0.00           0.00             0.00               0.00   \n",
       "25%           0.00           0.00             0.00               0.00   \n",
       "50%           0.00           0.00             0.00               0.00   \n",
       "75%          50.00          50.00            50.00               0.00   \n",
       "max         100.00         100.00           100.00              50.00   \n",
       "\n",
       "       org_p_at_2  correct_task  correct_method  correct_sci_task  \\\n",
       "count     3900.00       7200.00         7200.00           7200.00   \n",
       "mean        30.77         33.33           26.39             33.33   \n",
       "std         35.57         47.47           44.38             47.47   \n",
       "min          0.00          0.00            0.00              0.00   \n",
       "25%          0.00          0.00            0.00              0.00   \n",
       "50%          0.00          0.00            0.00              0.00   \n",
       "75%         50.00        100.00          100.00            100.00   \n",
       "max        100.00        100.00          100.00            100.00   \n",
       "\n",
       "       correct_sci_method  correct_org  task_ratio  method_ratio  \\\n",
       "count             7200.00      7200.00     7200.00       7200.00   \n",
       "mean                23.61        26.39     8237.50       8344.44   \n",
       "std                 42.77        44.38     2329.63       2185.28   \n",
       "min                  0.00         0.00     3100.00       2900.00   \n",
       "25%                  0.00         0.00     6375.00       6700.00   \n",
       "50%                  0.00         0.00    10000.00      10000.00   \n",
       "75%                  0.00       100.00    10000.00      10000.00   \n",
       "max                100.00       100.00    10000.00      10000.00   \n",
       "\n",
       "       task_sci_ratio  method_sci_ratio  org_ratio  correct_ratio_task  \\\n",
       "count         7200.00            7200.0     3900.0             7200.00   \n",
       "mean          6711.11               0.0    10000.0               63.89   \n",
       "std           3916.26               0.0        0.0               48.37   \n",
       "min              0.00               0.0    10000.0                0.00   \n",
       "25%           4125.00               0.0    10000.0                0.00   \n",
       "50%           9700.00               0.0    10000.0              100.00   \n",
       "75%          10000.00               0.0    10000.0              100.00   \n",
       "max          10000.00               0.0    10000.0              100.00   \n",
       "\n",
       "       correct_ratio_method  correct_ratio_sci_task  correct_ratio_sci_method  \\\n",
       "count               7200.00                 7200.00                   7200.00   \n",
       "mean                  65.28                   54.17                     59.72   \n",
       "std                   47.94                   50.18                     49.39   \n",
       "min                    0.00                    0.00                      0.00   \n",
       "25%                    0.00                    0.00                      0.00   \n",
       "50%                  100.00                  100.00                    100.00   \n",
       "75%                  100.00                  100.00                    100.00   \n",
       "max                  100.00                  100.00                    100.00   \n",
       "\n",
       "       correct_ratio_org  task_f1  task_precision  task_recall  method_f1  \\\n",
       "count            7200.00  7200.00         7200.00      7200.00    7200.00   \n",
       "mean               43.06    34.23           25.83        58.10      35.09   \n",
       "std                49.86    28.39           23.63        46.81      30.65   \n",
       "min                 0.00     0.00            0.00         0.00       0.00   \n",
       "25%                 0.00     0.00            0.00         0.00       0.00   \n",
       "50%                 0.00    40.00           33.33       100.00      40.00   \n",
       "75%               100.00    50.00           33.33       100.00      51.79   \n",
       "max               100.00   100.00          100.00       100.00     100.00   \n",
       "\n",
       "       method_precision  method_recall  task_sci_f1  task_sci_precision  \\\n",
       "count           7200.00        7200.00      7200.00             7200.00   \n",
       "mean              34.61          39.95        31.64               25.86   \n",
       "std               31.76          36.66        32.61               29.73   \n",
       "min                0.00           0.00         0.00                0.00   \n",
       "25%                0.00           0.00         0.00                0.00   \n",
       "50%               33.33          33.33        33.33               20.00   \n",
       "75%               66.67          66.67        51.79               50.00   \n",
       "max              100.00         100.00       100.00              100.00   \n",
       "\n",
       "       task_sci_recall  method_sci_f1  method_sci_precision  \\\n",
       "count          7200.00        7200.00               7200.00   \n",
       "mean             49.77          29.91                 32.03   \n",
       "std              48.25          27.85                 33.17   \n",
       "min               0.00           0.00                  0.00   \n",
       "25%               0.00           0.00                  0.00   \n",
       "50%              50.00          33.33                 30.95   \n",
       "75%             100.00          50.00                 50.00   \n",
       "max             100.00         100.00                100.00   \n",
       "\n",
       "       method_sci_recall   org_f1  org_precision  org_recall  \n",
       "count            7200.00  3900.00        3900.00     3900.00  \n",
       "mean               35.80    56.78          51.19       73.29  \n",
       "std                36.21    35.72          36.72       40.56  \n",
       "min                 0.00     0.00           0.00        0.00  \n",
       "25%                 0.00    33.33          20.19       50.00  \n",
       "50%                33.33    66.67          50.00      100.00  \n",
       "75%                54.17    85.71          87.50      100.00  \n",
       "max               100.00   100.00         100.00      100.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set.describe()*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "296d76a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_p_at_2</th>\n",
       "      <th>method_p_at_2</th>\n",
       "      <th>task_sci_p_at_2</th>\n",
       "      <th>method_sci_p_at_2</th>\n",
       "      <th>org_p_at_2</th>\n",
       "      <th>correct_task</th>\n",
       "      <th>correct_method</th>\n",
       "      <th>correct_sci_task</th>\n",
       "      <th>correct_sci_method</th>\n",
       "      <th>correct_org</th>\n",
       "      <th>task_ratio</th>\n",
       "      <th>method_ratio</th>\n",
       "      <th>task_sci_ratio</th>\n",
       "      <th>method_sci_ratio</th>\n",
       "      <th>org_ratio</th>\n",
       "      <th>correct_ratio_task</th>\n",
       "      <th>correct_ratio_method</th>\n",
       "      <th>correct_ratio_sci_task</th>\n",
       "      <th>correct_ratio_sci_method</th>\n",
       "      <th>correct_ratio_org</th>\n",
       "      <th>task_f1</th>\n",
       "      <th>task_precision</th>\n",
       "      <th>task_recall</th>\n",
       "      <th>method_f1</th>\n",
       "      <th>method_precision</th>\n",
       "      <th>method_recall</th>\n",
       "      <th>task_sci_f1</th>\n",
       "      <th>task_sci_precision</th>\n",
       "      <th>task_sci_recall</th>\n",
       "      <th>method_sci_f1</th>\n",
       "      <th>method_sci_precision</th>\n",
       "      <th>method_sci_recall</th>\n",
       "      <th>org_f1</th>\n",
       "      <th>org_precision</th>\n",
       "      <th>org_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>7200.00</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>3900.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.06</td>\n",
       "      <td>17.36</td>\n",
       "      <td>17.36</td>\n",
       "      <td>13.19</td>\n",
       "      <td>30.77</td>\n",
       "      <td>33.33</td>\n",
       "      <td>26.39</td>\n",
       "      <td>33.33</td>\n",
       "      <td>22.22</td>\n",
       "      <td>26.39</td>\n",
       "      <td>8237.50</td>\n",
       "      <td>8344.44</td>\n",
       "      <td>8290.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>63.89</td>\n",
       "      <td>65.28</td>\n",
       "      <td>75.00</td>\n",
       "      <td>69.44</td>\n",
       "      <td>43.06</td>\n",
       "      <td>34.23</td>\n",
       "      <td>25.83</td>\n",
       "      <td>58.10</td>\n",
       "      <td>35.09</td>\n",
       "      <td>34.61</td>\n",
       "      <td>39.95</td>\n",
       "      <td>36.26</td>\n",
       "      <td>27.43</td>\n",
       "      <td>68.29</td>\n",
       "      <td>33.96</td>\n",
       "      <td>31.17</td>\n",
       "      <td>47.70</td>\n",
       "      <td>56.78</td>\n",
       "      <td>51.19</td>\n",
       "      <td>73.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.94</td>\n",
       "      <td>31.58</td>\n",
       "      <td>25.40</td>\n",
       "      <td>26.53</td>\n",
       "      <td>35.57</td>\n",
       "      <td>47.47</td>\n",
       "      <td>44.38</td>\n",
       "      <td>47.47</td>\n",
       "      <td>41.87</td>\n",
       "      <td>44.38</td>\n",
       "      <td>2329.63</td>\n",
       "      <td>2185.28</td>\n",
       "      <td>3061.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.37</td>\n",
       "      <td>47.94</td>\n",
       "      <td>43.61</td>\n",
       "      <td>46.39</td>\n",
       "      <td>49.86</td>\n",
       "      <td>28.39</td>\n",
       "      <td>23.63</td>\n",
       "      <td>46.81</td>\n",
       "      <td>30.65</td>\n",
       "      <td>31.76</td>\n",
       "      <td>36.66</td>\n",
       "      <td>25.28</td>\n",
       "      <td>22.00</td>\n",
       "      <td>43.08</td>\n",
       "      <td>26.81</td>\n",
       "      <td>29.39</td>\n",
       "      <td>38.75</td>\n",
       "      <td>35.72</td>\n",
       "      <td>36.72</td>\n",
       "      <td>40.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3100.00</td>\n",
       "      <td>2900.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6375.00</td>\n",
       "      <td>6700.00</td>\n",
       "      <td>7700.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>6.82</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>20.19</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>33.33</td>\n",
       "      <td>40.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>34.85</td>\n",
       "      <td>30.95</td>\n",
       "      <td>50.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>33.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51.79</td>\n",
       "      <td>66.67</td>\n",
       "      <td>66.67</td>\n",
       "      <td>57.14</td>\n",
       "      <td>50.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>55.19</td>\n",
       "      <td>50.00</td>\n",
       "      <td>81.25</td>\n",
       "      <td>85.71</td>\n",
       "      <td>87.50</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>85.71</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_p_at_2  method_p_at_2  task_sci_p_at_2  method_sci_p_at_2  \\\n",
       "count      7200.00        7200.00          7200.00            7200.00   \n",
       "mean         18.06          17.36            17.36              13.19   \n",
       "std          26.94          31.58            25.40              26.53   \n",
       "min           0.00           0.00             0.00               0.00   \n",
       "25%           0.00           0.00             0.00               0.00   \n",
       "50%           0.00           0.00             0.00               0.00   \n",
       "75%          50.00          50.00            50.00               0.00   \n",
       "max         100.00         100.00           100.00             100.00   \n",
       "\n",
       "       org_p_at_2  correct_task  correct_method  correct_sci_task  \\\n",
       "count     3900.00       7200.00         7200.00           7200.00   \n",
       "mean        30.77         33.33           26.39             33.33   \n",
       "std         35.57         47.47           44.38             47.47   \n",
       "min          0.00          0.00            0.00              0.00   \n",
       "25%          0.00          0.00            0.00              0.00   \n",
       "50%          0.00          0.00            0.00              0.00   \n",
       "75%         50.00        100.00          100.00            100.00   \n",
       "max        100.00        100.00          100.00            100.00   \n",
       "\n",
       "       correct_sci_method  correct_org  task_ratio  method_ratio  \\\n",
       "count             7200.00      7200.00     7200.00       7200.00   \n",
       "mean                22.22        26.39     8237.50       8344.44   \n",
       "std                 41.87        44.38     2329.63       2185.28   \n",
       "min                  0.00         0.00     3100.00       2900.00   \n",
       "25%                  0.00         0.00     6375.00       6700.00   \n",
       "50%                  0.00         0.00    10000.00      10000.00   \n",
       "75%                  0.00       100.00    10000.00      10000.00   \n",
       "max                100.00       100.00    10000.00      10000.00   \n",
       "\n",
       "       task_sci_ratio  method_sci_ratio  org_ratio  correct_ratio_task  \\\n",
       "count         7200.00            7200.0     3900.0             7200.00   \n",
       "mean          8290.28               0.0    10000.0               63.89   \n",
       "std           3061.44               0.0        0.0               48.37   \n",
       "min              0.00               0.0    10000.0                0.00   \n",
       "25%           7700.00               0.0    10000.0                0.00   \n",
       "50%          10000.00               0.0    10000.0              100.00   \n",
       "75%          10000.00               0.0    10000.0              100.00   \n",
       "max          10000.00               0.0    10000.0              100.00   \n",
       "\n",
       "       correct_ratio_method  correct_ratio_sci_task  correct_ratio_sci_method  \\\n",
       "count               7200.00                 7200.00                   7200.00   \n",
       "mean                  65.28                   75.00                     69.44   \n",
       "std                   47.94                   43.61                     46.39   \n",
       "min                    0.00                    0.00                      0.00   \n",
       "25%                    0.00                   75.00                      0.00   \n",
       "50%                  100.00                  100.00                    100.00   \n",
       "75%                  100.00                  100.00                    100.00   \n",
       "max                  100.00                  100.00                    100.00   \n",
       "\n",
       "       correct_ratio_org  task_f1  task_precision  task_recall  method_f1  \\\n",
       "count            7200.00  7200.00         7200.00      7200.00    7200.00   \n",
       "mean               43.06    34.23           25.83        58.10      35.09   \n",
       "std                49.86    28.39           23.63        46.81      30.65   \n",
       "min                 0.00     0.00            0.00         0.00       0.00   \n",
       "25%                 0.00     0.00            0.00         0.00       0.00   \n",
       "50%                 0.00    40.00           33.33       100.00      40.00   \n",
       "75%               100.00    50.00           33.33       100.00      51.79   \n",
       "max               100.00   100.00          100.00       100.00     100.00   \n",
       "\n",
       "       method_precision  method_recall  task_sci_f1  task_sci_precision  \\\n",
       "count           7200.00        7200.00      7200.00             7200.00   \n",
       "mean              34.61          39.95        36.26               27.43   \n",
       "std               31.76          36.66        25.28               22.00   \n",
       "min                0.00           0.00         0.00                0.00   \n",
       "25%                0.00           0.00        12.50                6.82   \n",
       "50%               33.33          33.33        40.00               25.00   \n",
       "75%               66.67          66.67        57.14               50.00   \n",
       "max              100.00         100.00        80.00              100.00   \n",
       "\n",
       "       task_sci_recall  method_sci_f1  method_sci_precision  \\\n",
       "count          7200.00        7200.00               7200.00   \n",
       "mean             68.29          33.96                 31.17   \n",
       "std              43.08          26.81                 29.39   \n",
       "min               0.00           0.00                  0.00   \n",
       "25%              25.00           0.00                  0.00   \n",
       "50%             100.00          34.85                 30.95   \n",
       "75%             100.00          55.19                 50.00   \n",
       "max             100.00          85.71                100.00   \n",
       "\n",
       "       method_sci_recall   org_f1  org_precision  org_recall  \n",
       "count            7200.00  3900.00        3900.00     3900.00  \n",
       "mean               47.70    56.78          51.19       73.29  \n",
       "std                38.75    35.72          36.72       40.56  \n",
       "min                 0.00     0.00           0.00        0.00  \n",
       "25%                 0.00    33.33          20.19       50.00  \n",
       "50%                50.00    66.67          50.00      100.00  \n",
       "75%                81.25    85.71          87.50      100.00  \n",
       "max               100.00   100.00         100.00      100.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set.describe()*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
